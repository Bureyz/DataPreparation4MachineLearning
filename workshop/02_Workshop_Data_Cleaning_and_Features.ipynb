{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "091b4f2a-6121-478a-8647-3de7fb1679a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Workshop 2: Data Cleaning and Feature Engineering\n",
    "\n",
    "## Objective\n",
    "\n",
    "Clean the RetailMax sales data and build customer-level features (Customer 360) for the ML model.\n",
    "\n",
    "## Context and Requirements\n",
    "\n",
    "- **Workshop:** Customer Segmentation for RetailMax\n",
    "- **Notebook type:** Hands-on Exercise\n",
    "- **Prerequisites:** `01_Workshop_Data_Exploration.ipynb` completed\n",
    "- **Technical requirements:**\n",
    "  - Databricks Runtime 14.x LTS or newer\n",
    "  - Unity Catalog enabled\n",
    "- **Execution time:** ~30 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Theoretical Background\n",
    "\n",
    "**Data Cleaning Strategy:**\n",
    "\n",
    "| Issue Type | Strategy | Example |\n",
    "|------------|----------|---------|\n",
    "| Missing key IDs | Remove row | `customer_id` is NULL |\n",
    "| Missing descriptive fields | Impute default | `email` = \"unknown@example.com\" |\n",
    "| Invalid values | Filter out | `quantity` <= 0 |\n",
    "| Duplicates | Remove | Exact row duplicates |\n",
    "\n",
    "**RFM Analysis:**\n",
    "\n",
    "RFM (Recency, Frequency, Monetary) is a customer segmentation technique:\n",
    "- **Recency:** Days since last purchase (lower = better)\n",
    "- **Frequency:** Number of purchases (higher = better)\n",
    "- **Monetary:** Total spend (higher = better)\n",
    "\n",
    "---\n",
    "\n",
    "## Data Leakage Warning\n",
    "\n",
    "> **Critical Rule:** When calculating imputation statistics (mean, median) or any transformation parameters, use ONLY training data. Applying statistics calculated from test data leaks future information into the model.\n",
    "\n",
    "| Correct | Incorrect |\n",
    "|---------|-----------|\n",
    "| Calculate mean on train set | Calculate mean on full dataset |\n",
    "| Apply train mean to test set | Calculate separate mean for test |\n",
    "| Fit scaler on train only | Fit scaler on all data |\n",
    "\n",
    "In this workshop, we work with raw data before splitting. The Pipeline in Workshop 03 handles this correctly by fitting transformers only on training data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6ca404b-291b-4821-aa13-c9c75ae4652e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../demo/00_Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6184e0b5-dbcf-42d2-9033-0bf62791a867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6505e4e-f28a-438a-bbba-44924b03367c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Load Data\n",
    "df = spark.table(\"workshop_sales_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e6a618b-833e-46c8-86a0-a89ecb481d60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 2: Data Cleaning\n",
    "\n",
    "In the previous workshop, the following issues were identified:\n",
    "- Negative quantities (`quantity <= 0`)\n",
    "- Missing values (nulls)\n",
    "- Potential duplicates\n",
    "\n",
    "These must be removed or corrected to create a clean \"Silver\" layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7a3d7f2-8f20-4b05-9853-b90502cf8b3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, when, lit, max, datediff, current_date, to_date\n",
    "\n",
    "# Exercise 1: Filter invalid records\n",
    "# Remove rows where 'quantity' <= 0 OR 'total_amount' < 0\n",
    "# Save result to 'df_clean'\n",
    "# Compare row counts before and after\n",
    "\n",
    "df_clean = # TODO: Filter invalid records\n",
    "# print(f\"Original: {df.count()}, Clean: {df_clean.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16a92fe7-f144-4a09-ab0d-2e05ccbbaf84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 3: Handle Missing Values\n",
    "\n",
    "Strategy depends on the column type:\n",
    "- **Key IDs (customer_id, order_id):** If missing, record is unusable -> REMOVE\n",
    "- **Descriptive attributes (email, phone):** Can impute default value -> IMPUTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c013214-21d0-4fc6-b9cb-e5aaf9940b24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 2: Handle missing values\n",
    "# a) Remove rows where 'customer_id' OR 'order_id' is NULL\n",
    "# b) Fill missing 'email' values with \"unknown@example.com\"\n",
    "# Hint: Use .dropna() and .fillna()\n",
    "\n",
    "df_clean = # TODO: Handle nulls\n",
    "\n",
    "# print(f\"Count after null handling: {df_clean.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81e6084e-62dd-4075-be0a-4e1414c9a569",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 3: Deduplication\n",
    "# Check for and remove duplicate rows (exact duplicates)\n",
    "# Hint: Use .dropDuplicates()\n",
    "\n",
    "df_clean = # TODO: Remove duplicates\n",
    "# print(f\"Count after deduplication: {df_clean.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d04a340-327e-4b72-b1f9-2a1b2fb4f8aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Missing Flags (Informative Missingness)\n",
    "\n",
    "Sometimes the fact that data is missing is informative. For example:\n",
    "- A customer without email may be privacy-conscious\n",
    "- A missing phone number may indicate online-only customer\n",
    "\n",
    "Creating a \"missing flag\" allows the model to learn from these patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67611cf3-b8d3-4fe1-a357-3be3ff345e16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 3b (Optional): Create missing flag for email\n",
    "# Create a column 'email_missing' that is 1 if email was NULL, 0 otherwise\n",
    "# This should be done BEFORE filling nulls\n",
    "\n",
    "# TODO: Create missing flag\n",
    "# df_clean = df_clean.withColumn(\"email_missing\", when(col(\"email\").isNull(), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71a0b35d-c7db-4bfd-ab7c-7c3d1dc7421b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 4: Feature Engineering - Customer 360\n",
    "\n",
    "The goal is to predict customer behavior (segment). Transaction data must be aggregated to the customer level.\n",
    "\n",
    "**Planned features (RFM + Demographics):**\n",
    "\n",
    "| Feature | Type | Description |\n",
    "|---------|------|-------------|\n",
    "| `total_spend` | Monetary | Sum of all purchases |\n",
    "| `order_count` | Frequency | Number of orders |\n",
    "| `recency` | Recency | Days since last purchase |\n",
    "| `tenure` | Demographics | Days since registration |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59e9b0d1-1524-4d6b-bb8d-eee108de3831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, count, avg, min, max, datediff, lit\n",
    "\n",
    "# Helper: Find reference date (max date in dataset) for Recency calculation\n",
    "max_date = df_clean.select(max(\"order_datetime\")).collect()[0][0]\n",
    "print(f\"Reference Date: {max_date}\")\n",
    "\n",
    "# Exercise 4: Aggregate data\n",
    "# Group by 'customer_id', 'customer_segment', 'country', 'registration_date'\n",
    "# Calculate:\n",
    "# - total_spend (sum of total_amount)\n",
    "# - order_count (count of order_id)\n",
    "# - last_purchase_date (max of order_datetime)\n",
    "\n",
    "customer_features = df_clean.groupBy(\"customer_id\", \"customer_segment\", \"country\", \"registration_date\") \\\n",
    "    .agg(\n",
    "        # TODO: Add aggregations\n",
    "    )\n",
    "\n",
    "# Exercise 5: Calculate derived features\n",
    "# - recency: days between max_date and last_purchase_date\n",
    "# - tenure: days between last_purchase_date and registration_date\n",
    "\n",
    "customer_features = customer_features.withColumn(\"recency\", ...) \\\n",
    "                                     .withColumn(\"tenure\", ...)\n",
    "\n",
    "display(customer_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b52844f-4269-4a56-b0b8-384dcd95bb0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Encoding Categorical Features\n",
    "Machine Learning models require numerical input. We need to encode `country`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c07acc8-b141-42a8-b349-e3a1b798ff43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Exercise 6: Encode categorical features\n",
    "# Use StringIndexer to convert 'country' to 'country_index'\n",
    "\n",
    "indexer = # TODO: Create StringIndexer\n",
    "# customer_features_encoded = ...\n",
    "\n",
    "# display(customer_features_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "751fc31a-c173-4288-a960-85e788064cfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Save Feature Table\n",
    "Save this aggregated and cleaned dataset for the next workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5c788bc-df9f-42f4-ade8-3279922b33ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 7: Save the feature table as 'workshop_customer_features'\n",
    "\n",
    "# TODO: Save table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7e0cf2f-3e3d-458e-a61f-29b20b02e893",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Best Practices: Data Cleaning. \n",
    "\n",
    "| Practice | Description |\n",
    "|----------|-------------|\n",
    "| **Clean before split** | Remove invalid data before train/test split |\n",
    "| **Document decisions** | Record why you removed/imputed each column |\n",
    "| **Preserve information** | Create missing flags before filling nulls |\n",
    "| **Validate counts** | Check row counts after each cleaning step |\n",
    "| **Avoid data leakage** | Calculate statistics on training data only |\n",
    "\n",
    "---\n",
    "\n",
    "# Solutions\n",
    "\n",
    "Reference solutions for the exercises above.Compare your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1c4da98-4e7f-407a-a6fb-3e04c690e0e0",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764618354371}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = spark.table(\"workshop_sales_data\")\n",
    "\n",
    "from pyspark.sql.functions import max, datediff, lit, col, sum, count\n",
    "\n",
    "# 1. Filter\n",
    "df_clean = df.filter((col(\"quantity\") > 0) & (col(\"total_amount\") >= 0))\n",
    "\n",
    "# 2. Nulls & Duplicates\n",
    "df_clean = df_clean.dropna(subset=[\"customer_id\", \"order_id\",\"customer_segment\"]) \\\n",
    "                   .fillna({\"email\": \"unknown@example.com\"}) \\\n",
    "                   .dropDuplicates()\n",
    "\n",
    "# 3. Aggregation (RFM + Tenure)\n",
    "from pyspark.sql.functions import max, datediff, lit, col\n",
    "\n",
    "max_date_row = df_clean.select(max(\"order_datetime\")).collect()\n",
    "max_date = max_date_row[0][0]\n",
    "\n",
    "customer_features = df_clean.groupBy(\"customer_id\", \"customer_segment\", \"country\", \"registration_date\") \\\n",
    "    .agg(\n",
    "        sum(\"total_amount\").alias(\"total_spend\"),\n",
    "        count(\"order_id\").alias(\"order_count\"),\n",
    "        max(\"order_datetime\").alias(\"last_purchase_date\")\n",
    "    ) \\\n",
    "    .withColumn(\"recency\", datediff(lit(max_date), col(\"last_purchase_date\"))) \\\n",
    "    .withColumn(\"tenure\", datediff(col(\"last_purchase_date\"), col(\"registration_date\")))\n",
    "\n",
    "# 4. Encoding\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"country\", outputCol=\"country_index\", handleInvalid='keep')\n",
    "customer_features_encoded = indexer.fit(customer_features).transform(customer_features)\n",
    "\n",
    "# 5. Save\n",
    "customer_features_encoded.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"workshop_customer_features\")\n",
    "display(customer_features_encoded)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Workshop_Data_Cleaning_and_Features",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
