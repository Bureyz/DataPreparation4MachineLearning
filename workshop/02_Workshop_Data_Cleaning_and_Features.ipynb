{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc0b7aad",
   "metadata": {},
   "source": [
    "# Workshop 2: Data Cleaning and Feature Engineering\n",
    "\n",
    "## Objective\n",
    "\n",
    "Clean the RetailMax sales data and build customer-level features (Customer 360) for the ML model.\n",
    "\n",
    "## Context and Requirements\n",
    "\n",
    "- **Workshop:** Customer Segmentation for RetailMax\n",
    "- **Notebook type:** Hands-on Exercise\n",
    "- **Prerequisites:** `01_Workshop_Data_Exploration.ipynb` completed\n",
    "- **Technical requirements:**\n",
    "  - Databricks Runtime 14.x LTS or newer\n",
    "  - Unity Catalog enabled\n",
    "- **Execution time:** ~30 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Theoretical Background\n",
    "\n",
    "**Data Cleaning Strategy:**\n",
    "\n",
    "| Issue Type | Strategy | Example |\n",
    "|------------|----------|---------|\n",
    "| Missing key IDs | Remove row | `customer_id` is NULL |\n",
    "| Missing descriptive fields | Impute default | `email` = \"unknown@example.com\" |\n",
    "| Invalid values | Filter out | `quantity` <= 0 |\n",
    "| Duplicates | Remove | Exact row duplicates |\n",
    "\n",
    "**RFM Analysis:**\n",
    "\n",
    "RFM (Recency, Frequency, Monetary) is a customer segmentation technique:\n",
    "- **Recency:** Days since last purchase (lower = better)\n",
    "- **Frequency:** Number of purchases (higher = better)\n",
    "- **Monetary:** Total spend (higher = better)\n",
    "\n",
    "---\n",
    "\n",
    "## Data Leakage Warning\n",
    "\n",
    "> **Critical Rule:** When calculating imputation statistics (mean, median) or any transformation parameters, use ONLY training data. Applying statistics calculated from test data leaks future information into the model.\n",
    "\n",
    "| Correct | Incorrect |\n",
    "|---------|-----------|\n",
    "| Calculate mean on train set | Calculate mean on full dataset |\n",
    "| Apply train mean to test set | Calculate separate mean for test |\n",
    "| Fit scaler on train only | Fit scaler on all data |\n",
    "\n",
    "In this workshop, we work with raw data before splitting. The Pipeline in Workshop 03 handles this correctly by fitting transformers only on training data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f1c7b5",
   "metadata": {},
   "source": [
    "## Section 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5535db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Data\n",
    "df = spark.table(\"workshop_sales_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30ac1ba",
   "metadata": {},
   "source": [
    "## Section 2: Data Cleaning\n",
    "\n",
    "In the previous workshop, the following issues were identified:\n",
    "- Negative quantities (`quantity <= 0`)\n",
    "- Missing values (nulls)\n",
    "- Potential duplicates\n",
    "\n",
    "These must be removed or corrected to create a clean \"Silver\" layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72fd0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, when, lit, max, datediff, current_date, to_date\n",
    "\n",
    "# Exercise 1: Filter invalid records\n",
    "# Remove rows where 'quantity' <= 0 OR 'total_amount' < 0\n",
    "# Save result to 'df_clean'\n",
    "# Compare row counts before and after\n",
    "\n",
    "df_clean = # TODO: Filter invalid records\n",
    "# print(f\"Original: {df.count()}, Clean: {df_clean.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea27057",
   "metadata": {},
   "source": [
    "## Section 3: Handle Missing Values\n",
    "\n",
    "Strategy depends on the column type:\n",
    "- **Key IDs (customer_id, order_id):** If missing, record is unusable -> REMOVE\n",
    "- **Descriptive attributes (email, phone):** Can impute default value -> IMPUTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661315ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Handle missing values\n",
    "# a) Remove rows where 'customer_id' OR 'order_id' is NULL\n",
    "# b) Fill missing 'email' values with \"unknown@example.com\"\n",
    "# Hint: Use .dropna() and .fillna()\n",
    "\n",
    "df_clean = # TODO: Handle nulls\n",
    "\n",
    "# print(f\"Count after null handling: {df_clean.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7509b7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Deduplication\n",
    "# Check for and remove duplicate rows (exact duplicates)\n",
    "# Hint: Use .dropDuplicates()\n",
    "\n",
    "df_clean = # TODO: Remove duplicates\n",
    "# print(f\"Count after deduplication: {df_clean.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e0437a",
   "metadata": {},
   "source": [
    "### Missing Flags (Informative Missingness)\n",
    "\n",
    "Sometimes the fact that data is missing is informative. For example:\n",
    "- A customer without email may be privacy-conscious\n",
    "- A missing phone number may indicate online-only customer\n",
    "\n",
    "Creating a \"missing flag\" allows the model to learn from these patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb8dc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3b (Optional): Create missing flag for email\n",
    "# Create a column 'email_missing' that is 1 if email was NULL, 0 otherwise\n",
    "# This should be done BEFORE filling nulls\n",
    "\n",
    "# TODO: Create missing flag\n",
    "# df_clean = df_clean.withColumn(\"email_missing\", when(col(\"email\").isNull(), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c543d6d",
   "metadata": {},
   "source": [
    "## Section 4: Feature Engineering - Customer 360\n",
    "\n",
    "The goal is to predict customer behavior (segment). Transaction data must be aggregated to the customer level.\n",
    "\n",
    "**Planned features (RFM + Demographics):**\n",
    "\n",
    "| Feature | Type | Description |\n",
    "|---------|------|-------------|\n",
    "| `total_spend` | Monetary | Sum of all purchases |\n",
    "| `order_count` | Frequency | Number of orders |\n",
    "| `recency` | Recency | Days since last purchase |\n",
    "| `tenure` | Demographics | Days since registration |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b33d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, count, avg, min, max, datediff, lit\n",
    "\n",
    "# Helper: Find reference date (max date in dataset) for Recency calculation\n",
    "max_date = df_clean.select(max(\"order_datetime\")).collect()[0][0]\n",
    "print(f\"Reference Date: {max_date}\")\n",
    "\n",
    "# Exercise 4: Aggregate data\n",
    "# Group by 'customer_id', 'customer_segment', 'country', 'registration_date'\n",
    "# Calculate:\n",
    "# - total_spend (sum of total_amount)\n",
    "# - order_count (count of order_id)\n",
    "# - last_purchase_date (max of order_datetime)\n",
    "\n",
    "customer_features = df_clean.groupBy(\"customer_id\", \"customer_segment\", \"country\", \"registration_date\") \\\n",
    "    .agg(\n",
    "        # TODO: Add aggregations\n",
    "    )\n",
    "\n",
    "# Exercise 5: Calculate derived features\n",
    "# - recency: days between max_date and last_purchase_date\n",
    "# - tenure: days between last_purchase_date and registration_date\n",
    "\n",
    "customer_features = customer_features.withColumn(\"recency\", ...) \\\n",
    "                                     .withColumn(\"tenure\", ...)\n",
    "\n",
    "display(customer_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ba11c4",
   "metadata": {},
   "source": [
    "## 5. Encoding Categorical Features\n",
    "Machine Learning models require numerical input. We need to encode `country`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4be7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Exercise 6: Encode categorical features\n",
    "# Use StringIndexer to convert 'country' to 'country_index'\n",
    "\n",
    "indexer = # TODO: Create StringIndexer\n",
    "# customer_features_encoded = ...\n",
    "\n",
    "# display(customer_features_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c605cff9",
   "metadata": {},
   "source": [
    "## 6. Save Feature Table\n",
    "Save this aggregated and cleaned dataset for the next workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2cd620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7: Save the feature table as 'workshop_customer_features'\n",
    "\n",
    "# TODO: Save table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7d316d",
   "metadata": {},
   "source": [
    "## Best Practices: Data Cleaning\n",
    "\n",
    "| Practice | Description |\n",
    "|----------|-------------|\n",
    "| **Clean before split** | Remove invalid data before train/test split |\n",
    "| **Document decisions** | Record why you removed/imputed each column |\n",
    "| **Preserve information** | Create missing flags before filling nulls |\n",
    "| **Validate counts** | Check row counts after each cleaning step |\n",
    "| **Avoid data leakage** | Calculate statistics on training data only |\n",
    "\n",
    "---\n",
    "\n",
    "# Solutions\n",
    "\n",
    "Reference solutions for the exercises above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dd18d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filter\n",
    "df_clean = df.filter((col(\"quantity\") > 0) & (col(\"total_amount\") >= 0))\n",
    "\n",
    "# 2. Nulls & Duplicates\n",
    "df_clean = df_clean.dropna(subset=[\"customer_id\", \"order_id\"]) \\\n",
    "                   .fillna({\"email\": \"unknown@example.com\"}) \\\n",
    "                   .dropDuplicates()\n",
    "\n",
    "# 3. Aggregation (RFM + Tenure)\n",
    "from pyspark.sql.functions import max, datediff, lit\n",
    "\n",
    "max_date_row = df_clean.select(max(\"order_datetime\")).collect()\n",
    "max_date = max_date_row[0][0]\n",
    "\n",
    "customer_features = df_clean.groupBy(\"customer_id\", \"customer_segment\", \"country\", \"registration_date\") \\\n",
    "    .agg(\n",
    "        sum(\"total_amount\").alias(\"total_spend\"),\n",
    "        count(\"order_id\").alias(\"order_count\"),\n",
    "        max(\"order_datetime\").alias(\"last_purchase_date\")\n",
    "    ) \\\n",
    "    .withColumn(\"recency\", datediff(lit(max_date), col(\"last_purchase_date\"))) \\\n",
    "    .withColumn(\"tenure\", datediff(col(\"last_purchase_date\"), col(\"registration_date\")))\n",
    "\n",
    "# 4. Encoding\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"country\", outputCol=\"country_index\")\n",
    "customer_features_encoded = indexer.fit(customer_features).transform(customer_features)\n",
    "\n",
    "# 5. Save\n",
    "customer_features_encoded.write.mode(\"overwrite\").saveAsTable(\"workshop_customer_features\")\n",
    "display(customer_features_encoded)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
