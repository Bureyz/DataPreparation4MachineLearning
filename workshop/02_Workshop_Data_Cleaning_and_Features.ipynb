{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "091b4f2a-6121-478a-8647-3de7fb1679a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Workshop 2: Data Cleaning and Feature Engineering\n",
    "\n",
    "## Business Context: From Raw Transactions to Customer Intelligence\n",
    "\n",
    "In Workshop 1, we discovered our data has problems: missing values, negative quantities, and mathematical inconsistencies. The marketing team can't use this data yet.\n",
    "\n",
    "**The challenge:** Raw transaction data shows *what* happened, but marketing needs to know *who* the customer is. They need a **Customer 360** view - a single row per customer with their complete profile.\n",
    "\n",
    "---\n",
    "\n",
    "## What We're Building: Customer 360\n",
    "\n",
    "**Input:** 10,000+ individual transactions\n",
    "```\n",
    "order_id | customer_id | product | quantity | amount | date\n",
    "1001     | C123        | Widget  | 2        | 50.00  | 2024-01-15\n",
    "1002     | C123        | Gadget  | 1        | 30.00  | 2024-01-20\n",
    "...\n",
    "```\n",
    "\n",
    "**Output:** One row per customer with aggregated features\n",
    "```\n",
    "customer_id | total_spend | order_count | recency | tenure | segment\n",
    "C123        | 580.00      | 8           | 15      | 365    | Premium\n",
    "```\n",
    "\n",
    "**This enables:**\n",
    "- Marketing to identify who is a Premium customer\n",
    "- ML model to learn patterns: \"What makes someone Premium?\"\n",
    "- Future predictions: \"Will this new customer become Premium?\"\n",
    "\n",
    "---\n",
    "\n",
    "## RFM Analysis: The Marketing Standard\n",
    "\n",
    "We'll use **RFM (Recency, Frequency, Monetary)** - a proven customer segmentation technique:\n",
    "\n",
    "| Metric | Definition | Why It Matters |\n",
    "|--------|------------|----------------|\n",
    "| **Recency** | Days since last purchase | Recent buyers are more likely to buy again |\n",
    "| **Frequency** | Number of orders | Frequent buyers are loyal |\n",
    "| **Monetary** | Total spend | High spenders are valuable |\n",
    "\n",
    "**Example insights:**\n",
    "- Customer with low recency (5 days) + high frequency (20 orders) = Loyal, Active → Premium\n",
    "- Customer with high recency (180 days) + low frequency (1 order) = At risk of churn → Basic\n",
    "\n",
    "---\n",
    "\n",
    "## Context and Requirements\n",
    "\n",
    "- **Workshop:** Customer Segmentation for RetailMax\n",
    "- **Notebook type:** Hands-on Exercise\n",
    "- **Prerequisites:** `01_Workshop_Data_Exploration.ipynb` completed\n",
    "- **Technical requirements:**\n",
    "  - Databricks Runtime 14.x LTS or newer\n",
    "  - Unity Catalog enabled\n",
    "- **Execution time:** ~30 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Data Cleaning Strategy\n",
    "\n",
    "| Issue Type | Strategy | Business Reason |\n",
    "|------------|----------|-----------------|\n",
    "| Missing key IDs | Remove row | Can't identify customer → useless |\n",
    "| Missing descriptive fields | Impute default | Email unknown but customer is real |\n",
    "| Invalid values | Filter out | Negative quantity = data error |\n",
    "| Duplicates | Remove | Avoids double-counting spend |\n",
    "\n",
    "---\n",
    "\n",
    "## Data Leakage Warning\n",
    "\n",
    "> **Critical Rule:** When calculating imputation statistics (mean, median) or any transformation parameters, use ONLY training data. Applying statistics calculated from test data leaks future information into the model.\n",
    "\n",
    "| Correct | Incorrect |\n",
    "|---------|-----------|\n",
    "| Calculate mean on train set | Calculate mean on full dataset |\n",
    "| Apply train mean to test set | Calculate separate mean for test |\n",
    "| Fit scaler on train only | Fit scaler on all data |\n",
    "\n",
    "In this workshop, we work with raw data before splitting. The Pipeline in Workshop 03 handles this correctly by fitting transformers only on training data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6ca404b-291b-4821-aa13-c9c75ae4652e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../demo/00_Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6184e0b5-dbcf-42d2-9033-0bf62791a867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 1: Load Data\n",
    "\n",
    "**Current state:** Raw transaction data with issues identified in Workshop 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6505e4e-f28a-438a-bbba-44924b03367c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Load Data\n",
    "df = spark.table(\"workshop_sales_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e6a618b-833e-46c8-86a0-a89ecb481d60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 2: Data Cleaning\n",
    "\n",
    "**Business context:** We can't build Customer 360 on dirty data. First, we clean.\n",
    "\n",
    "**Issues found in Workshop 1:**\n",
    "- ❌ Negative quantities (`quantity <= 0`) - likely returns or errors\n",
    "- ❌ Missing customer IDs - can't aggregate without identity\n",
    "- ❌ Duplicate transactions - would inflate spending totals\n",
    "\n",
    "**Goal:** Create a \"Silver\" layer - clean, validated data ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7a3d7f2-8f20-4b05-9853-b90502cf8b3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, when, lit, max, datediff, current_date, to_date\n",
    "\n",
    "# Exercise 1: Filter invalid records\n",
    "# Remove rows where 'quantity' <= 0 OR 'total_amount' < 0\n",
    "# Save result to 'df_clean'\n",
    "# Compare row counts before and after\n",
    "\n",
    "df_clean = # TODO: Filter invalid records\n",
    "# print(f\"Original: {df.count()}, Clean: {df_clean.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16a92fe7-f144-4a09-ab0d-2e05ccbbaf84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 3: Handle Missing Values\n",
    "\n",
    "**Business context:** Not all missing data is equal. Strategy depends on business impact:\n",
    "\n",
    "| Column | If Missing | Business Decision |\n",
    "|--------|------------|-------------------|\n",
    "| `customer_id` | Can't identify customer | DELETE - unusable |\n",
    "| `order_id` | Can't track transaction | DELETE - unusable |\n",
    "| `email` | Customer prefers privacy | IMPUTE - customer is still real |\n",
    "\n",
    "**Key insight:** Deleting rows loses data. Only delete when absolutely necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c013214-21d0-4fc6-b9cb-e5aaf9940b24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 2: Handle missing values\n",
    "# a) Remove rows where 'customer_id' OR 'order_id' is NULL\n",
    "# b) Fill missing 'email' values with \"unknown@example.com\"\n",
    "# Hint: Use .dropna() and .fillna()\n",
    "\n",
    "df_clean = # TODO: Handle nulls\n",
    "\n",
    "# print(f\"Count after null handling: {df_clean.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81e6084e-62dd-4075-be0a-4e1414c9a569",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 3: Deduplication\n",
    "# Check for and remove duplicate rows (exact duplicates)\n",
    "# Hint: Use .dropDuplicates()\n",
    "\n",
    "df_clean = # TODO: Remove duplicates\n",
    "# print(f\"Count after deduplication: {df_clean.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d04a340-327e-4b72-b1f9-2a1b2fb4f8aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Missing Flags (Informative Missingness)\n",
    "\n",
    "Sometimes the fact that data is missing is informative. For example:\n",
    "- A customer without email may be privacy-conscious\n",
    "- A missing phone number may indicate online-only customer\n",
    "\n",
    "Creating a \"missing flag\" allows the model to learn from these patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67611cf3-b8d3-4fe1-a357-3be3ff345e16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 3b (Optional): Create missing flag for email\n",
    "# Create a column 'email_missing' that is 1 if email was NULL, 0 otherwise\n",
    "# This should be done BEFORE filling nulls\n",
    "\n",
    "# TODO: Create missing flag\n",
    "# df_clean = df_clean.withColumn(\"email_missing\", when(col(\"email\").isNull(), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71a0b35d-c7db-4bfd-ab7c-7c3d1dc7421b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 4: Feature Engineering - Customer 360\n",
    "\n",
    "**This is the core transformation.** We convert many transactions → one customer profile.\n",
    "\n",
    "**Why?**\n",
    "- ML models need one row per prediction target\n",
    "- We predict segment *per customer*, not per transaction\n",
    "- Marketing acts on *customers*, not individual orders\n",
    "\n",
    "**Planned features (RFM + Demographics):**\n",
    "\n",
    "| Feature | Type | Formula | Business Meaning |\n",
    "|---------|------|---------|------------------|\n",
    "| `total_spend` | Monetary | SUM(total_amount) | Customer lifetime value |\n",
    "| `order_count` | Frequency | COUNT(orders) | Purchase frequency |\n",
    "| `recency` | Recency | Days since last purchase | Engagement level |\n",
    "| `tenure` | Demographics | Days since registration | Customer maturity |\n",
    "\n",
    "**Example aggregation:**\n",
    "```\n",
    "Customer C123: \n",
    "  - Order 1: $50 on Jan 15\n",
    "  - Order 2: $30 on Jan 20\n",
    "  - Order 3: $100 on Feb 1 (last purchase)\n",
    "  \n",
    "→ total_spend = $180, order_count = 3, recency = 30 days (if today is March 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59e9b0d1-1524-4d6b-bb8d-eee108de3831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, count, avg, min, max, datediff, lit\n",
    "\n",
    "# Helper: Find reference date (max date in dataset) for Recency calculation\n",
    "max_date = df_clean.select(max(\"order_datetime\")).collect()[0][0]\n",
    "print(f\"Reference Date: {max_date}\")\n",
    "\n",
    "# Exercise 4: Aggregate data\n",
    "# Group by 'customer_id', 'customer_segment', 'country', 'registration_date'\n",
    "# Calculate:\n",
    "# - total_spend (sum of total_amount)\n",
    "# - order_count (count of order_id)\n",
    "# - last_purchase_date (max of order_datetime)\n",
    "\n",
    "customer_features = df_clean.groupBy(\"customer_id\", \"customer_segment\", \"country\", \"registration_date\") \\\n",
    "    .agg(\n",
    "        # TODO: Add aggregations\n",
    "    )\n",
    "\n",
    "# Exercise 5: Calculate derived features\n",
    "# - recency: days between max_date and last_purchase_date\n",
    "# - tenure: days between last_purchase_date and registration_date\n",
    "\n",
    "customer_features = customer_features.withColumn(\"recency\", ...) \\\n",
    "                                     .withColumn(\"tenure\", ...)\n",
    "\n",
    "display(customer_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b52844f-4269-4a56-b0b8-384dcd95bb0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 5: Encoding Categorical Features\n",
    "\n",
    "**Business context:** Countries are text (`USA`, `Germany`), but math requires numbers.\n",
    "\n",
    "**Why StringIndexer?**\n",
    "- Converts text → number: `USA` → 0, `Germany` → 1, `UK` → 2\n",
    "- ML model can now use country as a feature\n",
    "- Preserves information: different countries → different numbers\n",
    "\n",
    "**Alternative approaches:**\n",
    "- **OneHotEncoder:** USA → [1,0,0], Germany → [0,1,0] - better for some models\n",
    "- **TargetEncoder:** Replace with segment probability - advanced technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c07acc8-b141-42a8-b349-e3a1b798ff43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Exercise 6: Encode categorical features\n",
    "# Use StringIndexer to convert 'country' to 'country_index'\n",
    "\n",
    "indexer = # TODO: Create StringIndexer\n",
    "# customer_features_encoded = ...\n",
    "\n",
    "# display(customer_features_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "751fc31a-c173-4288-a960-85e788064cfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 6: Save Feature Table\n",
    "\n",
    "**Business context:** This Customer 360 table is our \"Gold\" layer - ready for ML.\n",
    "\n",
    "**What we're saving:**\n",
    "- One row per customer\n",
    "- Clean, validated data\n",
    "- RFM features calculated\n",
    "- Categorical features encoded\n",
    "\n",
    "**Next step:** Workshop 3 will use this table to train the ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5c788bc-df9f-42f4-ade8-3279922b33ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 7: Save the feature table as 'workshop_customer_features'\n",
    "\n",
    "# TODO: Save table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7e0cf2f-3e3d-458e-a61f-29b20b02e893",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Best Practices: Data Cleaning. \n",
    "\n",
    "| Practice | Description |\n",
    "|----------|-------------|\n",
    "| **Clean before split** | Remove invalid data before train/test split |\n",
    "| **Document decisions** | Record why you removed/imputed each column |\n",
    "| **Preserve information** | Create missing flags before filling nulls |\n",
    "| **Validate counts** | Check row counts after each cleaning step |\n",
    "| **Avoid data leakage** | Calculate statistics on training data only |\n",
    "\n",
    "---\n",
    "\n",
    "# Solutions\n",
    "\n",
    "Reference solutions for the exercises above.Compare your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1c4da98-4e7f-407a-a6fb-3e04c690e0e0",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764618354371}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = spark.table(\"workshop_sales_data\")\n",
    "\n",
    "from pyspark.sql.functions import max, datediff, lit, col, sum, count\n",
    "\n",
    "# 1. Filter\n",
    "df_clean = df.filter((col(\"quantity\") > 0) & (col(\"total_amount\") >= 0))\n",
    "\n",
    "# 2. Nulls & Duplicates\n",
    "df_clean = df_clean.dropna(subset=[\"customer_id\", \"order_id\",\"customer_segment\"]) \\\n",
    "                   .fillna({\"email\": \"unknown@example.com\"}) \\\n",
    "                   .dropDuplicates()\n",
    "\n",
    "# 3. Aggregation (RFM + Tenure)\n",
    "from pyspark.sql.functions import max, datediff, lit, col\n",
    "\n",
    "max_date_row = df_clean.select(max(\"order_datetime\")).collect()\n",
    "max_date = max_date_row[0][0]\n",
    "\n",
    "customer_features = df_clean.groupBy(\"customer_id\", \"customer_segment\", \"country\", \"registration_date\") \\\n",
    "    .agg(\n",
    "        sum(\"total_amount\").alias(\"total_spend\"),\n",
    "        count(\"order_id\").alias(\"order_count\"),\n",
    "        max(\"order_datetime\").alias(\"last_purchase_date\")\n",
    "    ) \\\n",
    "    .withColumn(\"recency\", datediff(lit(max_date), col(\"last_purchase_date\"))) \\\n",
    "    .withColumn(\"tenure\", datediff(col(\"last_purchase_date\"), col(\"registration_date\")))\n",
    "\n",
    "# 4. Encoding\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"country\", outputCol=\"country_index\", handleInvalid='keep')\n",
    "customer_features_encoded = indexer.fit(customer_features).transform(customer_features)\n",
    "\n",
    "# 5. Save\n",
    "customer_features_encoded.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"workshop_customer_features\")\n",
    "display(customer_features_encoded)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Workshop_Data_Cleaning_and_Features",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
