{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77af5a47",
   "metadata": {},
   "source": [
    "# Workshop: Customer Segmentation for RetailMax\n",
    "\n",
    "## Business Context\n",
    "\n",
    "**Company:** RetailMax - a multi-category retail chain operating in the USA  \n",
    "**Role:** Data Scientist in the Customer Analytics team  \n",
    "**Scenario:** January 2025. The company has completed a record sales year, but leadership has identified an issue with marketing efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## Business Problem\n",
    "\n",
    "RetailMax has 10,000 customers and 100,000 transactions, but currently treats all customers identically. The same promotions are sent to high-value customers and one-time buyers, resulting in inefficient marketing spend.\n",
    "\n",
    "**Objective:** Build an ML model that automatically classifies customers into segments (Basic, Standard, Premium) to enable personalized marketing campaigns.\n",
    "\n",
    "---\n",
    "\n",
    "## Available Data\n",
    "\n",
    "| Source | Description | Size | Known Issues |\n",
    "|--------|-------------|------|--------------|\n",
    "| `customers.csv` | Customer data (name, email, location, segment) | 10,000 | Missing values, duplicate IDs |\n",
    "| `products.csv` | Product catalog (name, brand, price) | 2,000 | Negative prices, missing names |\n",
    "| `orders_batch.json` | Order history | 100,000 | Null IDs, negative quantities, future dates |\n",
    "\n",
    "**Data Quality Note:** The data comes directly from transactional systems and contains quality issues typical of real-world data. Returns are recorded as negative quantities, and some orders lack customer IDs.\n",
    "\n",
    "---\n",
    "\n",
    "## Workshop Roadmap\n",
    "\n",
    "| # | Notebook | Objective | Deliverable |\n",
    "|---|----------|-----------|-------------|\n",
    "| **0** | Setup | Environment and data preparation | Bronze Tables |\n",
    "| **1** | Data Exploration | Data understanding, issue identification | EDA Report |\n",
    "| **2** | Cleaning & Features | Data cleaning + Customer 360 | Feature Table |\n",
    "| **3** | ML Pipeline | Classification model + MLflow | Deployed Model |\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this workshop, participants will be able to:\n",
    "- Perform exploratory data analysis (EDA) on real-world datasets\n",
    "- Identify and resolve data quality issues\n",
    "- Build customer-level features (RFM analysis)\n",
    "- Create reproducible ML pipelines in Spark\n",
    "- Track experiments using MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a1a7fa",
   "metadata": {},
   "source": [
    "## Context and Requirements\n",
    "\n",
    "- **Workshop:** Customer Segmentation for RetailMax\n",
    "- **Notebook type:** Setup (run first!)\n",
    "- **Technical requirements:**\n",
    "  - Databricks Runtime 14.x LTS or newer\n",
    "  - Unity Catalog enabled\n",
    "  - Catalog `data_ml_preparation` must exist (created by instructor)\n",
    "  - Permissions: CREATE SCHEMA, CREATE VOLUME, CREATE TABLE\n",
    "- **Execution time:** ~3 minutes (includes data download)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f852bd",
   "metadata": {},
   "source": [
    "## Section 1: User Detection & Environment Setup\n",
    "\n",
    "Automatic detection of current user and unique schema creation for isolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f8fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current user and create isolated schema\n",
    "current_user_email = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "username = current_user_email.split(\"@\")[0].replace(\".\", \"_\").replace(\"-\", \"_\")\n",
    "\n",
    "# Configuration - SAME as Demo notebooks for consistency\n",
    "catalog_name = \"data_ml_preparation\"  # Must exist (created by instructor)\n",
    "schema_name = f\"ml_dp_{username}\"\n",
    "\n",
    "print(f\"Detected user: {current_user_email}\")\n",
    "print(f\"Username for schema: {username}\")\n",
    "print(f\"Target: {catalog_name}.{schema_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56177ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set catalog and create schema (catalog must exist - created by instructor)\n",
    "try:\n",
    "    spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n",
    "    spark.sql(f\"USE SCHEMA {schema_name}\")\n",
    "    print(f\"Environment configured: {catalog_name}.{schema_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Fallback to hive_metastore...\")\n",
    "    catalog_name = \"hive_metastore\"\n",
    "    spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n",
    "    spark.sql(f\"USE SCHEMA {schema_name}\")\n",
    "    print(f\"Using: {catalog_name}.{schema_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e16924",
   "metadata": {},
   "source": [
    "## Section 2: Create Volume and Download Data\n",
    "\n",
    "Create a Unity Catalog Volume to store raw data files, then download the dataset from GitHub repository.\n",
    "\n",
    "**Volume:** Managed storage location in Unity Catalog for files (CSV, JSON, Parquet, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0965d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Volume for raw data storage\n",
    "volume_name = \"raw_data\"\n",
    "\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {volume_name}\")\n",
    "volume_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}\"\n",
    "\n",
    "print(f\"Volume created: {volume_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5955e586",
   "metadata": {},
   "source": [
    "### 2.1 Download Data from GitHub Repository\n",
    "\n",
    "Download the training dataset files from the course repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317c27df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# GitHub raw content URLs\n",
    "repo_base = \"https://raw.githubusercontent.com/Bureyz/DataPreparation4MachineLearning/main/dataset\"\n",
    "\n",
    "files_to_download = [\n",
    "    (\"customers/customers.csv\", \"customers.csv\"),\n",
    "    (\"products/csv/products.csv\", \"products.csv\"),\n",
    "    (\"orders/orders_batch.json\", \"orders_batch.json\")\n",
    "]\n",
    "\n",
    "# Download files to Volume\n",
    "for remote_path, local_name in files_to_download:\n",
    "    url = f\"{repo_base}/{remote_path}\"\n",
    "    local_path = f\"{volume_path}/{local_name}\"\n",
    "    \n",
    "    try:\n",
    "        # Download using dbutils\n",
    "        response = urllib.request.urlopen(url)\n",
    "        content = response.read()\n",
    "        \n",
    "        # Write to volume using dbutils\n",
    "        dbutils.fs.put(local_path, content.decode('utf-8'), overwrite=True)\n",
    "        print(f\"Downloaded: {local_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {local_name}: {e}\")\n",
    "\n",
    "print(f\"\\nFiles in volume:\")\n",
    "display(dbutils.fs.ls(volume_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e1d8d5",
   "metadata": {},
   "source": [
    "## Section 3: Load Raw Data (Bronze Layer)\n",
    "\n",
    "Load data from Volume into Bronze tables.\n",
    "Bronze layer contains raw data exactly as received from source systems.\n",
    "\n",
    "**Note:** The data intentionally contains quality issues that will be identified and resolved in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b04360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Customers from Volume (CRM System - 10,000 records)\n",
    "customers_path = f\"{volume_path}/customers.csv\"\n",
    "\n",
    "df_customers = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(customers_path)\n",
    "\n",
    "print(f\"Loaded {df_customers.count()} customers\")\n",
    "display(df_customers.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545505ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Products from Volume (Product Catalog - 2,000 SKUs)\n",
    "products_path = f\"{volume_path}/products.csv\"\n",
    "\n",
    "df_products = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(products_path)\n",
    "\n",
    "print(f\"Loaded {df_products.count()} products\")\n",
    "display(df_products.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fe77e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Orders from Volume (POS/E-commerce - 100,000 transactions)\n",
    "orders_path = f\"{volume_path}/orders_batch.json\"\n",
    "\n",
    "df_orders = spark.read.format(\"json\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(orders_path)\n",
    "\n",
    "print(f\"Loaded {df_orders.count()} orders\")\n",
    "display(df_orders.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7215f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Bronze Tables\n",
    "df_customers.write.mode(\"overwrite\").saveAsTable(\"customers_bronze\")\n",
    "df_products.write.mode(\"overwrite\").saveAsTable(\"products_bronze\")\n",
    "df_orders.write.mode(\"overwrite\").saveAsTable(\"orders_bronze\")\n",
    "\n",
    "print(\"Bronze tables created: customers_bronze, products_bronze, orders_bronze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97ba999",
   "metadata": {},
   "source": [
    "## Section 4: Create Unified Dataset\n",
    "\n",
    "Join Orders with Customers and Products to create a unified sales dataset.\n",
    "This dataset will be the starting point for EDA and cleaning in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9923123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Join Orders with Customers and Products\n",
    "# Note: This dataset contains data quality issues (nulls, negatives) to be cleaned in the workshop\n",
    "\n",
    "df_joined = df_orders.alias(\"o\") \\\n",
    "    .join(df_customers.alias(\"c\"), col(\"o.customer_id\") == col(\"c.customer_id\"), \"left\") \\\n",
    "    .join(df_products.alias(\"p\"), col(\"o.product_id\") == col(\"p.product_id\"), \"left\") \\\n",
    "    .select(\n",
    "        col(\"o.order_id\"),\n",
    "        col(\"o.order_datetime\"),\n",
    "        col(\"o.customer_id\"),\n",
    "        col(\"c.first_name\"),\n",
    "        col(\"c.last_name\"),\n",
    "        col(\"c.email\"),\n",
    "        col(\"c.country\"),\n",
    "        col(\"c.registration_date\"),\n",
    "        col(\"c.customer_segment\"),\n",
    "        col(\"o.product_id\"),\n",
    "        col(\"p.product_name\"),\n",
    "        col(\"p.brand\"),\n",
    "        col(\"p.unit_cost\"),\n",
    "        col(\"o.unit_price\").alias(\"sales_price\"),\n",
    "        col(\"o.quantity\"),\n",
    "        col(\"o.total_amount\"),\n",
    "        col(\"o.payment_method\")\n",
    "    )\n",
    "\n",
    "# Save as the starting point for the workshop\n",
    "df_joined.write.mode(\"overwrite\").saveAsTable(\"workshop_sales_data\")\n",
    "\n",
    "print(f\"Unified dataset created: workshop_sales_data ({df_joined.count()} rows)\")\n",
    "display(df_joined.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36625baf",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Setup complete. The following resources have been created:\n",
    "\n",
    "| Resource | Name | Description |\n",
    "|----------|------|-------------|\n",
    "| **Schema** | `ml_dp_{username}` | Per-user isolated schema |\n",
    "| **Volume** | `raw_data` | Storage for raw data files |\n",
    "| **Table** | `customers_bronze` | Raw customer data |\n",
    "| **Table** | `products_bronze` | Raw product catalog |\n",
    "| **Table** | `orders_bronze` | Raw order transactions |\n",
    "| **Table** | `workshop_sales_data` | Unified sales dataset |\n",
    "\n",
    "**Volume Path:** `/Volumes/{catalog}/{schema}/raw_data/`\n",
    "\n",
    "**Next Step:** Proceed to `01_Workshop_Data_Exploration.ipynb` to analyze data quality."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
