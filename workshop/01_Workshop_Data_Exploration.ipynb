{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be79e302-02ed-4c0d-bffd-9e700d4b834c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Workshop 1: Exploratory Data Analysis (EDA)\n",
    "\n",
    "## Business Context: The Marketing Challenge\n",
    "\n",
    "RetailMax's CMO has a problem: **marketing campaigns have a 2% conversion rate**, far below the industry average of 5%. The reason? All 10,000 customers receive the same generic promotions.\n",
    "\n",
    "**Your mission:** Build an ML model that automatically classifies customers into segments (Basic, Standard, Premium) so marketing can send targeted campaigns.\n",
    "\n",
    "But before we can build any model, we need to **understand our data**. This is where EDA comes in.\n",
    "\n",
    "---\n",
    "\n",
    "## What We're Doing and Why\n",
    "\n",
    "| Step | Business Question | Technical Task |\n",
    "|------|------------------|----------------|\n",
    "| **1. Load Data** | What data do we have? | Load and preview tables |\n",
    "| **2. Profile Data** | Is the data usable? | Check types, stats, distributions |\n",
    "| **3. Find Issues** | What's broken? | Identify nulls, errors, inconsistencies |\n",
    "| **4. Analyze Distributions** | Are customer segments balanced? | Check class distribution |\n",
    "\n",
    "**Why EDA matters:**\n",
    "- A model trained on dirty data will make dirty predictions\n",
    "- \"Garbage In, Garbage Out\" - even the best algorithm can't fix bad data\n",
    "- Issues found now save weeks of debugging later\n",
    "\n",
    "---\n",
    "\n",
    "## Context and Requirements\n",
    "\n",
    "- **Workshop:** Customer Segmentation for RetailMax\n",
    "- **Notebook type:** Hands-on Exercise\n",
    "- **Prerequisites:** `00_Workshop_Setup.ipynb` completed\n",
    "- **Technical requirements:**\n",
    "  - Databricks Runtime 14.x LTS or newer\n",
    "  - Unity Catalog enabled\n",
    "- **Execution time:** ~30 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Theoretical Background\n",
    "\n",
    "**EDA helps answer:**\n",
    "\n",
    "| Aspect | Question | Impact on ML |\n",
    "|--------|----------|--------------|\n",
    "| **Data Quality** | Missing values? Duplicates? Invalid values? | Requires imputation or removal |\n",
    "| **Distribution** | Normal or skewed? | Affects model choice and scaling |\n",
    "| **Outliers** | Extreme values present? | Can distort linear models |\n",
    "| **Correlations** | Which features are related? | Informs feature selection |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6fc102e-84ae-4347-85f5-bbfa3369fba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../demo/00_Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ca19894-6627-4398-85e6-d156f81cbb1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 1: Load Data\n",
    "\n",
    "**Business context:** We have one year of RetailMax sales transactions. Each row represents a single purchase - who bought what, when, and for how much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33cd03ae-f3c8-4a3a-b7e5-614c00b27720",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 1: Load Data\n",
    "# Load the 'workshop_sales_data' table into a DataFrame named 'df'.\n",
    "# Display the row count and first 5 records to verify the data loaded correctly.\n",
    "\n",
    "df = spark.table(\"workshop_sales_data\")\n",
    "print(f\"Rows: {df.count()}\")\n",
    "display(df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb32d2c6-34a2-4f6e-8077-d3759df4c8bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 1: Load Data\n",
    "# Load the 'workshop_sales_data' table into a DataFrame named 'df'.\n",
    "# Display the row count and first 5 records to verify the data loaded correctly.\n",
    "\n",
    "df = # TODO: Load table\n",
    "# print(...)\n",
    "# display(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dda275e-2e76-4c69-86cc-dd69b0ac25c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 2: Data Profiling\n",
    "\n",
    "**Business context:** Before marketing trusts our model, we need to trust our data. Profiling answers: \"Can we use this data, or is it full of errors?\"\n",
    "\n",
    "**What to check:**\n",
    "1. **Data types:** Are dates stored as dates? Are numbers numeric?\n",
    "2. **Statistics:** Do mean values make sense? Are there outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "899e4053-802c-4295-8944-3844c3d547bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 2: Display the data schema\n",
    "# Pay attention to columns: 'order_datetime', 'quantity', 'total_amount'. Are their types correct?\n",
    "\n",
    "# TODO: Print schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75156861-9210-4660-a301-b4850ce72ce7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 3: Display summary statistics for numeric columns\n",
    "# Columns: 'quantity', 'unit_cost', 'sales_price', 'total_amount'\n",
    "# Check MIN and MAX values. Do you see anything concerning (negative prices, extreme quantities)?\n",
    "\n",
    "# TODO: Display summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5d4e123-a245-480c-8d02-70d1e0bb88c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 3b: Check skewness of numeric columns\n",
    "# Skewness > 1 or < -1 indicates highly skewed data\n",
    "# Highly skewed features may need Log Transformation before modeling\n",
    "\n",
    "from pyspark.sql.functions import skewness\n",
    "\n",
    "# TODO: Calculate skewness for 'total_amount' and 'quantity'\n",
    "# Hint: df.select(skewness(\"column_name\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da5bfc85-e08b-4e50-a5d0-96b1b7ae4423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 3: Data Quality Issue Identification\n",
    "\n",
    "**Business context:** Real-world data is messy. Orders get duplicated, systems crash mid-transaction, users enter invalid data. We need to find and document every issue.\n",
    "\n",
    "**Common retail data problems:**\n",
    "- Customer didn't provide email → NULL\n",
    "- Return processed incorrectly → negative quantity  \n",
    "- POS system error → quantity * price ≠ total\n",
    "\n",
    "**Why it matters:** If 5% of transactions have errors, our model learns from lies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33d9921c-3583-430d-bb41-c754a7e268ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, when, isnan, round, abs\n",
    "\n",
    "# Exercise 4: Count NULL values in each column\n",
    "# Hint: Use a loop over df.columns or list comprehension\n",
    "\n",
    "null_counts = # TODO: Count nulls per column\n",
    "display(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3203a8d4-c0a0-4b26-a91d-4e10b5abec2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 5: Check for logical errors\n",
    "# Find rows where:\n",
    "# a) 'quantity' is less than or equal to 0\n",
    "# b) 'total_amount' is less than 0\n",
    "# Display the invalid records\n",
    "\n",
    "invalid_orders = # TODO: Filter invalid orders\n",
    "# print(...)\n",
    "display(invalid_orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "236502e4-2da2-4ff6-9bf8-36569113e35e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 6 (Challenge): Check mathematical consistency\n",
    "# Does 'total_amount' equal 'quantity' * 'sales_price'?\n",
    "# Account for floating point precision (difference > 0.01)\n",
    "# Find records where the calculation does not match\n",
    "\n",
    "inconsistent_prices = # TODO: Find inconsistent records\n",
    "\n",
    "# print(...)\n",
    "# display(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceab2df0-7ff7-45d6-8888-817ba8167b22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 4: Distribution Analysis\n",
    "\n",
    "**Business context:** Marketing wants to know the customer mix. Are most customers Basic, or is Premium the majority? This affects:\n",
    "- **Class imbalance:** If 90% are Basic, model might just predict \"Basic\" for everyone\n",
    "- **Business strategy:** If Premium is rare, acquiring one is very valuable\n",
    "\n",
    "**Key questions:**\n",
    "1. Payment method distribution → Do Premium customers use credit cards?\n",
    "2. Customer segment distribution → How balanced are our classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92613b2d-6e5f-4afa-8815-6b2e401b1126",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 7: Visualize 'payment_method' distribution\n",
    "# Use display() on grouped data. Which payment method is most common?\n",
    "\n",
    "# TODO: Group and display payment methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aeb2d4d-b0db-40d3-822a-cb96f37f5d21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 8: Visualize 'customer_segment' distribution\n",
    "# Are the classes balanced or does one segment dominate? This is important for the ML model.\n",
    "\n",
    "# TODO: Group and display customer segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0807c65a-6096-42b9-8981-408fd06d9d7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Checkpoint: What Did We Discover?\n",
    "\n",
    "**At this point, you should have identified:**\n",
    "\n",
    "| Finding | Example | Impact on ML |\n",
    "|---------|---------|--------------|\n",
    "|  Dataset shape | 10,000 rows, 15 columns | Enough data for training |\n",
    "| ️ Null values | 5% missing emails | Need imputation strategy |\n",
    "|  Invalid records | Negative quantities | Must filter before modeling |\n",
    "| ️ Skewed distributions | total_amount skew > 2 | May need log transform |\n",
    "| ️ Class imbalance | 60% Basic, 25% Standard, 15% Premium | Consider stratified sampling |\n",
    "\n",
    "**Business translation for stakeholders:**\n",
    "> \"We analyzed the sales data and found data quality issues that would corrupt the ML model. About 5% of transactions have errors (negative values, missing IDs). Once cleaned, we'll have reliable data for customer segmentation.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Key findings to address in Workshop 2:**\n",
    "1. Remove invalid records (negative quantities, amounts)\n",
    "2. Handle missing values (delete key IDs, impute descriptive fields)\n",
    "3. Consider log transformation for skewed features\n",
    "4. Plan stratified sampling for imbalanced classes\n",
    "\n",
    "---\n",
    "\n",
    "## Best Practices: EDA\n",
    "\n",
    "| Practice | Description |\n",
    "|----------|-------------|\n",
    "| **Start with shape** | `df.count()`, `len(df.columns)` before anything else |\n",
    "| **Check types first** | `printSchema()` - wrong types cause silent errors |\n",
    "| **Use summary()** | Quick view of min/max reveals impossible values |\n",
    "| **Check skewness** | Values > 1 need transformation (log, sqrt) |\n",
    "| **Document findings** | Write down issues for the cleaning phase |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3b862cd-2cad-48ca-a7c0-cee144662c98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# Solutions\n",
    "\n",
    "Reference solutions for the exercises above. Compare your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbc6b172-84b5-47fe-9b06-1ab3cc4e6184",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Load Data\n",
    "df = spark.table(\"workshop_sales_data\")\n",
    "print(f\"Rows: {df.count()}\")\n",
    "display(df.limit(5))\n",
    "\n",
    "# 2. Schema\n",
    "df.printSchema()\n",
    "\n",
    "# 3. Stats\n",
    "display(df.select(\"quantity\", \"unit_cost\", \"sales_price\", \"total_amount\").summary())\n",
    "\n",
    "# 3b. Skewness\n",
    "from pyspark.sql.functions import skewness\n",
    "display(df.select(\n",
    "    skewness(\"total_amount\").alias(\"total_amount_skew\"),\n",
    "    skewness(\"quantity\").alias(\"quantity_skew\")\n",
    "))\n",
    "\n",
    "# 4. Nulls\n",
    "from pyspark.sql.functions import col, count, when, abs, round\n",
    "null_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "display(null_counts)\n",
    "\n",
    "# 5. Invalid Logic\n",
    "invalid_orders = df.filter((col(\"quantity\") <= 0) | (col(\"total_amount\") < 0))\n",
    "display(invalid_orders)\n",
    "\n",
    "# 6. Math Consistency\n",
    "inconsistent = df.withColumn(\"calc_total\", round(col(\"quantity\") * col(\"sales_price\"), 2)) \\\n",
    "                 .filter(abs(col(\"calc_total\") - col(\"total_amount\")) > 0.01)\n",
    "display(inconsistent)\n",
    "\n",
    "# 7 & 8. Visualizations\n",
    "display(df.groupBy(\"payment_method\").count())\n",
    "display(df.groupBy(\"customer_segment\").count())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Workshop_Data_Exploration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
