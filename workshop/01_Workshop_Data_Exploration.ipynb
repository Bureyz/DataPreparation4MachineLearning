{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96751271",
   "metadata": {},
   "source": [
    "# Workshop 1: Exploratory Data Analysis (EDA)\n",
    "\n",
    "## Objective\n",
    "\n",
    "Perform exploratory data analysis to understand the RetailMax sales dataset and identify data quality issues before building an ML model.\n",
    "\n",
    "## Context and Requirements\n",
    "\n",
    "- **Workshop:** Customer Segmentation for RetailMax\n",
    "- **Notebook type:** Hands-on Exercise\n",
    "- **Prerequisites:** `00_Workshop_Setup.ipynb` completed\n",
    "- **Technical requirements:**\n",
    "  - Databricks Runtime 14.x LTS or newer\n",
    "  - Unity Catalog enabled\n",
    "- **Execution time:** ~30 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Theoretical Background\n",
    "\n",
    "**Why is EDA important for ML?**\n",
    "\n",
    "Exploratory Data Analysis is the first step in any ML project. The \"Garbage In, Garbage Out\" principle means that even the best model cannot compensate for poor data quality.\n",
    "\n",
    "**EDA helps answer:**\n",
    "\n",
    "| Aspect | Question | Impact on ML |\n",
    "|--------|----------|--------------|\n",
    "| **Data Quality** | Missing values? Duplicates? Invalid values? | Requires imputation or removal |\n",
    "| **Distribution** | Normal or skewed? | Affects model choice and scaling |\n",
    "| **Outliers** | Extreme values present? | Can distort linear models |\n",
    "| **Correlations** | Which features are related? | Informs feature selection |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e0e2b5",
   "metadata": {},
   "source": [
    "## Section 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ed47bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Load Data\n",
    "# Load the 'workshop_sales_data' table into a DataFrame named 'df'.\n",
    "# Display the row count and first 5 records to verify the data loaded correctly.\n",
    "\n",
    "df = # TODO: Load table\n",
    "# print(...)\n",
    "# display(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b8fe79",
   "metadata": {},
   "source": [
    "## Section 2: Data Profiling\n",
    "\n",
    "Before cleaning, understand the nature of the data.\n",
    "\n",
    "**Check:**\n",
    "1. **Data types:** Are dates stored as dates? Are numbers numeric?\n",
    "2. **Statistics:** Do mean values make sense? Are there outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a401b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Display the data schema\n",
    "# Pay attention to columns: 'order_datetime', 'quantity', 'total_amount'. Are their types correct?\n",
    "\n",
    "# TODO: Print schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4975af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Display summary statistics for numeric columns\n",
    "# Columns: 'quantity', 'unit_cost', 'sales_price', 'total_amount'\n",
    "# Check MIN and MAX values. Do you see anything concerning (negative prices, extreme quantities)?\n",
    "\n",
    "# TODO: Display summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3116ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3b: Check skewness of numeric columns\n",
    "# Skewness > 1 or < -1 indicates highly skewed data\n",
    "# Highly skewed features may need Log Transformation before modeling\n",
    "\n",
    "from pyspark.sql.functions import skewness\n",
    "\n",
    "# TODO: Calculate skewness for 'total_amount' and 'quantity'\n",
    "# Hint: df.select(skewness(\"column_name\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca30c2a4",
   "metadata": {},
   "source": [
    "## Section 3: Data Quality Issue Identification\n",
    "\n",
    "Real-world data is rarely perfect. Identify specific issues to plan remediation.\n",
    "\n",
    "**Look for:**\n",
    "1. **Missing values (Nulls):** Which columns are incomplete?\n",
    "2. **Logical errors:** Negative quantities, negative prices\n",
    "3. **Inconsistencies:** Does `quantity * sales_price` equal `total_amount`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a70cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, when, isnan, round, abs\n",
    "\n",
    "# Exercise 4: Count NULL values in each column\n",
    "# Hint: Use a loop over df.columns or list comprehension\n",
    "\n",
    "null_counts = # TODO: Count nulls per column\n",
    "display(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfeb6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Check for logical errors\n",
    "# Find rows where:\n",
    "# a) 'quantity' is less than or equal to 0\n",
    "# b) 'total_amount' is less than 0\n",
    "# Display the invalid records\n",
    "\n",
    "invalid_orders = # TODO: Filter invalid orders\n",
    "# print(...)\n",
    "display(invalid_orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a7d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6 (Challenge): Check mathematical consistency\n",
    "# Does 'total_amount' equal 'quantity' * 'sales_price'?\n",
    "# Account for floating point precision (difference > 0.01)\n",
    "# Find records where the calculation does not match\n",
    "\n",
    "inconsistent_prices = # TODO: Find inconsistent records\n",
    "\n",
    "# print(...)\n",
    "# display(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e8dbea",
   "metadata": {},
   "source": [
    "## Section 4: Distribution Analysis\n",
    "\n",
    "Visualizations help identify trends and anomalies quickly.\n",
    "\n",
    "**Focus on:**\n",
    "1. Payment method distribution\n",
    "2. Customer segment distribution\n",
    "3. Sales over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6fe03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7: Visualize 'payment_method' distribution\n",
    "# Use display() on grouped data. Which payment method is most common?\n",
    "\n",
    "# TODO: Group and display payment methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bf9526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8: Visualize 'customer_segment' distribution\n",
    "# Are the classes balanced or does one segment dominate? This is important for the ML model.\n",
    "\n",
    "# TODO: Group and display customer segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fe0f25",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "At this point, you should have:\n",
    "- Loaded and examined the dataset structure\n",
    "- Identified null values and their distribution\n",
    "- Found logical errors (negative quantities, amounts)\n",
    "- Checked skewness of numeric features\n",
    "- Analyzed class balance in customer segments\n",
    "\n",
    "**Key findings to address in the next notebook:**\n",
    "- Missing values in key columns\n",
    "- Invalid records (negative values)\n",
    "- Mathematical inconsistencies\n",
    "- Skewed distributions (may need log transform)\n",
    "\n",
    "---\n",
    "\n",
    "## Best Practices: EDA\n",
    "\n",
    "| Practice | Description |\n",
    "|----------|-------------|\n",
    "| **Start with shape** | `df.count()`, `len(df.columns)` before anything else |\n",
    "| **Check types first** | `printSchema()` - wrong types cause silent errors |\n",
    "| **Use summary()** | Quick view of min/max reveals impossible values |\n",
    "| **Check skewness** | Values > 1 need transformation (log, sqrt) |\n",
    "| **Document findings** | Write down issues for the cleaning phase |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11758da3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Solutions\n",
    "\n",
    "Reference solutions for the exercises above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c687f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Data\n",
    "df = spark.table(\"workshop_sales_data\")\n",
    "print(f\"Rows: {df.count()}\")\n",
    "display(df.limit(5))\n",
    "\n",
    "# 2. Schema\n",
    "df.printSchema()\n",
    "\n",
    "# 3. Stats\n",
    "display(df.select(\"quantity\", \"unit_cost\", \"sales_price\", \"total_amount\").summary())\n",
    "\n",
    "# 3b. Skewness\n",
    "from pyspark.sql.functions import skewness\n",
    "display(df.select(\n",
    "    skewness(\"total_amount\").alias(\"total_amount_skew\"),\n",
    "    skewness(\"quantity\").alias(\"quantity_skew\")\n",
    "))\n",
    "\n",
    "# 4. Nulls\n",
    "from pyspark.sql.functions import col, count, when, abs, round\n",
    "null_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "display(null_counts)\n",
    "\n",
    "# 5. Invalid Logic\n",
    "invalid_orders = df.filter((col(\"quantity\") <= 0) | (col(\"total_amount\") < 0))\n",
    "display(invalid_orders)\n",
    "\n",
    "# 6. Math Consistency\n",
    "inconsistent = df.withColumn(\"calc_total\", round(col(\"quantity\") * col(\"sales_price\"), 2)) \\\n",
    "                 .filter(abs(col(\"calc_total\") - col(\"total_amount\")) > 0.01)\n",
    "display(inconsistent)\n",
    "\n",
    "# 7 & 8. Visualizations\n",
    "display(df.groupBy(\"payment_method\").count())\n",
    "display(df.groupBy(\"customer_segment\").count())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
