{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "315ed1a0-4e5b-4f42-aecf-5667064729aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Workshop 3: ML Pipeline with MLflow\n",
    "\n",
    "## Business Context: From Analysis to Prediction\n",
    "\n",
    "In Workshops 1 and 2, we cleaned data and built the Customer 360 table with RFM features. Now it's time to **teach a computer to predict customer segments**.\n",
    "\n",
    "**The business goal:**\n",
    "- Currently: Manual segment assignment by analysts (slow, inconsistent)\n",
    "- Target: Automatic classification for new customers in real-time\n",
    "\n",
    "**How it will work in production:**\n",
    "\n",
    "```\n",
    "New Customer Signs Up → Customer 360 Calculated → ML Model → Segment Prediction\n",
    "                                                              ↓\n",
    "                                                    Marketing Automation\n",
    "                                                    • Premium → VIP offer\n",
    "                                                    • Standard → Loyalty program\n",
    "                                                    • Basic → Welcome discount\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## What the ML Model Will Learn\n",
    "\n",
    "We're training a **classification model** that learns patterns from historical data:\n",
    "\n",
    "| Customer Features | Actual Segment | Model Learns |\n",
    "|-------------------|----------------|--------------|\n",
    "| Spend: $5000, Orders: 50, Recency: 3 days | Premium | High spend + frequent + recent = Premium |\n",
    "| Spend: $500, Orders: 5, Recency: 30 days | Standard | Medium activity = Standard |\n",
    "| Spend: $50, Orders: 1, Recency: 180 days | Basic | Low engagement = Basic |\n",
    "\n",
    "**Once trained, the model can predict:**\n",
    "- \"This new customer spent $3000 in first week → likely Premium (87% confidence)\"\n",
    "- Marketing can treat them as Premium from day 1, not after 6 months\n",
    "\n",
    "---\n",
    "\n",
    "## Why Pipelines? Preventing Expensive Mistakes\n",
    "\n",
    "**The problem:** In production, you need the EXACT same data transformations as in training.\n",
    "\n",
    "| Training | Production | Result |\n",
    "|----------|------------|--------|\n",
    "| Scale features 0-1 | Forget to scale |  Garbage predictions |\n",
    "| Impute missing with mean=500 | Use mean=600 | ️ Inconsistent results |\n",
    "\n",
    "**The solution:** Spark ML Pipeline bundles ALL steps into one object:\n",
    "\n",
    "```\n",
    "Pipeline = [Imputer → Assembler → Scaler → Model]\n",
    "           ↓\n",
    "        pipeline.save(\"production_model\")\n",
    "           ↓\n",
    "        pipeline.load() → pipeline.transform(new_data) → Predictions\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Context and Requirements\n",
    "\n",
    "- **Workshop:** Customer Segmentation for RetailMax\n",
    "- **Notebook type:** Hands-on Exercise\n",
    "- **Prerequisites:** `02_Workshop_Data_Cleaning_and_Features.ipynb` completed\n",
    "- **Technical requirements:**\n",
    "  - Databricks Runtime 14.x LTS or newer\n",
    "  - Unity Catalog enabled\n",
    "  - MLflow enabled (default in Databricks)\n",
    "- **Execution time:** ~30 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Theoretical Background\n",
    "\n",
    "**Pipeline Components:**\n",
    "\n",
    "| Stage | Purpose | Business Value |\n",
    "|-------|---------|----------------|\n",
    "| **Imputer** | Fill missing values | Handle incomplete customer data |\n",
    "| **Assembler** | Combine features into vector | Required format for ML |\n",
    "| **Scaler** | Normalize scale (0-1) | Fair comparison of features |\n",
    "| **Model** | Learn patterns | Make predictions |\n",
    "\n",
    "**Unity Catalog Models (recommended):**\n",
    "\n",
    "| Feature | Business Value |\n",
    "|---------|---------------|\n",
    "| **Model Registry** | Central catalog of all models |\n",
    "| **Versioning** | Track which version is in production |\n",
    "| **Aliases** | `@champion` = current production model |\n",
    "| **Governance** | Who can deploy? Who can access? |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33382c31-f842-4494-852d-36d6f5a66266",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../demo/00_Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "573a7478-fdd0-430d-a598-a69fd1a510ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 1: Load Feature Data\n",
    "\n",
    "**Current state:** Customer 360 table with RFM features from Workshop 2\n",
    "\n",
    "**What we have per customer:**\n",
    "- `total_spend`, `order_count`, `recency`, `tenure` (numeric features)\n",
    "- `country_index` (encoded categorical)\n",
    "- `customer_segment` (our target label: Basic/Standard/Premium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "424851c0-bb27-422d-89fd-846a61b146ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Environment setup (same as Workshop Setup)\n",
    "current_user_email = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "username = current_user_email.split(\"@\")[0].replace(\".\", \"_\").replace(\"-\", \"_\")\n",
    "\n",
    "if \"trainer\" in username or \"krzysztof_burejza\" in username:\n",
    "    effective_user = \"trainer\"\n",
    "else:\n",
    "    effective_user = username\n",
    "\n",
    "catalog_name = \"data_ml_preparation\"\n",
    "schema_name = f\"ml_dp_{effective_user}\"\n",
    "\n",
    "spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_name}\")\n",
    "\n",
    "print(f\"Using: {catalog_name}.{schema_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75a50852-3a2b-4054-861d-72f150d6a827",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load customer features from previous workshop\n",
    "df_features = spark.table(\"workshop_customer_features\")\n",
    "display(df_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8104c479-a76c-4d2c-bd49-470ebece7ddf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 2: Data Splitting\n",
    "\n",
    "**Business context:** We need to simulate production conditions. The model must predict customers it has never seen.\n",
    "\n",
    "**How it works:**\n",
    "1. **Training set (80%):** Model learns patterns from these customers\n",
    "2. **Test set (20%):** We pretend these are \"new\" customers and measure accuracy\n",
    "\n",
    "**Why 80/20?**\n",
    "- More training data → better learning\n",
    "- Enough test data → reliable accuracy estimate\n",
    "- Industry standard balance\n",
    "\n",
    "**Warning: Class Imbalance**\n",
    "If 70% of customers are Basic, a lazy model could predict \"Basic\" always and get 70% accuracy. Stratified sampling ensures both train and test have the same class proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e8dc9b5-73cd-4df5-a84f-6c7ff9ad44e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 1: Split data into training (80%) and testing (20%) sets\n",
    "# Set seed=42 for reproducibility\n",
    "\n",
    "train_df, test_df = # TODO: Split data using randomSplit\n",
    "# print(f\"Train: {train_df.count()}, Test: {test_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "663fe8b7-d386-4264-855a-0fd4bdc5dab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 1b (Challenge): Stratified Split\n",
    "# Check class distribution before and after split\n",
    "# Use sampleBy() for stratified sampling if classes are imbalanced\n",
    "\n",
    "# Check distribution\n",
    "print(\"Full dataset distribution:\")\n",
    "display(df_features.groupBy(\"customer_segment\").count())\n",
    "\n",
    "# TODO: Verify that train and test have similar distributions\n",
    "# print(\"Train distribution:\")\n",
    "# display(train_df.groupBy(\"customer_segment\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46ebdc55-0a1a-4bff-9b19-9e063d80bf7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 3: Define ML Pipeline\n",
    "\n",
    "**Business context:** This is where the magic happens. We define the \"recipe\" that transforms raw features into predictions.\n",
    "\n",
    "**Think of it as a production line:**\n",
    "\n",
    "```\n",
    "Customer Data → [Clean] → [Assemble] → [Scale] → [Classify] → Segment Prediction\n",
    "```\n",
    "\n",
    "**Pipeline stages explained:**\n",
    "\n",
    "| Stage | What it does | Example |\n",
    "|-------|--------------|---------|\n",
    "| **Imputer** | Fill missing values with mean | NULL → 500.0 |\n",
    "| **Assembler** | Combine columns into one vector | [5000, 50, 3, 365] |\n",
    "| **Scaler** | Normalize to 0-1 range | 5000 → 0.75 |\n",
    "| **Indexer** | Convert \"Premium\" → 2 | Text → Number |\n",
    "| **Model** | Learn patterns, make predictions | Features → Segment |\n",
    "\n",
    "**Why Pipeline, not separate steps?**\n",
    "- Guarantees same transformations in training and production\n",
    "- One object to save, load, deploy\n",
    "- Prevents \"I forgot to scale\" errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b824047-2447-434b-b49d-0e6d873362b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, Imputer\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "\n",
    "# 1. Label Indexer (Target: customer_segment)\n",
    "label_indexer = StringIndexer(inputCol=\"customer_segment\", outputCol=\"label\")\n",
    "\n",
    "# Exercise 2a: Define Imputer for columns 'total_spend', 'recency', 'tenure'\n",
    "imputer = # TODO: Create Imputer\n",
    "\n",
    "# Exercise 2b: Define VectorAssembler using imputed columns plus 'order_count' and 'country_index'\n",
    "assembler = # TODO: Create VectorAssembler\n",
    "\n",
    "# 4. Scaler\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\")\n",
    "\n",
    "# Exercise 2c: Choose model (LogisticRegression or RandomForestClassifier)\n",
    "lr = # TODO: Create classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2971cb33-cd27-4d28-8ddb-827dafef421d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 3: Create Pipeline combining all stages\n",
    "\n",
    "pipeline = # TODO: Create Pipeline with all stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "709e5a1c-1c7f-4d8f-8426-1731eebb6b38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 4: Training, Evaluation, and Tuning with MLflow\n",
    "\n",
    "**Business context:** Training one model is not enough. We need to:\n",
    "1. **Track experiments:** What parameters did we try? What worked?\n",
    "2. **Compare versions:** Is the new model better than the old one?\n",
    "3. **Deploy safely:** Register the best model for production use\n",
    "\n",
    "**MLflow answers these questions:**\n",
    "\n",
    "| Feature | Business Value |\n",
    "|---------|---------------|\n",
    "| **Experiment Tracking** | \"The model with regParam=0.01 had 85% accuracy\" |\n",
    "| **Model Registry** | \"Version 3 is in production, Version 4 is testing\" |\n",
    "| **Artifacts** | \"Here's the exact model used to make this prediction\" |\n",
    "\n",
    "**Hyperparameter Tuning:**\n",
    "Models have \"settings\" (hyperparameters) that affect performance. CrossValidator automatically tries different combinations and picks the best one.\n",
    "\n",
    "```\n",
    "regParam: [0.1, 0.01] × elasticNetParam: [0.0, 0.5, 1.0] = 6 combinations\n",
    "→ CrossValidator tests all 6 → Returns best performing model\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ️ Unity Catalog Requires Model Signature\n",
    "\n",
    "Unity Catalog models **MUST** include a **signature** (input/output schema). Without it, you get:\n",
    "\n",
    "```\n",
    "MlflowException: Model passed for registration did not contain any signature metadata.\n",
    "```\n",
    "\n",
    "**Solution:** Use `infer_signature()` to automatically detect schema:\n",
    "\n",
    "```python\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "signature = infer_signature(\n",
    "    train_df.select(input_columns).toPandas(),  # Input schema\n",
    "    predictions.select(\"prediction\").toPandas()  # Output schema\n",
    ")\n",
    "\n",
    "mlflow.spark.log_model(model, \"model\", signature=signature, registered_model_name=...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4174a45-56e9-4448-ab39-06f31e677982",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Setup MLflow with Unity Catalog\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Setup Experiment\n",
    "username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "experiment_path = f\"/Users/{username}/workshop_customer_segmentation\"\n",
    "mlflow.set_experiment(experiment_path)\n",
    "\n",
    "# Model name for Unity Catalog (uses catalog and schema from Workshop Setup)\n",
    "# These variables should be defined if you ran 00_Workshop_Setup.ipynb\n",
    "model_name = f\"{catalog_name}.{schema_name}.customer_segmentation_model\"\n",
    "\n",
    "# Exercise 4: Run MLflow experiment\n",
    "# Inside 'with mlflow.start_run():' block:\n",
    "# 1. Train pipeline on training set\n",
    "# 2. Make predictions on test set\n",
    "# 3. Calculate Accuracy and F1 Score\n",
    "# 4. Log metrics and register model to Unity Catalog\n",
    "#\n",
    "# IMPORTANT: Unity Catalog requires model SIGNATURE (input/output schema)\n",
    "# Use infer_signature() to automatically detect schema from data\n",
    "\n",
    "# with mlflow.start_run(run_name=\"LR_Baseline\"):\n",
    "#     # Train\n",
    "#     model = pipeline.fit(train_df)\n",
    "#     \n",
    "#     # Predict\n",
    "#     predictions = model.transform(test_df)\n",
    "#     \n",
    "#     # Evaluate\n",
    "#     accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\").evaluate(predictions)\n",
    "#     mlflow.log_metric(\"accuracy\", accuracy)\n",
    "#     \n",
    "#     # Infer signature for Unity Catalog (REQUIRED!)\n",
    "#     input_cols = [\"total_spend\", \"recency\", \"tenure\", \"order_count\", \"country_index\", \"customer_segment\"]\n",
    "#     signature = infer_signature(train_df.select(input_cols).toPandas(), predictions.select(\"prediction\").toPandas())\n",
    "#     \n",
    "#     # Register model to Unity Catalog with signature\n",
    "#     mlflow.spark.log_model(model, \"model\", signature=signature, registered_model_name=model_name)\n",
    "    \n",
    "# Exercise 4 (Challenge): Implement CrossValidation (Grid Search)\n",
    "# Create paramGrid for regParam (0.1, 0.01) and elasticNetParam (0.0, 0.5, 1.0)\n",
    "# Use CrossValidator with 3 folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19657389-9a5f-47ee-a1bb-548450aee118",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# Solutions\n",
    "\n",
    "Reference solutions for the exercises above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a31a642-b860-40e9-b391-419ccaf4fd6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Environment setup (same as Workshop Setup)\n",
    "current_user_email = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "username = current_user_email.split(\"@\")[0].replace(\".\", \"_\").replace(\"-\", \"_\")\n",
    "\n",
    "if \"trainer\" in username or \"krzysztof_burejza\" in username:\n",
    "    effective_user = \"trainer\"\n",
    "else:\n",
    "    effective_user = username\n",
    "\n",
    "catalog_name = \"data_ml_preparation\"\n",
    "schema_name = f\"ml_dp_{effective_user}\"\n",
    "\n",
    "spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_name}\")\n",
    "\n",
    "print(f\"Using: {catalog_name}.{schema_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d22f9809-64f7-47a2-806e-f96adcb0504c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load customer features from previous workshop\n",
    "df_features = spark.table(\"workshop_customer_features\")\n",
    "display(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75d5b8e0-83d6-458b-8035-9ee27abd848d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Split\n",
    "train_df, test_df = df_features.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Train: {train_df.count()}, Test: {test_df.count()}\")\n",
    "\n",
    "# 1b. Verify distribution (stratification check)\n",
    "print(\"Train distribution:\")\n",
    "display(train_df.groupBy(\"customer_segment\").count())\n",
    "print(\"Test distribution:\")\n",
    "display(test_df.groupBy(\"customer_segment\").count())\n",
    "\n",
    "# 2. Pipeline Definition\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, Imputer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "label_indexer = StringIndexer(inputCol=\"customer_segment\", outputCol=\"label\")\n",
    "\n",
    "imputer = Imputer(inputCols=[\"total_spend\", \"recency\", \"tenure\"], outputCols=[\"total_spend_imp\", \"recency_imp\", \"tenure_imp\"])\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"total_spend_imp\", \"recency_imp\", \"tenure_imp\", \"order_count\", \"country_index\"], outputCol=\"features_raw\", handleInvalid='keep')\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\")\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[label_indexer, imputer, assembler, scaler, lr])\n",
    "\n",
    "# 3. MLflow with Unity Catalog & CrossValidation\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from mlflow.models.signature import infer_signature\n",
    "import mlflow\n",
    "\n",
    "# Setup Unity Catalog for model registry\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "model_name = f\"{catalog_name}.{schema_name}.customer_segmentation_model\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"LR_GridSearch\"):\n",
    "    # Grid\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "        .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "        .build()\n",
    "    \n",
    "    # CV\n",
    "    crossval = CrossValidator(estimator=pipeline,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\"),\n",
    "                              numFolds=3)\n",
    "    \n",
    "    # Fit\n",
    "    cvModel = crossval.fit(train_df)\n",
    "    \n",
    "    # Best Model Metrics\n",
    "    best_model = cvModel.bestModel\n",
    "    predictions = best_model.transform(test_df)\n",
    "    accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\").evaluate(predictions)\n",
    "    \n",
    "    mlflow.log_metric(\"accuracy_cv\", accuracy)\n",
    "    \n",
    "    # Infer model signature from input and output data\n",
    "    # Unity Catalog requires signature for model registration\n",
    "    input_example = train_df.select(\"total_spend\", \"recency\", \"tenure\", \"order_count\", \"country_index\", \"customer_segment\").limit(5).toPandas()\n",
    "    signature = infer_signature(\n",
    "        train_df.select(\"total_spend\", \"recency\", \"tenure\", \"order_count\", \"country_index\", \"customer_segment\").toPandas(),\n",
    "        predictions.select(\"prediction\").toPandas()\n",
    "    )\n",
    "    \n",
    "    # Register model to Unity Catalog with signature\n",
    "    mlflow.spark.log_model(\n",
    "        best_model, \n",
    "        \"best_model\",\n",
    "        signature=signature,\n",
    "        input_example=input_example,\n",
    "        registered_model_name=model_name\n",
    "    )\n",
    "    \n",
    "    print(f\"Best CV Accuracy: {accuracy}\")\n",
    "    print(f\"Model registered to Unity Catalog: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dac9190-0b82-4978-9ba0-10510559a5b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary: From Data to Business Value\n",
    "\n",
    "### What We Built\n",
    "\n",
    "```\n",
    "Workshop 1          Workshop 2          Workshop 3\n",
    "    ↓                   ↓                   ↓\n",
    "Raw Transactions → Customer 360 → ML Model in Production\n",
    "                       ↓                   ↓\n",
    "                   RFM Features    Automatic Segment Prediction\n",
    "```\n",
    "\n",
    "### Business Impact\n",
    "\n",
    "| Before (Manual) | After (ML Model) |\n",
    "|-----------------|------------------|\n",
    "| Analysts review customer data weekly | Real-time prediction for new customers |\n",
    "| 2% campaign conversion rate | Targeted campaigns → 5%+ expected |\n",
    "| 6 months to identify Premium customers | Day 1 segment assignment |\n",
    "| Inconsistent segment definitions | Consistent, reproducible classification |\n",
    "\n",
    "### What Marketing Can Now Do\n",
    "\n",
    "1. **New customer signs up** → Model predicts \"Premium\" with 87% confidence\n",
    "2. **Marketing automation** → Immediately sends VIP welcome package\n",
    "3. **Result** → Customer feels valued, more likely to buy again\n",
    "\n",
    "---\n",
    "\n",
    "## Workshop Achievements\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| Data Splitting | 80/20 train/test split with seed for reproducibility |\n",
    "| Pipeline | Imputer → Assembler → Scaler → Classifier |\n",
    "| MLflow Tracking | Experiment logged with params and metrics |\n",
    "| Unity Catalog Model | Model registered with governance and versioning |\n",
    "| Hyperparameter Tuning | CrossValidator with grid search |\n",
    "\n",
    "### Unity Catalog Artifacts Created:\n",
    "\n",
    "| Artifact | Location |\n",
    "|----------|----------|\n",
    "| Experiment | `/Users/{user}/workshop_customer_segmentation` |\n",
    "| Model | `{catalog}.{schema}.customer_segmentation_model` |\n",
    "\n",
    "---\n",
    "\n",
    "## Best Practices: ML Pipelines\n",
    "\n",
    "| Practice | Description |\n",
    "|----------|-------------|\n",
    "| **Use Pipelines** | Prevents data leakage, ensures reproducibility |\n",
    "| **Set random seed** | `seed=42` for reproducible splits |\n",
    "| **Stratify if imbalanced** | Preserve class distribution in splits |\n",
    "| **Use Unity Catalog Models** | Governance, versioning, lineage tracking |\n",
    "| **Log everything** | Params, metrics, and model artifacts to MLflow |\n",
    "| **Evaluate on holdout** | Final metric only from test set (never validation) |\n",
    "\n",
    "## Common Mistakes to Avoid\n",
    "\n",
    "| Mistake | Consequence |\n",
    "|---------|-------------|\n",
    "| Fitting scaler on all data | Data leakage - inflated metrics |\n",
    "| Using test set for tuning | Overfitting to test distribution |\n",
    "| Saving model locally | No governance, hard to share |\n",
    "| No random seed | Non-reproducible results |\n",
    "| Not logging to MLflow | Lost experiments, no comparison |\n",
    "\n",
    "---\n",
    "\n",
    "## The Complete Journey\n",
    "\n",
    "```\n",
    " EDA (Workshop 1)\n",
    "   \"We have dirty data - 5% invalid transactions found\"\n",
    "           ↓\n",
    " Data Cleaning (Workshop 2)  \n",
    "   \"Silver layer ready - 10,000 transactions → 2,000 customers\"\n",
    "           ↓\n",
    " Feature Engineering (Workshop 2)\n",
    "   \"Customer 360 with RFM features built\"\n",
    "           ↓\n",
    " ML Pipeline (Workshop 3)\n",
    "   \"Model trained: 85% accuracy on segment prediction\"\n",
    "           ↓\n",
    " Unity Catalog (Workshop 3)\n",
    "   \"Model registered, ready for production\"\n",
    "           ↓\n",
    " Business Impact\n",
    "   \"Marketing can now personalize campaigns for 10,000+ customers\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps (Production):**\n",
    "- Set model alias `@champion` for production deployment\n",
    "- Deploy model for batch or real-time inference\n",
    "- Set up monitoring for model performance drift\n",
    "- A/B test: ML segments vs manual segments"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Workshop_ML_Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}