{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9440b437",
      "metadata": {},
      "source": [
        "# Module 6: Machine Learning Pipelines\n",
        "\n",
        "**Training Objective:** Master Spark ML Pipelines to create reproducible, production-ready ML workflows with MLflow tracking.\n",
        "\n",
        "**Scope:**\n",
        "- Pipeline Concepts: Why use Pipelines?\n",
        "- Defining Stages: Chaining Imputers, Encoders, Scalers, and Models\n",
        "- MLflow Tracking: Logging experiments, parameters, and metrics\n",
        "- Hyperparameter Tuning: Using CrossValidator for automatic model selection\n",
        "- Model Persistence: Saving the pipeline for production use"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea969e57",
      "metadata": {},
      "source": [
        "## Context and Requirements\n",
        "\n",
        "- **Training day:** Day 1 - Data Preparation Fundamentals\n",
        "- **Notebook type:** Demo\n",
        "- **Technical requirements:**\n",
        "  - Databricks Runtime 14.x LTS or newer\n",
        "  - Unity Catalog enabled\n",
        "  - MLflow enabled (default in Databricks)\n",
        "  - Permissions: CREATE TABLE, SELECT, MODIFY\n",
        "- **Dependencies:** `02_Data_Splitting.ipynb` (creates `customer_train`, `customer_test` tables)\n",
        "- **Execution time:** ~30 minutes\n",
        "\n",
        "> **Note:** This module brings together all previous concepts into a production-ready workflow."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c77d1db",
      "metadata": {},
      "source": [
        "## Theoretical Introduction\n",
        "\n",
        "**Why use Pipelines?**\n",
        "\n",
        "| Benefit | Description |\n",
        "|---------|-------------|\n",
        "| **Data Leakage Prevention** | `fit()` on train, `transform()` on test - automatically |\n",
        "| **Reproducibility** | Single artifact contains all preprocessing + model |\n",
        "| **Simplicity** | Save/load entire workflow as one object |\n",
        "| **Consistency** | Same transformations in training and production |\n",
        "\n",
        "**Pipeline Components:**\n",
        "\n",
        "```\n",
        "Data ‚Üí [Stage 1: Imputer] ‚Üí [Stage 2: Encoder] ‚Üí [Stage 3: Scaler] ‚Üí [Stage 4: Model] ‚Üí Predictions\n",
        "```\n",
        "\n",
        "**CrossValidator for Hyperparameter Tuning:**\n",
        "- Define a **ParamGrid** (list of hyperparameter combinations)\n",
        "- CrossValidator trains $k$ models for each combination (k-fold CV)\n",
        "- Picks the best model based on evaluation metric\n",
        "- Total models trained: `len(paramGrid) √ó numFolds`\n",
        "\n",
        "**MLflow Integration:**\n",
        "- **Experiments**: Group related runs together\n",
        "- **Runs**: Single training execution with params, metrics, artifacts\n",
        "- **Model Registry**: Version and stage models (Staging ‚Üí Production)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8ceec56",
      "metadata": {},
      "source": [
        "## Per-User Isolation\n",
        "\n",
        "Run the initialization script for per-user catalog and schema isolation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b511725c",
      "metadata": {},
      "outputs": [],
      "source": [
        "%run ./00_Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e5a4b41",
      "metadata": {},
      "source": [
        "**Import Libraries and Load Data:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38012fa1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import mlflow.spark\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import Imputer, StringIndexer, OneHotEncoder, VectorAssembler, RobustScaler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Load Raw Split Data (We start from scratch in the pipeline!)\n",
        "train_df = spark.table(\"customer_train\")\n",
        "test_df = spark.table(\"customer_test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab7db4b3",
      "metadata": {},
      "source": [
        "## Section 1: Defining Pipeline Stages\n",
        "\n",
        "**Why use a Pipeline?**\n",
        "1.  **Prevention of Data Leakage:** When we calculate things like \"Mean\" for imputation or \"Max\" for scaling, we must calculate them **only on the Training set** and apply them to the Test set. A Pipeline ensures `fit()` is called on Train and `transform()` on Test.\n",
        "2.  **Reproducibility:** It bundles all preprocessing steps and the model into a single artifact.\n",
        "3.  **Simplicity:** You can save/load the entire workflow as one object.\n",
        "\n",
        "We will reconstruct our manual steps into a reusable Pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3f804e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Imputation\n",
        "imputer = Imputer(inputCols=[\"age\", \"salary\"], outputCols=[\"age_imp\", \"salary_imp\"]).setStrategy(\"median\")\n",
        "\n",
        "# 2. Encoding\n",
        "indexer = StringIndexer(inputCol=\"country\", outputCol=\"country_idx\", handleInvalid=\"keep\")\n",
        "encoder = OneHotEncoder(inputCols=[\"country_idx\"], outputCols=[\"country_vec\"])\n",
        "\n",
        "# 3. Assembly\n",
        "assembler = VectorAssembler(inputCols=[\"age_imp\", \"salary_imp\", \"country_vec\"], outputCol=\"features_raw\")\n",
        "\n",
        "# 4. Scaling (RobustScaler because we have outliers!)\n",
        "scaler = RobustScaler(inputCol=\"features_raw\", outputCol=\"features\")\n",
        "\n",
        "# 5. Model (Predicting Salary based on Age and Country - just for demo)\n",
        "lr = LinearRegression(labelCol=\"salary\", featuresCol=\"features\")\n",
        "\n",
        "# --- The Pipeline ---\n",
        "pipeline = Pipeline(stages=[imputer, indexer, encoder, assembler, scaler, lr])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6867d784",
      "metadata": {},
      "source": [
        "## Section 2: Training with MLflow\n",
        "\n",
        "We use `mlflow.start_run()` to track this experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77a2b485",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set Experiment\n",
        "username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
        "experiment_path = f\"/Users/{username}/dp4ml_pipeline_demo\"\n",
        "mlflow.set_experiment(experiment_path)\n",
        "\n",
        "with mlflow.start_run(run_name=\"salary_prediction_v1\"):\n",
        "    \n",
        "    # Log Parameters\n",
        "    mlflow.log_param(\"model\", \"LinearRegression\")\n",
        "    mlflow.log_param(\"scaler\", \"RobustScaler\")\n",
        "    \n",
        "    # Fit Pipeline\n",
        "    print(\"Training Pipeline...\")\n",
        "    model = pipeline.fit(train_df)\n",
        "    \n",
        "    # Evaluate\n",
        "    predictions = model.transform(test_df)\n",
        "    evaluator = RegressionEvaluator(labelCol=\"salary\", metricName=\"rmse\")\n",
        "    rmse = evaluator.evaluate(predictions)\n",
        "    \n",
        "    print(f\"RMSE: {rmse}\")\n",
        "    \n",
        "    # Log Metrics\n",
        "    mlflow.log_metric(\"rmse\", rmse)\n",
        "    \n",
        "    # Log Model\n",
        "    mlflow.spark.log_model(model, \"model\")\n",
        "    \n",
        "    print(\"Run saved to MLflow.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e84d3f95",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the Pipeline Model for Production\n",
        "# This allows us to load the exact same transformations and model later for inference.\n",
        "\n",
        "model_path = f\"/Users/{username}/dp4ml_pipeline_model\"\n",
        "model.write().overwrite().save(model_path)\n",
        "\n",
        "print(f\"Pipeline Model saved to: {model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6b4a014",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the best model on TEST set\n",
        "predictions_cv = best_model.transform(test_df)\n",
        "rmse_cv = evaluator.evaluate(predictions_cv)\n",
        "\n",
        "# Extract the best hyperparameters from the LinearRegression stage\n",
        "# The last stage in the pipeline is the LinearRegressionModel\n",
        "lr_model_stage = best_model.stages[-1]\n",
        "\n",
        "print(f\"Best Model RMSE: {rmse_cv}\")\n",
        "print(f\"Best regParam: {lr_model_stage.getRegParam()}\")\n",
        "print(f\"Best elasticNetParam: {lr_model_stage.getElasticNetParam()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67b12ba1",
      "metadata": {},
      "source": [
        "## Best Practices\n",
        "\n",
        "### üéØ Pipeline Strategy Guide:\n",
        "\n",
        "| Component | Best Practice | Why |\n",
        "|-----------|--------------|-----|\n",
        "| **Order of stages** | Impute ‚Üí Encode ‚Üí Scale ‚Üí Model | Data dependencies |\n",
        "| **handleInvalid** | Use \"keep\" for StringIndexer | Handle new categories |\n",
        "| **Scaler choice** | RobustScaler for outliers | Most robust default |\n",
        "| **CrossValidator folds** | 3-5 for large data, 5-10 for small | Balance bias/variance |\n",
        "| **MLflow logging** | Log params, metrics, AND model | Full reproducibility |\n",
        "\n",
        "### ‚ö†Ô∏è Common Mistakes to Avoid:\n",
        "\n",
        "1. **Fitting pipeline on all data** ‚Üí Data leakage\n",
        "2. **Too many CV folds** ‚Üí Slow training, no benefit\n",
        "3. **Not logging to MLflow** ‚Üí Lost experiments\n",
        "4. **Huge param grids** ‚Üí Combinatorial explosion\n",
        "5. **Not saving the best model** ‚Üí Can't reproduce\n",
        "\n",
        "### üí° Pro Tips:\n",
        "\n",
        "- Use `parallelism` parameter in CrossValidator for faster training\n",
        "- Start with small param grid, expand based on results\n",
        "- Always evaluate on holdout TEST set (not validation)\n",
        "- Use MLflow Model Registry for production deployment\n",
        "- Save both the pipeline AND the fitted model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b21d2b2d",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### What we achieved:\n",
        "\n",
        "- **Pipeline Definition**: Created end-to-end workflow (Impute ‚Üí Encode ‚Üí Scale ‚Üí Model)\n",
        "- **MLflow Tracking**: Logged parameters, metrics, and model artifacts\n",
        "- **CrossValidator**: Automated hyperparameter search with 3-fold CV\n",
        "- **Model Persistence**: Saved pipeline for production use\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "| # | Principle |\n",
        "|---|-----------|\n",
        "| 1 | **Pipelines prevent data leakage** - fit on train, transform on test |\n",
        "| 2 | **MLflow is essential** - track all experiments |\n",
        "| 3 | **CrossValidator automates tuning** - finds best hyperparameters |\n",
        "| 4 | **Save entire pipeline** - includes all preprocessing |\n",
        "| 5 | **Evaluate on TEST only once** - final unbiased estimate |\n",
        "\n",
        "### MLflow Artifacts Created:\n",
        "\n",
        "| Artifact | Location | Purpose |\n",
        "|----------|----------|---------|\n",
        "| Experiment | `/Users/{user}/dp4ml_pipeline_demo` | Group runs |\n",
        "| Run | Auto-generated | Track this training |\n",
        "| Model | `/Users/{user}/dp4ml_pipeline_model` | Production deployment |\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "üìö **Next Module:** Module 7 - Feature Store & MLflow (production ML)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f1c2de0",
      "metadata": {},
      "source": [
        "## Cleanup\n",
        "\n",
        "Optionally remove demo artifacts created during exercises:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1de704d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup - remove demo artifacts created in this notebook\n",
        "\n",
        "# Uncomment the lines below to remove demo artifacts:\n",
        "\n",
        "# import shutil\n",
        "# shutil.rmtree(model_path, ignore_errors=True)\n",
        "# mlflow.delete_experiment(mlflow.get_experiment_by_name(experiment_path).experiment_id)\n",
        "\n",
        "# print(\"‚úÖ All demo artifacts removed\")\n",
        "\n",
        "print(\"‚ÑπÔ∏è Cleanup disabled (uncomment code to remove demo artifacts)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33a432eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create CrossValidator\n",
        "# numFolds=3 means we do 3-fold cross-validation for each param combo\n",
        "# Total fits = len(paramGrid) * numFolds = 9 * 3 = 27 models!\n",
        "\n",
        "crossval = CrossValidator(\n",
        "    estimator=pipeline_tune,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=RegressionEvaluator(labelCol=\"salary\", metricName=\"rmse\"),\n",
        "    numFolds=3,\n",
        "    parallelism=4  # Train 4 models in parallel (faster on clusters)\n",
        ")\n",
        "\n",
        "print(\"CrossValidator configured. Training will evaluate 27 models...\")\n",
        "\n",
        "# Fit CrossValidator (This takes longer!)\n",
        "cv_model = crossval.fit(train_df)\n",
        "\n",
        "# Get best model\n",
        "best_model = cv_model.bestModel\n",
        "print(\"‚úÖ Best model found!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c03d6771",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "# We reuse our pipeline but replace the model stage to allow tuning\n",
        "# Let's create a fresh pipeline for tuning\n",
        "lr_tune = LinearRegression(labelCol=\"salary\", featuresCol=\"features\")\n",
        "\n",
        "pipeline_tune = Pipeline(stages=[imputer, indexer, encoder, assembler, scaler, lr_tune])\n",
        "\n",
        "# Define the Parameter Grid\n",
        "# We will try different values of regParam (L2 regularization) and elasticNetParam (L1 vs L2 mix)\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(lr_tune.regParam, [0.01, 0.1, 0.5]) \\\n",
        "    .addGrid(lr_tune.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
        "    .build()\n",
        "\n",
        "print(f\"Number of hyperparameter combinations to test: {len(paramGrid)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b084e007",
      "metadata": {},
      "source": [
        "## Section 3: Hyperparameter Tuning with CrossValidator\n",
        "\n",
        "**Why tune hyperparameters?**\n",
        "In the previous example, we used default settings for `LinearRegression`. But models have \"knobs\" (hyperparameters) that can drastically change performance (e.g., `regParam` for regularization).\n",
        "\n",
        "**CrossValidator** automates this:\n",
        "1.  Define a **ParamGrid** (list of hyperparameters to try).\n",
        "2.  CrossValidator trains $k$ models for each combination (k-fold).\n",
        "3.  It picks the best model based on the evaluation metric."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
