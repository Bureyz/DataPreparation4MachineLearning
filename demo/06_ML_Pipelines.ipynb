{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d67373d4-a2c7-4282-a859-635015697ae0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Module 6: Machine Learning Pipelines\n",
    "\n",
    "**Training Objective:** Master Spark ML Pipelines to create reproducible, production-ready ML workflows with MLflow tracking.\n",
    "\n",
    "**Scope:**\n",
    "- Pipeline Concepts: Why use Pipelines?\n",
    "- Defining Stages: Chaining Imputers, Encoders, Scalers, and Models\n",
    "- MLflow Tracking: Logging experiments, parameters, and metrics\n",
    "- Hyperparameter Tuning: Using CrossValidator for automatic model selection\n",
    "- Model Persistence: Saving the pipeline for production use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bace3dea-02ab-4464-918f-b4ceb8c62ab5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Context and Requirements\n",
    "\n",
    "- **Training day:** Day 1 - Data Preparation Fundamentals\n",
    "- **Notebook type:** Demo\n",
    "- **Technical requirements:**\n",
    "  - Databricks Runtime 14.x LTS or newer\n",
    "  - Unity Catalog enabled\n",
    "  - MLflow enabled (default in Databricks)\n",
    "  - Permissions: CREATE TABLE, SELECT, MODIFY\n",
    "- **Dependencies:** `02_Data_Splitting.ipynb` (creates `customer_train`, `customer_test` tables)\n",
    "- **Execution time:** ~30 minutes\n",
    "\n",
    "> **Note:** This module brings together all previous concepts into a production-ready workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2252c80d-e352-48fe-82b7-66ab3a2e2bdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Theoretical Background\n",
    "\n",
    "**Spark ML Pipelines:**\n",
    "A Pipeline is a sequence of stages (Transformers and Estimators) executed in order. This ensures:\n",
    "1. Consistent data transformation between training and inference\n",
    "2. No data leakage (transformers fit only on training data)\n",
    "3. Easy saving/loading of the entire workflow\n",
    "\n",
    "**CrossValidator:**\n",
    "Performs k-fold cross-validation with hyperparameter tuning:\n",
    "- Splits training data into k folds\n",
    "- Trains model on k-1 folds, validates on remaining fold\n",
    "- Repeats for all combinations of hyperparameters\n",
    "- Total models trained: `len(paramGrid) × numFolds`\n",
    "\n",
    "**MLflow with Unity Catalog Models:**\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Model Registry** | `catalog.schema.model_name` format |\n",
    "| **Versioning** | Automatic version tracking (v1, v2, ...) |\n",
    "| **Aliases** | `@champion`, `@challenger` for deployment stages |\n",
    "| **Governance** | Unity Catalog permissions apply |\n",
    "| **Lineage** | Track data and model dependencies |\n",
    "\n",
    "**⚠️ Unity Catalog requires Model Signature:**\n",
    "\n",
    "Unity Catalog models **MUST** include a signature (input/output schema). Without it, registration fails.\n",
    "\n",
    "```python\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Infer signature from training data and predictions\n",
    "signature = infer_signature(train_df.toPandas(), predictions.select(\"prediction\").toPandas())\n",
    "\n",
    "# Register with signature\n",
    "mlflow.spark.log_model(\n",
    "    model, \n",
    "    \"model\", \n",
    "    signature=signature,\n",
    "    input_example=train_df.limit(5).toPandas(),\n",
    "    registered_model_name=\"catalog.schema.model\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12754d2a-cd81-4bfe-a09a-08ff7ea79b2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Per-User Isolation\n",
    "\n",
    "Run the initialization script for per-user catalog and schema isolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3189da62-9293-44b8-b06f-e52717ca05cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29214168-8795-4f82-b507-8c9da48ddb8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Import Libraries and Load Data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f82d870-ef82-4b9e-896c-e6d5f93d1271",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Imputer, StringIndexer, OneHotEncoder, VectorAssembler, RobustScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load Raw Split Data (We start from scratch in the pipeline!)\n",
    "train_df = spark.table(\"customer_train\")\n",
    "test_df = spark.table(\"customer_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "648d065a-0304-4d69-98de-132cd9b82b9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 1: Defining Pipeline Stages\n",
    "\n",
    "**Why use a Pipeline?**\n",
    "1.  **Prevention of Data Leakage:** When we calculate things like \"Mean\" for imputation or \"Max\" for scaling, we must calculate them **only on the Training set** and apply them to the Test set. A Pipeline ensures `fit()` is called on Train and `transform()` on Test.\n",
    "2.  **Reproducibility:** It bundles all preprocessing steps and the model into a single artifact.\n",
    "3.  **Simplicity:** You can save/load the entire workflow as one object.\n",
    "\n",
    "We will reconstruct our manual steps into a reusable Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a64e5782-9d90-4702-a96e-f338330a781b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Imputation\n",
    "imputer = Imputer(inputCols=[\"age\", \"salary\"], outputCols=[\"age_imp\", \"salary_imp\"]).setStrategy(\"median\")\n",
    "\n",
    "# 2. Encoding\n",
    "indexer = StringIndexer(inputCol=\"country\", outputCol=\"country_idx\", handleInvalid=\"keep\")\n",
    "encoder = OneHotEncoder(inputCols=[\"country_idx\"], outputCols=[\"country_vec\"])\n",
    "\n",
    "# 3. Assembly\n",
    "assembler = VectorAssembler(inputCols=[\"age_imp\", \"salary_imp\", \"country_vec\"], outputCol=\"features_raw\")\n",
    "\n",
    "# 4. Scaling (RobustScaler because we have outliers!)\n",
    "scaler = RobustScaler(inputCol=\"features_raw\", outputCol=\"features\")\n",
    "\n",
    "# 5. Model (Predicting Salary based on Age and Country - just for demo)\n",
    "lr = LinearRegression(labelCol=\"salary\", featuresCol=\"features\")\n",
    "\n",
    "# --- The Pipeline ---\n",
    "pipeline = Pipeline(stages=[imputer, indexer, encoder, assembler, scaler, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3599b4c-f1e5-48e3-be4e-3b89024a4831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 2: Training with MLflow\n",
    "\n",
    "We use `mlflow.start_run()` to track this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98a8ee38-b17f-48a2-853a-97cf5ff1ef25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set MLflow to use Unity Catalog for model registry\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Import signature inference for Unity Catalog (REQUIRED!)\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Set Experiment\n",
    "username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "experiment_path = f\"/Users/{username}/dp4ml_pipeline_demo\"\n",
    "mlflow.set_experiment(experiment_path)\n",
    "\n",
    "# Model name for Unity Catalog\n",
    "model_name = f\"{catalog_name}.{schema_name}.salary_prediction_model\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"salary_prediction_v1\"):\n",
    "    \n",
    "    # Log Parameters\n",
    "    mlflow.log_param(\"model\", \"LinearRegression\")\n",
    "    mlflow.log_param(\"scaler\", \"RobustScaler\")\n",
    "    \n",
    "    # Fit Pipeline\n",
    "    print(\"Training Pipeline...\")\n",
    "    model = pipeline.fit(train_df)\n",
    "    \n",
    "    # Evaluate\n",
    "    predictions = model.transform(test_df)\n",
    "    evaluator = RegressionEvaluator(labelCol=\"salary\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    \n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    \n",
    "    # Log Metrics\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    \n",
    "    # Infer model signature from input and output data\n",
    "    # Unity Catalog REQUIRES signature for model registration\n",
    "    input_example = train_df.limit(5).toPandas()\n",
    "    signature = infer_signature(\n",
    "        train_df.toPandas(),\n",
    "        predictions.select(\"prediction\").toPandas()\n",
    "    )\n",
    "    \n",
    "    # Log Model and Register to Unity Catalog with signature\n",
    "    mlflow.spark.log_model(\n",
    "        model, \n",
    "        \"model\",\n",
    "        signature=signature,\n",
    "        input_example=input_example,\n",
    "        registered_model_name=model_name  # Auto-register to UC\n",
    "    )\n",
    "    \n",
    "    print(f\"Model registered to Unity Catalog: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3678cf06-558d-496d-a0e8-1e9c61c6f5ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register the Pipeline Model in Unity Catalog\n",
    "# Unity Catalog Models provide governance, versioning, and lineage tracking\n",
    "\n",
    "# Set registry to Unity Catalog\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Model name in Unity Catalog format: catalog.schema.model_name\n",
    "model_name = f\"{catalog_name}.{schema_name}.salary_prediction_model\"\n",
    "\n",
    "# Register the model\n",
    "model_info = mlflow.register_model(\n",
    "    model_uri=f\"runs:/{mlflow.active_run().info.run_id if mlflow.active_run() else None}/model\",\n",
    "    name=model_name\n",
    ")\n",
    "\n",
    "print(f\"Model registered in Unity Catalog: {model_name}\")\n",
    "print(f\"Model version: {model_info.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53ba3ca0-de51-4a6e-a087-af4ec75d47a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 3: Hyperparameter Tuning with CrossValidator\n",
    "\n",
    "**Why tune hyperparameters?**\n",
    "In the previous example, we used default settings for `LinearRegression`. But models have \"knobs\" (hyperparameters) that can drastically change performance (e.g., `regParam` for regularization).\n",
    "\n",
    "**CrossValidator** automates this:\n",
    "1.  Define a **ParamGrid** (list of hyperparameters to try).\n",
    "2.  CrossValidator trains $k$ models for each combination (k-fold).\n",
    "3.  It picks the best model based on the evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c8f3ca6-3733-4b5a-9ec7-052f1342fce5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# We reuse our pipeline but replace the model stage to allow tuning\n",
    "# Let's create a fresh pipeline for tuning\n",
    "lr_tune = LinearRegression(labelCol=\"salary\", featuresCol=\"features\")\n",
    "\n",
    "pipeline_tune = Pipeline(stages=[imputer, indexer, encoder, assembler, scaler, lr_tune])\n",
    "\n",
    "# Define the Parameter Grid\n",
    "# We will try different values of regParam (L2 regularization) and elasticNetParam (L1 vs L2 mix)\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr_tune.regParam, [0.01, 0.1, 0.5]) \\\n",
    "    .addGrid(lr_tune.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "print(f\"Number of hyperparameter combinations to test: {len(paramGrid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cef2839f-3c22-4550-92e1-eca1deccc9fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create CrossValidator\n",
    "# numFolds=3 means we do 3-fold cross-validation for each param combo\n",
    "# Total fits = len(paramGrid) * numFolds = 9 * 3 = 27 models!\n",
    "\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline_tune,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=RegressionEvaluator(labelCol=\"salary\", metricName=\"rmse\"),\n",
    "    numFolds=3,\n",
    "    parallelism=4  # Train 4 models in parallel (faster on clusters)\n",
    ")\n",
    "\n",
    "print(\"CrossValidator configured. Training will evaluate 27 models...\")\n",
    "\n",
    "# Fit CrossValidator (This takes longer!)\n",
    "cv_model = crossval.fit(train_df)\n",
    "\n",
    "# Get best model\n",
    "best_model = cv_model.bestModel\n",
    "print(\"Best model found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21470dcc-4ea3-4295-8b4f-2d6c82b41a97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "854be9d2-2761-4a90-b88d-f691a06abe60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the best model on TEST set\n",
    "predictions_cv = best_model.transform(test_df)\n",
    "rmse_cv = evaluator.evaluate(predictions_cv)\n",
    "\n",
    "# Extract the best hyperparameters from the LinearRegression stage\n",
    "# The last stage in the pipeline is the LinearRegressionModel\n",
    "lr_model_stage = best_model.stages[-1]\n",
    "\n",
    "# Compute R2 on test set\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"salary\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2_cv = evaluator_r2.evaluate(predictions_cv)\n",
    "\n",
    "print(f\"Best Model RMSE: {rmse_cv}\")\n",
    "print(f\"Best Model R2: {r2_cv}\")\n",
    "print(f\"Best regParam: {lr_model_stage.getRegParam()}\")\n",
    "print(f\"Best elasticNetParam: {lr_model_stage.getElasticNetParam()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00affcf9-bca0-495e-addf-d1ef1ce99cdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "These lines print the evaluation metrics and hyperparameters of the best model found by cross-validation:\n",
    "\n",
    "- **Best Model RMSE: `{rmse_cv}`**  \n",
    "  Shows the Root Mean Squared Error (RMSE) of the best model on the test set. Lower RMSE means better predictive accuracy.\n",
    "\n",
    "- **Best Model R2: `{r2_cv}`**  \n",
    "  Shows the R-squared (coefficient of determination) of the best model on the test set. Higher R2 (closer to 1) means the model explains more variance in the target variable.\n",
    "\n",
    "- **Best regParam: `{lr_model_stage.getRegParam()}`**  \n",
    "  Displays the value of the regularization parameter (regParam) chosen by cross-validation. This controls the amount of regularization applied to the model to prevent overfitting.\n",
    "\n",
    "- **Best elasticNetParam: `{lr_model_stage.getElasticNetParam()}`**  \n",
    "  Displays the value of the elasticNet mixing parameter (elasticNetParam) chosen by cross-validation. This controls the mix between L1 (lasso) and L2 (ridge) regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c2cfcf8-b12f-4763-bee1-29eb9d1b750a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Best Practices\n",
    "\n",
    "### Pipeline Strategy Guide:\n",
    "\n",
    "| Component | Best Practice | Why |\n",
    "|-----------|--------------|-----|\n",
    "| **Order of stages** | Impute → Encode → Scale → Model | Data dependencies |\n",
    "| **handleInvalid** | Use \"keep\" for StringIndexer | Handle new categories |\n",
    "| **Scaler choice** | RobustScaler for outliers | Most robust default |\n",
    "| **CrossValidator folds** | 3-5 for large data, 5-10 for small | Balance bias/variance |\n",
    "| **MLflow logging** | Log params, metrics, AND model | Full reproducibility |\n",
    "\n",
    "### Common Mistakes to Avoid:\n",
    "\n",
    "1. **Fitting pipeline on all data** → Data leakage\n",
    "2. **Too many CV folds** → Slow training, no benefit\n",
    "3. **Not logging to MLflow** → Lost experiments\n",
    "4. **Huge param grids** → Combinatorial explosion\n",
    "5. **Not saving the best model** → Can't reproduce\n",
    "\n",
    "### Pro Tips:\n",
    "\n",
    "- Use `parallelism` parameter in CrossValidator for faster training\n",
    "- Start with small param grid, expand based on results\n",
    "- Always evaluate on holdout TEST set (not validation)\n",
    "- Use MLflow Model Registry for production deployment\n",
    "- Save both the pipeline AND the fitted model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7ec5583-7def-4b0c-9ea3-17899891f8ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "### What we achieved:\n",
    "\n",
    "- **Pipeline Definition**: Created end-to-end workflow (Impute → Encode → Scale → Model)\n",
    "- **MLflow Tracking**: Logged parameters, metrics, and model artifacts\n",
    "- **Unity Catalog Models**: Registered model with governance and versioning\n",
    "- **CrossValidator**: Automated hyperparameter search with 3-fold CV\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "| # | Principle |\n",
    "|---|-----------|\n",
    "| 1 | **Pipelines prevent data leakage** - fit on train, transform on test |\n",
    "| 2 | **MLflow is essential** - track all experiments |\n",
    "| 3 | **Unity Catalog Models** - governance, versioning, lineage |\n",
    "| 4 | **CrossValidator automates tuning** - finds best hyperparameters |\n",
    "| 5 | **Evaluate on TEST only once** - final unbiased estimate |\n",
    "\n",
    "### Unity Catalog Artifacts Created:\n",
    "\n",
    "| Artifact | Location | Purpose |\n",
    "|----------|----------|---------|\n",
    "| Experiment | `/Users/{user}/dp4ml_pipeline_demo` | Group runs |\n",
    "| Model | `{catalog}.{schema}.salary_prediction_model` | Production deployment |\n",
    "| Versions | v1, v2, ... | Track model iterations |\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "**Next Module:** Module 7 - Feature Store & MLflow (production ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46f57225-7c95-4201-afe0-dc1ccd71a69e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "Optionally remove demo artifacts created during exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2502c8fb-9ca4-4fdc-b23f-c95c432de57d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup - remove demo artifacts created in this notebook\n",
    "\n",
    "# Uncomment the lines below to remove demo artifacts:\n",
    "\n",
    "# import shutil\n",
    "# shutil.rmtree(model_path, ignore_errors=True)\n",
    "# mlflow.delete_experiment(mlflow.get_experiment_by_name(experiment_path).experiment_id)\n",
    "\n",
    "# print(\"All demo artifacts removed\")\n",
    "\n",
    "print(\"Cleanup disabled (uncomment code to remove demo artifacts)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06_ML_Pipelines",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
