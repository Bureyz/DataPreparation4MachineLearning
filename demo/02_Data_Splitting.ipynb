{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4e15d555",
      "metadata": {},
      "source": [
        "# Module 2: Data Splitting Strategies\n",
        "\n",
        "**Training Objective:** Master data splitting techniques to prevent overfitting and data leakage in ML projects.\n",
        "\n",
        "**Scope:**\n",
        "- Random Split: Standard Train/Validation/Test split\n",
        "- Stratified Sampling: Handling imbalanced datasets\n",
        "- Time-based Split: Correctly splitting time-series data\n",
        "- Cross-Validation concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "757196c9",
      "metadata": {},
      "source": [
        "## Context and Requirements\n",
        "\n",
        "- **Training day:** Day 1 - Data Preparation Fundamentals\n",
        "- **Notebook type:** Demo\n",
        "- **Technical requirements:**\n",
        "  - Databricks Runtime 14.x LTS or newer\n",
        "  - Unity Catalog enabled\n",
        "  - Permissions: CREATE TABLE, SELECT, MODIFY\n",
        "- **Dependencies:** `01_EDA_and_Validation.ipynb` (creates `customer_bronze` table)\n",
        "- **Execution time:** ~20 minutes\n",
        "\n",
        "> **Important:** This notebook saves split data for use in subsequent modules."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bea9f0f9",
      "metadata": {},
      "source": [
        "## Theoretical Introduction\n",
        "\n",
        "**Why do we split data?**\n",
        "\n",
        "If we test our model on the same data we trained it on, it will \"cheat\" (memorize the answers). This is called **Overfitting**.\n",
        "\n",
        "**The Three Sets:**\n",
        "\n",
        "| Set | Percentage | Purpose | When Used |\n",
        "|-----|------------|---------|-----------|\n",
        "| **Training** | 60-80% | Model learns patterns (weights) | During training |\n",
        "| **Validation** | 10-20% | Tune hyperparameters | During development |\n",
        "| **Test** | 10-20% | Final evaluation (\"exam\") | Only once at the end |\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "| Technique | When to Use | Problem it Solves |\n",
        "|-----------|-------------|-------------------|\n",
        "| **Random Split** | Default for most cases | Basic train/test separation |\n",
        "| **Stratified Split** | Imbalanced classes (e.g., 1% fraud) | Preserves class distribution |\n",
        "| **Time-based Split** | Time-series data | Prevents \"time travel\" data leakage |\n",
        "| **Cross-Validation** | Limited data, need robust estimate | Uses all data for validation |\n",
        "\n",
        "**Data Leakage Warning:**\n",
        "> ‚ö†Ô∏è Never use test data for any preprocessing decisions (imputation, scaling parameters). Always fit transformers on training data only!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fafb1309",
      "metadata": {},
      "source": [
        "## Per-User Isolation\n",
        "\n",
        "Run the initialization script for per-user catalog and schema isolation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cd79033",
      "metadata": {},
      "outputs": [],
      "source": [
        "%run ./00_Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44904010",
      "metadata": {},
      "source": [
        "**Load Data:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eadad8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Data\n",
        "df = spark.table(\"customer_bronze\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22ab690d",
      "metadata": {},
      "source": [
        "## Section 1: Train / Validation / Test Split\n",
        "\n",
        "**Why three sets?**\n",
        "If we test our model on the same data we trained it on, it will cheat (memorize the answers). This is **Overfitting**.\n",
        "To prevent this, we use a \"Hold-out\" strategy:\n",
        "\n",
        "1.  **Training Set (60-80%)**: The model sees this data and learns patterns (weights).\n",
        "2.  **Validation Set (10-20%)**: Used to tune \"Hyperparameters\" (e.g., tree depth, learning rate). We evaluate the model here *during* development.\n",
        "3.  **Test Set (10-20%)**: The \"Final Exam\". Used **only once** at the very end to estimate how the model will perform in the real world. We *never* tune based on this set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5479560e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random Split\n",
        "train_df, val_df, test_df = df.randomSplit([0.6, 0.2, 0.2], seed=42)\n",
        "\n",
        "print(f\"Total: {df.count()}\")\n",
        "print(f\"Train: {train_df.count()}\")\n",
        "print(f\"Val:   {val_df.count()}\")\n",
        "print(f\"Test:  {test_df.count()}\")\n",
        "\n",
        "# Save for next modules\n",
        "# This is crucial! We save the split data so subsequent notebooks (Imputation, Feature Eng) \n",
        "# work on the exact same Training set, preventing Data Leakage from Test data.\n",
        "train_df.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.customer_train\")\n",
        "test_df.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.customer_test\")\n",
        "print(\"‚úÖ Saved 'customer_train' and 'customer_test' tables.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9935a1e3",
      "metadata": {},
      "source": [
        "## Section 2: Stratified Sampling\n",
        "\n",
        "**The Imbalance Problem:**\n",
        "Imagine a Fraud Detection dataset where only 1% of transactions are fraud.\n",
        "If we do a random split, the Test set might end up with **zero** fraud cases just by bad luck. The model would look perfect (100% accuracy) but fail in production.\n",
        "\n",
        "**Stratification** forces the split to respect the original distribution of classes (e.g., ensuring exactly 1% fraud in Train, Validation, and Test)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68c581d5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's simulate a rare target variable 'is_vip' based on salary\n",
        "from pyspark.sql.functions import when, col\n",
        "\n",
        "df_strat = df.withColumn(\"is_vip\", when(col(\"salary\") > 150000, 1).otherwise(0))\n",
        "\n",
        "# Check distribution\n",
        "display(df_strat.groupBy(\"is_vip\").count())\n",
        "\n",
        "# Stratified Split using sampleBy\n",
        "# We define the fraction of each class we want in the TRAINING set (e.g., 80%)\n",
        "fractions = {0: 0.8, 1: 0.8}\n",
        "train_strat = df_strat.stat.sampleBy(\"is_vip\", fractions, seed=42)\n",
        "test_strat = df_strat.subtract(train_strat)\n",
        "\n",
        "print(\"Train Distribution:\")\n",
        "display(train_strat.groupBy(\"is_vip\").count())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8060c1f",
      "metadata": {},
      "source": [
        "## Section 3: Cross-Validation (Concept)\n",
        "\n",
        "In Cross-Validation (k-fold), we split the data into $k$ folds. We train $k$ times, each time using $k-1$ folds for training and 1 fold for validation.\n",
        "\n",
        "*Note: In Spark ML, `CrossValidator` is an object that wraps the model and handles this automatically during training. We don't manually split the DataFrame into folds usually.*\n",
        "\n",
        "```python\n",
        "# Concept Code (We will use this in the Pipeline notebook - Module 6)\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "# cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1f0e824",
      "metadata": {},
      "source": [
        "## Section 4: Time-based Split\n",
        "\n",
        "**The Time Travel Problem (Data Leakage):**\n",
        "In time-series problems (e.g., Stock Price, Sales Forecasting), the order of data matters.\n",
        "If we do a random split, the model might learn from \"future\" data (e.g., sales in December) to predict \"past\" data (sales in January). This is impossible in real life.\n",
        "\n",
        "**Solution:**\n",
        "We must split by time.\n",
        "- **Train:** Oldest data (e.g., Jan - Oct).\n",
        "- **Test:** Newest data (e.g., Nov - Dec)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ec63a1d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import to_date, col, expr\n",
        "\n",
        "# Ensure date format\n",
        "df_time = df.withColumn(\"registration_date\", to_date(\"registration_date\"))\n",
        "\n",
        "# Dynamic Split Date: Let's take the date that separates the oldest 80% from the newest 20%\n",
        "# We calculate the 80th percentile of the date\n",
        "split_date_row = df_time.selectExpr(\"percentile_approx(to_unix_timestamp(registration_date), 0.8)\").collect()[0][0]\n",
        "split_date = spark.sql(f\"select to_date(from_unixtime({split_date_row}))\").collect()[0][0]\n",
        "\n",
        "print(f\"Dynamic Split Date (80% cutoff): {split_date}\")\n",
        "\n",
        "train_time = df_time.filter(col(\"registration_date\") < split_date)\n",
        "test_time = df_time.filter(col(\"registration_date\") >= split_date)\n",
        "\n",
        "print(f\"Train (Historical): {train_time.count()}\")\n",
        "print(f\"Test (Recent): {test_time.count()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae385e87",
      "metadata": {},
      "source": [
        "## Best Practices\n",
        "\n",
        "### üéØ Splitting Strategy Guide:\n",
        "\n",
        "| Scenario | Recommended Approach | Rationale |\n",
        "|----------|---------------------|-----------|\n",
        "| General classification/regression | 60/20/20 random split | Standard approach |\n",
        "| Imbalanced classes (<5% minority) | Stratified split | Preserves class distribution |\n",
        "| Time-series / forecasting | Time-based split | Prevents data leakage |\n",
        "| Small dataset (<1000 rows) | Cross-validation (5-10 folds) | Uses all data efficiently |\n",
        "| Large dataset (>1M rows) | Simple random split | Stratification less critical |\n",
        "\n",
        "### ‚ö†Ô∏è Common Mistakes to Avoid:\n",
        "\n",
        "1. **Using test data for feature engineering** ‚Üí Data leakage\n",
        "2. **Shuffling time-series before split** ‚Üí Invalid evaluation\n",
        "3. **Not setting random seed** ‚Üí Non-reproducible results\n",
        "4. **Using validation set for final evaluation** ‚Üí Overly optimistic results\n",
        "5. **Forgetting to stratify imbalanced datasets** ‚Üí Unrepresentative splits\n",
        "\n",
        "### üí° Pro Tips:\n",
        "\n",
        "- Always set `seed=42` (or any fixed number) for reproducibility\n",
        "- Save split data as tables for pipeline consistency\n",
        "- Document your split ratios and strategy\n",
        "- For production: consider rolling window validation for time-series"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bbb1d59",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### What we achieved:\n",
        "\n",
        "- **Random Split**: Created train/validation/test sets with `randomSplit()`\n",
        "- **Stratified Sampling**: Used `sampleBy()` to preserve class distribution\n",
        "- **Cross-Validation**: Understood k-fold concept for robust evaluation\n",
        "- **Time-based Split**: Applied temporal split to prevent data leakage\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "| # | Principle |\n",
        "|---|-----------|\n",
        "| 1 | **Always split before preprocessing** - fit transformers on train only |\n",
        "| 2 | **Use stratification for imbalanced data** - ensures representative splits |\n",
        "| 3 | **Respect temporal order for time-series** - no \"time travel\" allowed |\n",
        "| 4 | **Test set is sacred** - use it only once at the very end |\n",
        "| 5 | **Save splits as tables** - ensures consistency across pipeline |\n",
        "\n",
        "### Data Pipeline Status:\n",
        "\n",
        "| Table | Created | Used By |\n",
        "|-------|---------|---------|\n",
        "| `customer_bronze` | Module 1 | This module |\n",
        "| `customer_train` | ‚úÖ This module | Modules 3-7 |\n",
        "| `customer_test` | ‚úÖ This module | Module 6 (evaluation) |\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "üìö **Next Module:** Module 3 - Data Imputing (handling missing values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3665d87",
      "metadata": {},
      "source": [
        "## Cleanup\n",
        "\n",
        "Optionally remove demo tables created during exercises:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f355c5ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup - remove demo tables created in this notebook\n",
        "\n",
        "# ‚ö†Ô∏è WARNING: Do NOT delete customer_train and customer_test - they are needed for subsequent modules!\n",
        "\n",
        "# Uncomment the lines below to remove demo tables:\n",
        "\n",
        "# spark.sql(f\"DROP TABLE IF EXISTS {catalog_name}.{schema_name}.customer_train\")\n",
        "# spark.sql(f\"DROP TABLE IF EXISTS {catalog_name}.{schema_name}.customer_test\")\n",
        "\n",
        "# print(\"‚úÖ All demo tables removed\")\n",
        "\n",
        "print(\"‚ÑπÔ∏è Cleanup disabled (uncomment code to remove demo tables)\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
