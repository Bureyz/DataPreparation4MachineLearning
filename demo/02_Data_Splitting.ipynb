{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a1163e1-979d-465b-8d72-6ccf31e6c4fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Module 2: Data Splitting Strategies\n",
    "\n",
    "## Business Context: TechCorp HR Analytics (continued)\n",
    "\n",
    "**Where We Are:**\n",
    "In Module 1, we explored TechCorp's employee data and discovered patterns: salary depends on age (experience), country (market rates), and recruitment source. Now we need to split this data properly before training our Salary Prediction Model.\n",
    "\n",
    "**Why This Matters for HR:**\n",
    "If we test the model on data it has already \"seen\", it will appear accurate but fail on new hires. Proper splitting ensures:\n",
    "1. **Fair evaluation:** Model is tested on truly unseen employees\n",
    "2. **No cheating:** Prevents memorizing individual records\n",
    "3. **Realistic performance:** Reflects how well the model will work on future hires\n",
    "\n",
    "---\n",
    "\n",
    "**Training Objective:** Master data splitting techniques to prevent overfitting and data leakage in ML projects.\n",
    "\n",
    "**Scope:**\n",
    "- Random Split: Standard Train/Validation/Test split\n",
    "- Stratified Sampling: Handling imbalanced datasets\n",
    "- Time-based Split: Correctly splitting time-series data\n",
    "- Cross-Validation concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bccdab9-c735-461f-93a6-7e30fca63de8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Context and Requirements\n",
    "\n",
    "- **Training day:** Day 1 - Data Preparation Fundamentals\n",
    "- **Notebook type:** Demo\n",
    "- **Technical requirements:**\n",
    "  - Databricks Runtime 14.x LTS or newer\n",
    "  - Unity Catalog enabled\n",
    "  - Permissions: CREATE TABLE, SELECT, MODIFY\n",
    "- **Dependencies:** `01_EDA_and_Validation.ipynb` (creates `customer_bronze` table)\n",
    "- **Execution time:** ~20 minutes\n",
    "\n",
    "> **Important:** This notebook saves split data for use in subsequent modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69224e88-042a-4321-b56f-c17dc64227c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Theoretical Introduction\n",
    "\n",
    "**Why do we split data?**\n",
    "\n",
    "If we test our model on the same data we trained it on, it will \"cheat\" (memorize the answers). This is called **Overfitting**.\n",
    "\n",
    "**The Three Sets:**\n",
    "\n",
    "| Set | Percentage | Purpose | When Used |\n",
    "|-----|------------|---------|-----------|\n",
    "| **Training** | 60-80% | Model learns patterns (weights) | During training |\n",
    "| **Validation** | 10-20% | Tune hyperparameters | During development |\n",
    "| **Test** | 10-20% | Final evaluation (\"exam\") | Only once at the end |\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "| Technique | When to Use | Problem it Solves |\n",
    "|-----------|-------------|-------------------|\n",
    "| **Random Split** | Default for most cases | Basic train/test separation |\n",
    "| **Stratified Split** | Imbalanced classes (e.g., 1% fraud) | Preserves class distribution |\n",
    "| **Time-based Split** | Time-series data | Prevents \"time travel\" data leakage |\n",
    "| **Cross-Validation** | Limited data, need robust estimate | Uses all data for validation |\n",
    "\n",
    "**Data Leakage Warning:**\n",
    "> ️ Never use test data for any preprocessing decisions (imputation, scaling parameters). Always fit transformers on training data only!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aa02dac-e1dd-45d5-a129-2aff367d7da8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Per-User Isolation\n",
    "\n",
    "Run the initialization script for per-user catalog and schema isolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc388799-dea9-4431-83b4-137342e19ac5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b14d966-aed0-4f67-aa80-f683de1a80af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Load Data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7313b9dd-d2fb-43d8-8b7a-886fc50a2a2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = spark.table(\"customer_bronze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aadf114-7245-4baf-83bf-1d258cdd3caa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Basic Split - Train/Test (80/20)\n",
    "\n",
    "## Theory\n",
    "**Basic Split** divides data into two parts: training (80%) and test (20%).\n",
    "\n",
    "###  When to use:\n",
    "- **Large datasets** (>10,000 samples)\n",
    "- **Simple models** without hyperparameter tuning\n",
    "- **Quick prototyping** and baseline models\n",
    "\n",
    "###  Advantages:\n",
    "- **Simplicity** - easy to implement\n",
    "- **Speed** - minimal overhead\n",
    "- **Clarity** - clear structure\n",
    "\n",
    "###  Limitations:\n",
    "- **No tuning** - no validation set\n",
    "- **Variance** - results depend on random split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "592e08bd-e13e-492d-ba49-c20dcd9630a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split into training and test sets (80/20)\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "094cfd86-d18e-428d-ac1c-d821b10dc04f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**What is `seed=42`?**\n",
    "\n",
    "Setting `seed=42` (or any fixed number) ensures reproducibility in random operations (like data splitting or shuffling). Using a fixed seed means you'll get the same random results every time you run the code, making experiments consistent and results comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1289c5f-91b4-4685-b12e-10d5bba26005",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f95492c-903d-4333-93e6-8cbaba770316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a361637f-a25d-4e3e-b780-1ec76fa5b31f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 1: Train / Validation / Test Split\n",
    "\n",
    "**Why three sets?**\n",
    "If we test our model on the same data we trained it on, it will cheat (memorize the answers). This is **Overfitting**.\n",
    "To prevent this, we use a \"Hold-out\" strategy:\n",
    "\n",
    "1.  **Training Set (60-80%)**: The model sees this data and learns patterns (weights).\n",
    "2.  **Validation Set (10-20%)**: Used to tune \"Hyperparameters\" (e.g., tree depth, learning rate). We evaluate the model here *during* development.\n",
    "3.  **Test Set (10-20%)**: The \"Final Exam\". Used **only once** at the very end to estimate how the model will perform in the real world. We *never* tune based on this set.\n",
    "\n",
    "### Why do we use three sets?\n",
    "\n",
    "1. **Training Set**: Model learns patterns in the data\n",
    "2. **Validation Set**: We test different model configurations without \"looking\" at the test set\n",
    "3. **Test Set**: Final, objective evaluation of model performance\n",
    "\n",
    "### Process:\n",
    "1. Train model on **Training Set**\n",
    "2. Evaluate different hyperparameters on **Validation Set**\n",
    "3. Select the best configuration\n",
    "4. Final evaluation on **Test Set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fdd0e32-68ee-436c-922a-d051296f0d48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Three-way Split (60/20/20)\n",
    "```\n",
    "Dataset → Train (60%) + Validation (20%) + Test (20%)\n",
    "    ↓        ↓              ↓              ↓\n",
    "Training   Model      Hyperparameter    Final\n",
    " Data      Fitting      Tuning        Evaluation\n",
    "```\n",
    "- **Use case**: Model selection and hyperparameter optimization\n",
    "- **Pros**: Unbiased final evaluation, enables tuning\n",
    "- **Cons**: Reduces training data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6df832f3-e46d-44e0-87f9-0a60431a756e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Random Split\n",
    "train_df, val_df, test_df = df.randomSplit([0.6, 0.2, 0.2], seed=42)\n",
    "\n",
    "print(f\"Total: {df.count()}\")\n",
    "print(f\"Train: {train_df.count()}\")\n",
    "print(f\"Val:   {val_df.count()}\")\n",
    "print(f\"Test:  {test_df.count()}\")\n",
    "\n",
    "# Save for next modules\n",
    "# This is crucial! We save the split data so subsequent notebooks (Imputation, Feature Eng) \n",
    "# work on the exact same Training set, preventing Data Leakage from Test data.\n",
    "train_df.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.customer_train\")\n",
    "test_df.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.customer_test\")\n",
    "print(\" Saved 'customer_train' and 'customer_test' tables.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c325dda7-b361-474b-886d-7c4c5dbc9b65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 2: Stratified Sampling\n",
    "\n",
    "### Why Stratified Sampling?\n",
    "\n",
    "In imbalanced datasets (e.g. 1% fraud), a pure random split can cause:\n",
    "\n",
    "- Train/Validation/Test sets with too few or zero minority-class samples  \n",
    "- Over-optimistic metrics (e.g. high accuracy by always predicting the majority class)  \n",
    "- Unstable and non-comparable results between runs\n",
    "\n",
    "**Stratified sampling** ensures each split (Train, Validation, Test) preserves the original class proportions (e.g. ~1% fraud in every set).\n",
    "\n",
    "### Why do we use stratification?\n",
    "\n",
    "- Maintains class distribution in all splits  \n",
    "- Ensures the model sees minority cases during training  \n",
    "- Provides more reliable and comparable evaluation metrics  \n",
    "- Reduces risk of “degenerate” splits without the minority class\n",
    "\n",
    "### Process\n",
    "\n",
    "1. **Select target for stratification**  \n",
    "   - Use the classification label (e.g. `is_fraud`, `churn_flag`).\n",
    "\n",
    "2. **Inspect global class distribution**  \n",
    "   - Compute counts and percentages for each class.\n",
    "\n",
    "3. **Define split ratios**  \n",
    "   - Common: Train / Val / Test = 70% / 15% / 15% (or similar).\n",
    "\n",
    "4. **Apply stratified split**  \n",
    "   - Use a method that supports stratification:\n",
    "     - Single split: stratified train/val/test  \n",
    "     - Cross-validation: stratified k-fold\n",
    "\n",
    "5. **Validate distributions per split**  \n",
    "   - Check that class percentages in Train/Val/Test are close to the original dataset.\n",
    "\n",
    "6. **Handle special cases**  \n",
    "   - Time-series: respect temporal order, avoid random shuffling across time  \n",
    "   - Grouped data (e.g. customers): combine stratification with grouping to avoid leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "914bca4f-daa7-4f13-af27-4e7056ae9f01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df_vip_train = train_data.withColumn(\"is_vip\", when(col(\"salary\") > 150000, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc8d4fa6-fde6-404e-8e39-161947f53688",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_vip_train.groupBy(\"is_vip\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc787768-837d-44e9-8e4a-9a885fdbf024",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's simulate a rare target variable 'is_vip' based on salary\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df_strat = df.withColumn(\"is_vip\", when(col(\"salary\") > 150000, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aaf53c8-4058-447f-bcf9-01c2b439151b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check distribution\n",
    "display(df_strat.groupBy(\"is_vip\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a5672c5-98e2-4b31-bc58-2b3099e45eea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stratified Split using sampleBy\n",
    "# We define the fraction of each class we want in the TRAINING set (e.g., 80%)\n",
    "fractions = {0: 0.8, 1: 0.8}\n",
    "train_strat = df_strat.stat.sampleBy(\"is_vip\", fractions, seed=42)\n",
    "test_strat = df_strat.subtract(train_strat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25f1ea5f-9415-49ec-baa0-caf4e665e459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Train Distribution:\")\n",
    "display(train_strat.groupBy(\"is_vip\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b604129-7de1-4e6b-8795-ecc9a2621f39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Train Distribution:\")\n",
    "display(test_strat.groupBy(\"is_vip\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fe04465-95a0-408e-aee9-5ebad194c16f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 3: Cross-Validation (Concept)\n",
    "\n",
    "In Cross-Validation (k-fold), we split the data into $k$ folds. We train $k$ times, each time using $k-1$ folds for training and 1 fold for validation.\n",
    "\n",
    "*Note: In Spark ML, `CrossValidator` is an object that wraps the model and handles this automatically during training. We don't manually split the DataFrame into folds usually.*\n",
    "\n",
    "```python\n",
    "# Concept Code (We will use this in the Pipeline notebook - Module 6)\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\n",
    "```\n",
    "\n",
    "### Advantages:\n",
    "-  **Better data utilization** - every record is used for both training and testing\n",
    "-  **More stable evaluation** - averaging reduces the impact of randomness\n",
    "-  **Overfitting detection** - high variance between folds may indicate a problem\n",
    "\n",
    "### Disadvantages:\n",
    "-  **Computationally expensive** - training is performed K times\n",
    "-  **Longer duration** - especially for large models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0778d782-127e-481a-8461-63d209005676",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  Cross-Validation (K-fold)\n",
    "```\n",
    "Fold 1: [TEST ] [TRAIN] [TRAIN] [TRAIN] [TRAIN]\n",
    "Fold 2: [TRAIN] [TEST ] [TRAIN] [TRAIN] [TRAIN]\n",
    "Fold 3: [TRAIN] [TRAIN] [TEST ] [TRAIN] [TRAIN]\n",
    "...\n",
    "Final Score = Average(Fold1, Fold2, ..., FoldK)\n",
    "```\n",
    "- **Use case**: Robust model evaluation with limited data\n",
    "- **Pros**: Maximum data utilization, robust estimates\n",
    "- **Cons**: Computationally expensive (K times training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ceecd79-f592-400a-93c4-5f39efc32e5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 4: Time-based Split\n",
    "\n",
    "**The Time Travel Problem (Data Leakage):**\n",
    "In time-series problems (e.g., Stock Price, Sales Forecasting), the order of data matters.\n",
    "If we do a random split, the model might learn from \"future\" data (e.g., sales in December) to predict \"past\" data (sales in January). This is impossible in real life.\n",
    "\n",
    "**Solution:**\n",
    "We must split by time.\n",
    "- **Train:** Oldest data (e.g., Jan - Oct).\n",
    "- **Test:** Newest data (e.g., Nov - Dec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cff523ad-039b-4b32-891f-c0d6be8b0200",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, col, expr\n",
    "\n",
    "# Ensure date format\n",
    "df_time = df.withColumn(\"registration_date\", to_date(\"registration_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bff02db-eb70-4767-9bd7-1d8a1a02a215",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dynamic Split Date: Let's take the date that separates the oldest 80% from the newest 20%\n",
    "# We calculate the 80th percentile of the date\n",
    "split_date_row = df_time.selectExpr(\"percentile_approx(to_unix_timestamp(registration_date), 0.8)\").collect()[0][0]\n",
    "split_date = spark.sql(f\"select to_date(from_unixtime({split_date_row}))\").collect()[0][0]\n",
    "\n",
    "print(f\"Dynamic Split Date (80% cutoff): {split_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7140464-6b66-4d21-9a45-47f69e96b3ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "train_time = df_time.filter(col(\"registration_date\") < split_date)\n",
    "test_time = df_time.filter(col(\"registration_date\") >= split_date)\n",
    "\n",
    "print(f\"Train (Historical): {train_time.count()}\")\n",
    "print(f\"Test (Recent): {test_time.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ab6a2ac-235a-4167-b299-517c6b3359a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Best Practices\n",
    "\n",
    "###  Splitting Strategy Guide:\n",
    "\n",
    "| Scenario | Recommended Approach | Rationale |\n",
    "|----------|---------------------|-----------|\n",
    "| General classification/regression | 60/20/20 random split | Standard approach |\n",
    "| Imbalanced classes (<5% minority) | Stratified split | Preserves class distribution |\n",
    "| Time-series / forecasting | Time-based split | Prevents data leakage |\n",
    "| Small dataset (<1000 rows) | Cross-validation (5-10 folds) | Uses all data efficiently |\n",
    "| Large dataset (>1M rows) | Simple random split | Stratification less critical |\n",
    "\n",
    "### ️ Common Mistakes to Avoid:\n",
    "\n",
    "1. **Using test data for feature engineering** → Data leakage\n",
    "2. **Shuffling time-series before split** → Invalid evaluation\n",
    "3. **Not setting random seed** → Non-reproducible results\n",
    "4. **Using validation set for final evaluation** → Overly optimistic results\n",
    "5. **Forgetting to stratify imbalanced datasets** → Unrepresentative splits\n",
    "\n",
    "###  Pro Tips:\n",
    "\n",
    "- Always set `seed=42` (or any fixed number) for reproducibility\n",
    "- Save split data as tables for pipeline consistency\n",
    "- Document your split ratios and strategy\n",
    "- For production: consider rolling window validation for time-series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3225401-61cd-43be-a7c9-359fe13f7757",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "### What we achieved:\n",
    "\n",
    "- **Random Split**: Created train/validation/test sets with `randomSplit()`\n",
    "- **Stratified Sampling**: Used `sampleBy()` to preserve class distribution\n",
    "- **Cross-Validation**: Understood k-fold concept for robust evaluation\n",
    "- **Time-based Split**: Applied temporal split to prevent data leakage\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "| # | Principle |\n",
    "|---|-----------|\n",
    "| 1 | **Always split before preprocessing** - fit transformers on train only |\n",
    "| 2 | **Use stratification for imbalanced data** - ensures representative splits |\n",
    "| 3 | **Respect temporal order for time-series** - no \"time travel\" allowed |\n",
    "| 4 | **Test set is sacred** - use it only once at the very end |\n",
    "| 5 | **Save splits as tables** - ensures consistency across pipeline |\n",
    "\n",
    "### Data Pipeline Status:\n",
    "\n",
    "| Table | Created | Used By |\n",
    "|-------|---------|---------|\n",
    "| `customer_bronze` | Module 1 | This module |\n",
    "| `customer_train` |  This module | Modules 3-7 |\n",
    "| `customer_test` |  This module | Module 6 (evaluation) |\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    " **Next Module:** Module 3 - Data Imputing (handling missing values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99e81dde-89c7-4e9d-b8e9-8e1c154ce99c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "Optionally remove demo tables created during exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d0f38fe-fa16-4e6c-a87a-2e6b2aeb1b0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup - remove demo tables created in this notebook\n",
    "\n",
    "# ️ WARNING: Do NOT delete customer_train and customer_test - they are needed for subsequent modules!\n",
    "\n",
    "# Uncomment the lines below to remove demo tables:\n",
    "\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {catalog_name}.{schema_name}.customer_train\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {catalog_name}.{schema_name}.customer_test\")\n",
    "\n",
    "# print(\" All demo tables removed\")\n",
    "\n",
    "print(\"ℹ️ Cleanup disabled (uncomment code to remove demo tables)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Data_Splitting",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
