{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c2a7946d",
      "metadata": {},
      "source": [
        "# Module 5: Advanced Feature Engineering\n",
        "\n",
        "**Training Objective:** Master the art of creating new features from existing data to improve model performance.\n",
        "\n",
        "**Scope:**\n",
        "- Log Transformation: Handling skewed distributions\n",
        "- Interaction Features: Creating new signals (e.g., LTV Proxy)\n",
        "- VectorAssembler: Preparing final feature vector for Spark ML\n",
        "- Feature Selection: Correlation analysis for dimensionality reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91b916c7",
      "metadata": {},
      "source": [
        "## Context and Requirements\n",
        "\n",
        "- **Training day:** Day 1 - Data Preparation Fundamentals\n",
        "- **Notebook type:** Demo\n",
        "- **Technical requirements:**\n",
        "  - Databricks Runtime 14.x LTS or newer\n",
        "  - Unity Catalog enabled\n",
        "  - Permissions: CREATE TABLE, SELECT, MODIFY\n",
        "- **Dependencies:** `04_Feature_Transformation.ipynb` (creates `customer_train_transformed` table)\n",
        "- **Execution time:** ~20 minutes\n",
        "\n",
        "> **Note:** Feature engineering is often the difference between a mediocre model and a great one!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "200d70dd",
      "metadata": {},
      "source": [
        "## Theoretical Introduction\n",
        "\n",
        "**What is Feature Engineering?**\n",
        "\n",
        "Feature Engineering is the process of creating new features from raw data to improve model performance. It's often considered the most creative and impactful part of ML.\n",
        "\n",
        "**Common Techniques:**\n",
        "\n",
        "| Technique | When to Use | Example |\n",
        "|-----------|-------------|---------|\n",
        "| **Log Transform** | Skewed distributions | `log(salary)` |\n",
        "| **Polynomial Features** | Non-linear relationships | `age^2`, `age*income` |\n",
        "| **Interaction Features** | Combined effects | `salary * tenure` |\n",
        "| **Date Features** | Time-based patterns | `day_of_week`, `month` |\n",
        "| **Binning** | Continuous ‚Üí categorical | Age groups: 18-25, 26-35 |\n",
        "\n",
        "**Why VectorAssembler?**\n",
        "> Unlike Scikit-Learn which accepts a feature matrix $X$, Spark MLlib requires a **single column** of type `Vector`. `VectorAssembler` combines multiple columns into this vector.\n",
        "\n",
        "**Feature Selection Importance:**\n",
        "- Too many features ‚Üí Overfitting, slow training\n",
        "- Correlated features ‚Üí Multicollinearity (confuses linear models)\n",
        "- Irrelevant features ‚Üí Noise that hurts performance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56fbc485",
      "metadata": {},
      "source": [
        "## Per-User Isolation\n",
        "\n",
        "Run the initialization script for per-user catalog and schema isolation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77294d93",
      "metadata": {},
      "outputs": [],
      "source": [
        "%run ./00_Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "663d99bd",
      "metadata": {},
      "source": [
        "**Load Transformed Data:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fee1f7a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Transformed Data\n",
        "df = spark.table(\"customer_train_transformed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "113f42b1",
      "metadata": {},
      "source": [
        "## Section 1: Feature Extraction\n",
        "\n",
        "### Example 1.1: Log Transformation\n",
        "Many real-world variables (like Salary, House Prices, Population) follow a \"Power Law\" or \"Long Tail\" distribution.\n",
        "\n",
        "- **The Problem:** Linear models assume residuals are normally distributed. Highly skewed data violates this.\n",
        "- **The Solution:** Applying a Logarithm compresses the long tail, making the distribution more bell-shaped (Gaussian).\n",
        "- **Note:** We use `log1p` (log(x+1)) because `log(0)` is undefined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fb10810",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import log1p, col\n",
        "\n",
        "# log1p calculates log(x + 1) to handle zeros safely\n",
        "df_eng = df.withColumn(\"log_salary\", log1p(col(\"salary_imputed\")))\n",
        "\n",
        "display(df_eng.select(\"salary_imputed\", \"log_salary\").limit(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe2c3aee",
      "metadata": {},
      "source": [
        "### Example 1.2: Interaction Features\n",
        "Combining two features can reveal hidden patterns.\n",
        "*Example:* `LTV_Proxy = Salary * Tenure` (Lifetime Value approximation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82d4386c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import datediff, current_date\n",
        "\n",
        "# First, let's calculate Tenure (days since registration)\n",
        "df_eng = df_eng.withColumn(\"tenure_days\", datediff(current_date(), col(\"registration_date\")))\n",
        "\n",
        "# Now create the interaction: Salary * Tenure\n",
        "df_eng = df_eng.withColumn(\"ltv_proxy\", col(\"salary_imputed\") * col(\"tenure_days\"))\n",
        "\n",
        "display(df_eng.select(\"salary_imputed\", \"tenure_days\", \"ltv_proxy\").limit(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c33b1164",
      "metadata": {},
      "source": [
        "## Section 2: VectorAssembler (The Final Step)\n",
        "\n",
        "Unlike Scikit-Learn which accepts a matrix of features ($X$), Spark MLlib requires a **single column** of type `Vector` that contains all input features.\n",
        "\n",
        "`VectorAssembler` takes a list of columns (numerical, boolean, or vector) and combines them into this single feature vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67dad997",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# List of all numerical features we want to use\n",
        "input_cols = [\"age_imputed\", \"log_salary\", \"ltv_proxy\", \"reg_rank\"] \n",
        "# Note: We usually include encoded categorical vectors here too, e.g., \"country_vec\"\n",
        "\n",
        "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features_final\")\n",
        "df_final = assembler.transform(df_eng)\n",
        "\n",
        "display(df_final.select(\"features_final\").limit(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05185914",
      "metadata": {},
      "source": [
        "## Section 3: Feature Selection\n",
        "\n",
        "### Example 3.1: Correlation Analysis\n",
        "Which features are correlated with each other? (Multicollinearity)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ec59ca0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.stat import Correlation\n",
        "import pandas as pd\n",
        "\n",
        "# Calculate Correlation Matrix\n",
        "matrix = Correlation.corr(df_final, \"features_final\").head()\n",
        "corr_array = matrix[0].toArray()\n",
        "\n",
        "# Convert to Pandas DataFrame for better visualization\n",
        "corr_df = pd.DataFrame(corr_array, columns=input_cols, index=input_cols)\n",
        "\n",
        "# Display nicely\n",
        "print(f\"Features: {input_cols}\")\n",
        "display(corr_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a6cdf8b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save for Pipeline\n",
        "df_final.write.mode(\"overwrite\").saveAsTable(\"customer_train_engineered\")\n",
        "print(\"‚úÖ Saved 'customer_train_engineered'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77c389ac",
      "metadata": {},
      "source": [
        "## Best Practices\n",
        "\n",
        "### üéØ Feature Engineering Strategy Guide:\n",
        "\n",
        "| Technique | When to Use | Watch Out For |\n",
        "|-----------|-------------|---------------|\n",
        "| **Log Transform** | Right-skewed data (salary, prices) | Zero values (use log1p) |\n",
        "| **Polynomial** | Non-linear relationships | Overfitting, high dimensions |\n",
        "| **Interaction** | Combined effects matter | Exponential feature growth |\n",
        "| **Date Extraction** | Time patterns | Timezone issues |\n",
        "| **Binning** | Reduce noise, interpretability | Loss of information |\n",
        "\n",
        "### ‚ö†Ô∏è Common Mistakes to Avoid:\n",
        "\n",
        "1. **Creating too many features** ‚Üí Overfitting and slow training\n",
        "2. **Not checking correlations** ‚Üí Multicollinearity issues\n",
        "3. **Leaking target info** ‚Üí Features derived from target\n",
        "4. **Ignoring domain knowledge** ‚Üí Missing obvious patterns\n",
        "5. **Not validating on holdout** ‚Üí Overly optimistic results\n",
        "\n",
        "### üí° Pro Tips:\n",
        "\n",
        "- Always visualize new features vs target\n",
        "- Use domain knowledge to create meaningful interactions\n",
        "- Remove highly correlated features (>0.95 correlation)\n",
        "- Consider using automated feature selection (RFE, Lasso)\n",
        "- Log transform is often the most impactful single technique"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d41b5d24",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### What we achieved:\n",
        "\n",
        "- **Log Transformation**: Applied `log1p` to compress skewed salary distribution\n",
        "- **Interaction Features**: Created `LTV_Proxy` from salary √ó tenure\n",
        "- **VectorAssembler**: Combined all features into single vector column\n",
        "- **Correlation Analysis**: Identified multicollinearity between features\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "| # | Principle |\n",
        "|---|-----------|\n",
        "| 1 | **Log transform skewed data** - most impactful single technique |\n",
        "| 2 | **VectorAssembler is required** - Spark MLlib needs vector column |\n",
        "| 3 | **Check correlations** - avoid multicollinearity |\n",
        "| 4 | **Domain knowledge matters** - create meaningful features |\n",
        "| 5 | **Less can be more** - too many features cause overfitting |\n",
        "\n",
        "### Data Pipeline Status:\n",
        "\n",
        "| Table | Created | Used By |\n",
        "|-------|---------|---------|\n",
        "| `customer_train_transformed` | Module 4 | This module |\n",
        "| `customer_train_engineered` | ‚úÖ This module | Modules 6-7 |\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "üìö **Next Module:** Module 6 - ML Pipelines (putting it all together)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68200c07",
      "metadata": {},
      "source": [
        "## Cleanup\n",
        "\n",
        "Optionally remove demo tables created during exercises:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c3b0c9b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup - remove demo tables created in this notebook\n",
        "\n",
        "# Uncomment the lines below to remove demo tables:\n",
        "\n",
        "# spark.sql(f\"DROP TABLE IF EXISTS {catalog_name}.{schema_name}.customer_train_engineered\")\n",
        "\n",
        "# print(\"‚úÖ All demo tables removed\")\n",
        "\n",
        "print(\"‚ÑπÔ∏è Cleanup disabled (uncomment code to remove demo tables)\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
