{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50bac95f-7d70-4342-9e66-ccccb602172f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Module 5: Advanced Feature Engineering\n",
    "\n",
    "**Training Objective:** Master the art of creating new features from existing data to improve model performance.\n",
    "\n",
    "**Scope:**\n",
    "- Log Transformation: Handling skewed distributions\n",
    "- Interaction Features: Creating new signals (e.g., LTV Proxy)\n",
    "- VectorAssembler: Preparing final feature vector for Spark ML\n",
    "- Feature Selection: Correlation analysis for dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3035ac2a-51b9-42b1-9b18-fe5fbb19435d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Context and Requirements\n",
    "\n",
    "- **Training day:** Day 1 - Data Preparation Fundamentals\n",
    "- **Notebook type:** Demo\n",
    "- **Technical requirements:**\n",
    "  - Databricks Runtime 14.x LTS or newer\n",
    "  - Unity Catalog enabled\n",
    "  - Permissions: CREATE TABLE, SELECT, MODIFY\n",
    "- **Dependencies:** `04_Feature_Transformation.ipynb` (creates `customer_train_transformed` table)\n",
    "- **Execution time:** ~20 minutes\n",
    "\n",
    "> **Note:** Feature engineering is often the difference between a mediocre model and a great one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38577d0e-2823-4615-8165-2d87af5fb6b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Theoretical Introduction\n",
    "\n",
    "**What is Feature Engineering?**\n",
    "\n",
    "Feature Engineering is the process of creating new features from raw data to improve model performance. It's often considered the most creative and impactful part of ML.\n",
    "\n",
    "**Common Techniques:**\n",
    "\n",
    "| Technique | When to Use | Example |\n",
    "|-----------|-------------|---------|\n",
    "| **Log Transform** | Skewed distributions | `log(salary)` |\n",
    "| **Polynomial Features** | Non-linear relationships | `age^2`, `age*income` |\n",
    "| **Interaction Features** | Combined effects | `salary * tenure` |\n",
    "| **Date Features** | Time-based patterns | `day_of_week`, `month` |\n",
    "| **Binning** | Continuous ‚Üí categorical | Age groups: 18-25, 26-35 |\n",
    "\n",
    "**Why VectorAssembler?**\n",
    "> Unlike Scikit-Learn which accepts a feature matrix $X$, Spark MLlib requires a **single column** of type `Vector`. `VectorAssembler` combines multiple columns into this vector.\n",
    "\n",
    "**Feature Selection Importance:**\n",
    "- Too many features ‚Üí Overfitting, slow training\n",
    "- Correlated features ‚Üí Multicollinearity (confuses linear models)\n",
    "- Irrelevant features ‚Üí Noise that hurts performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d188883-e199-482d-a4d1-03a4311fbb1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Per-User Isolation\n",
    "\n",
    "Run the initialization script for per-user catalog and schema isolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "867e3358-3c6f-414a-8a05-d99863137857",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7b06445-c2fe-4799-bed5-7c468dcf198d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Load Transformed Data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85e6c1d9-1f75-4295-b840-3f4c3fa71a76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Transformed Data\n",
    "df = spark.table(\"customer_train_transformed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc5d3bbe-2d63-4b6c-bece-a3a89e3ec109",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 1: Feature Extraction\n",
    "\n",
    "### Example 1.1: Log Transformation\n",
    "Many real-world variables (like Salary, House Prices, Population) follow a \"Power Law\" or \"Long Tail\" distribution.\n",
    "\n",
    "- **The Problem:** Linear models assume residuals are normally distributed. Highly skewed data violates this.\n",
    "- **The Solution:** Applying a Logarithm compresses the long tail, making the distribution more bell-shaped (Gaussian).\n",
    "- **Note:** We use `log1p` (log(x+1)) because `log(0)` is undefined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "906461a0-db0e-4045-88c1-50f65a050883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import log1p, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e2508d8-a232-45fa-8b17-cc93e4963062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# log1p calculates log(x + 1) to handle zeros safely\n",
    "df_eng = df.withColumn(\"log_salary\", log1p(col(\"salary_imputed\")))\n",
    "\n",
    "display(df_eng.select(\"salary_imputed\", \"log_salary\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed84990c-92bf-493b-9f48-5647c6a9a00e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 1.2: Interaction Features\n",
    "Combining two features can reveal hidden patterns.\n",
    "*Example:* `LTV_Proxy = Salary * Tenure` (Lifetime Value approximation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfbe59b8-5223-4ea2-8a98-65162a526705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6114919a-cacf-4d59-aaf9-34c4be51ab7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# First, let's calculate Tenure (days since registration)\n",
    "df_eng = df_eng.withColumn(\"tenure_days\", datediff(current_date(), col(\"registration_date\")))\n",
    "\n",
    "# Now create the interaction: Salary * Tenure\n",
    "df_eng = df_eng.withColumn(\"ltv_proxy\", col(\"salary_imputed\") * col(\"tenure_days\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c5e76c8-9057-43cd-8a38-9610a17f0d6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_eng.select(\"salary_imputed\", \"tenure_days\", \"ltv_proxy\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "067da7ea-f96a-4ce8-adb6-83d5b285eb20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 2: VectorAssembler (The Final Step)\n",
    "\n",
    "Unlike Scikit-Learn which accepts a matrix of features ($X$), Spark MLlib requires a **single column** of type `Vector` that contains all input features.\n",
    "\n",
    "`VectorAssembler` takes a list of columns (numerical, boolean, or vector) and combines them into this single feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f3eef51-d40e-4c3f-92ea-ae087fab0134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# List of all numerical features we want to use\n",
    "input_cols = [\"age_imputed\", \"log_salary\", \"ltv_proxy\", \"reg_rank\"] \n",
    "# Note: We usually include encoded categorical vectors here too, e.g., \"country_vec\"\n",
    "\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features_final\")\n",
    "df_final = assembler.transform(df_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8aceb50-0056-4295-941c-c867d91aded7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_final.select(\"features_final\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2dc6b11-e321-4470-a554-08885c0e4df1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 3: Feature Selection\n",
    "\n",
    "### Example 3.1: Correlation Analysis\n",
    "Which features are correlated with each other? (Multicollinearity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "642b24ff-9859-4888-9fa2-9d18acd76bbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate Correlation Matrix\n",
    "matrix = Correlation.corr(df_final, \"features_final\").head()\n",
    "corr_array = matrix[0].toArray()\n",
    "\n",
    "# Convert to Pandas DataFrame for better visualization\n",
    "corr_df = pd.DataFrame(corr_array, columns=input_cols, index=input_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ddb5e49-db5f-46eb-a607-a94a88f920b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display nicely\n",
    "print(f\"Features: {input_cols}\")\n",
    "display(corr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "781244bf-6d74-4851-b501-4e70126dd456",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save for Pipeline\n",
    "df_final.write.mode(\"overwrite\").saveAsTable(\"customer_train_engineered\")\n",
    "print(\"‚úÖ Saved 'customer_train_engineered'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "587dc13f-035e-4f6d-b39a-9717ff9b59bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Best Practices\n",
    "\n",
    "### üéØ Feature Engineering Strategy Guide:\n",
    "\n",
    "| Technique | When to Use | Watch Out For |\n",
    "|-----------|-------------|---------------|\n",
    "| **Log Transform** | Right-skewed data (salary, prices) | Zero values (use log1p) |\n",
    "| **Polynomial** | Non-linear relationships | Overfitting, high dimensions |\n",
    "| **Interaction** | Combined effects matter | Exponential feature growth |\n",
    "| **Date Extraction** | Time patterns | Timezone issues |\n",
    "| **Binning** | Reduce noise, interpretability | Loss of information |\n",
    "\n",
    "### ‚ö†Ô∏è Common Mistakes to Avoid:\n",
    "\n",
    "1. **Creating too many features** ‚Üí Overfitting and slow training\n",
    "2. **Not checking correlations** ‚Üí Multicollinearity issues\n",
    "3. **Leaking target info** ‚Üí Features derived from target\n",
    "4. **Ignoring domain knowledge** ‚Üí Missing obvious patterns\n",
    "5. **Not validating on holdout** ‚Üí Overly optimistic results\n",
    "\n",
    "### üí° Pro Tips:\n",
    "\n",
    "- Always visualize new features vs target\n",
    "- Use domain knowledge to create meaningful interactions\n",
    "- Remove highly correlated features (>0.95 correlation)\n",
    "- Consider using automated feature selection (RFE, Lasso)\n",
    "- Log transform is often the most impactful single technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75510f66-8cdf-4a1f-8948-9b54876afbb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "### What we achieved:\n",
    "\n",
    "- **Log Transformation**: Applied `log1p` to compress skewed salary distribution\n",
    "- **Interaction Features**: Created `LTV_Proxy` from salary √ó tenure\n",
    "- **VectorAssembler**: Combined all features into single vector column\n",
    "- **Correlation Analysis**: Identified multicollinearity between features\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "| # | Principle |\n",
    "|---|-----------|\n",
    "| 1 | **Log transform skewed data** - most impactful single technique |\n",
    "| 2 | **VectorAssembler is required** - Spark MLlib needs vector column |\n",
    "| 3 | **Check correlations** - avoid multicollinearity |\n",
    "| 4 | **Domain knowledge matters** - create meaningful features |\n",
    "| 5 | **Less can be more** - too many features cause overfitting |\n",
    "\n",
    "### Data Pipeline Status:\n",
    "\n",
    "| Table | Created | Used By |\n",
    "|-------|---------|---------|\n",
    "| `customer_train_transformed` | Module 4 | This module |\n",
    "| `customer_train_engineered` | ‚úÖ This module | Modules 6-7 |\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "üìö **Next Module:** Module 6 - ML Pipelines (putting it all together)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bf472b4-0422-4015-a397-c27ef6d33b8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "Optionally remove demo tables created during exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a461e7b7-a4ae-4806-bebd-c0d3956eb4fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup - remove demo tables created in this notebook\n",
    "\n",
    "# Uncomment the lines below to remove demo tables:\n",
    "\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {catalog_name}.{schema_name}.customer_train_engineered\")\n",
    "\n",
    "# print(\"‚úÖ All demo tables removed\")\n",
    "\n",
    "print(\"‚ÑπÔ∏è Cleanup disabled (uncomment code to remove demo tables)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_Feature_Engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
