{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50bac95f-7d70-4342-9e66-ccccb602172f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Module 5: Advanced Feature Engineering\n",
    "\n",
    "## Business Context: TechCorp HR Analytics (continued)\n",
    "\n",
    "**Where We Are:**\n",
    "Data is encoded and scaled. Now we create NEW features from existing columns for **salary prediction**.\n",
    "\n",
    "**Features to Engineer for Salary Prediction:**\n",
    "| Feature | Logic | Business Meaning |\n",
    "|---------|-------|------------------|\n",
    "| `experience_years` | age - 22 | Years since graduation |\n",
    "| `tenure_days` | today - registration_date | Time with company |\n",
    "| `ltv_proxy` | age × tenure_days | Experience-loyalty interaction |\n",
    "\n",
    "> **Note:** We also demo `log_salary` transformation as a technique, but it CANNOT be used as a feature when predicting salary (data leakage!)\n",
    "\n",
    "---\n",
    "\n",
    "**Training Objective:** Master feature engineering techniques.\n",
    "\n",
    "**Scope:**\n",
    "- Log Transformation for skewed distributions (demo only)\n",
    "- Experience and Tenure calculations\n",
    "- Interaction Features\n",
    "- VectorAssembler for Spark ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3035ac2a-51b9-42b1-9b18-fe5fbb19435d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Context and Requirements\n",
    "\n",
    "- **Training day:** Day 1 - Data Preparation Fundamentals\n",
    "- **Notebook type:** Demo\n",
    "- **Technical requirements:**\n",
    "  - Databricks Runtime 14.x LTS or newer\n",
    "  - Unity Catalog enabled\n",
    "  - Permissions: CREATE TABLE, SELECT, MODIFY\n",
    "- **Dependencies:** `04_Feature_Transformation.ipynb` (creates `customer_train_transformed` table)\n",
    "- **Execution time:** ~20 minutes\n",
    "\n",
    "> **Note:** Feature engineering is often the difference between a mediocre model and a great one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38577d0e-2823-4615-8165-2d87af5fb6b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Theoretical Introduction\n",
    "\n",
    "**What is Feature Engineering?**\n",
    "\n",
    "Feature Engineering is the process of creating new features from raw data to improve model performance. It's often considered the most creative and impactful part of ML.\n",
    "\n",
    "**Common Techniques:**\n",
    "\n",
    "| Technique | When to Use | Example |\n",
    "|-----------|-------------|---------|\n",
    "| **Log Transform** | Skewed distributions | `log(salary)` |\n",
    "| **Polynomial Features** | Non-linear relationships | `age^2`, `age*income` |\n",
    "| **Interaction Features** | Combined effects | `salary * tenure` |\n",
    "| **Date Features** | Time-based patterns | `day_of_week`, `month` |\n",
    "| **Binning** | Continuous → categorical | Age groups: 18-25, 26-35 |\n",
    "\n",
    "**Why VectorAssembler?**\n",
    "> Unlike Scikit-Learn which accepts a feature matrix $X$, Spark MLlib requires a **single column** of type `Vector`. `VectorAssembler` combines multiple columns into this vector.\n",
    "\n",
    "**Feature Selection Importance:**\n",
    "- Too many features → Overfitting, slow training\n",
    "- Correlated features → Multicollinearity (confuses linear models)\n",
    "- Irrelevant features → Noise that hurts performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d188883-e199-482d-a4d1-03a4311fbb1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Per-User Isolation\n",
    "\n",
    "Run the initialization script for per-user catalog and schema isolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "867e3358-3c6f-414a-8a05-d99863137857",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7b06445-c2fe-4799-bed5-7c468dcf198d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Load Transformed Data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85e6c1d9-1f75-4295-b840-3f4c3fa71a76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Transformed Data\n",
    "df = spark.table(\"customer_train_transformed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc5d3bbe-2d63-4b6c-bece-a3a89e3ec109",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 1: Feature Extraction\n",
    "\n",
    "### Example 1.1: Log Transformation (Demo Only)\n",
    "\n",
    "Many real-world variables (like Salary, House Prices, Population) follow a \"Power Law\" or \"Long Tail\" distribution.\n",
    "\n",
    "- **The Problem:** Linear models assume residuals are normally distributed. Highly skewed data violates this.\n",
    "- **The Solution:** Applying a Logarithm compresses the long tail, making the distribution more bell-shaped (Gaussian).\n",
    "- **Note:** We use `log1p` (log(x+1)) because `log(0)` is undefined.\n",
    "\n",
    "> **WARNING - DATA LEAKAGE:** \n",
    "> This is a **demonstration of the technique only**. In our salary prediction task, we CANNOT use `log_salary` as a feature because it's derived from the target variable (`salary`). Using it would cause **data leakage**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "906461a0-db0e-4045-88c1-50f65a050883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import log1p, col, datediff, current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e2508d8-a232-45fa-8b17-cc93e4963062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DEMO: Log transformation technique\n",
    "# log1p calculates log(x + 1) to handle zeros safely\n",
    "#\n",
    "# WARNING: This is for DEMONSTRATION only!\n",
    "# We do NOT use log_salary as a feature in our model because:\n",
    "# - We are predicting salary\n",
    "# - log_salary = log(salary) is derived from target = DATA LEAKAGE!\n",
    "\n",
    "df_eng = df.withColumn(\"log_salary_demo\", log1p(col(\"salary_imputed\")))\n",
    "\n",
    "print(\"Log transformation demo (NOT used as feature!):\")\n",
    "display(df_eng.select(\"salary_imputed\", \"log_salary_demo\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed84990c-92bf-493b-9f48-5647c6a9a00e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 1.2: Experience, Tenure and Interaction Features\n",
    "\n",
    "Combining features can reveal hidden patterns useful for salary prediction.\n",
    "\n",
    "**Experience Years:** `age - 22` - approximates years of work experience (assuming graduation at 22).\n",
    "\n",
    "**Tenure Days:** How long the customer has been with us (days since registration).\n",
    "\n",
    "**LTV Proxy:** `Age * Tenure` - approximates lifetime value based on experience and loyalty.\n",
    "\n",
    "> **Note:** These features are safe to use because they don't contain the target variable (salary)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6114919a-cacf-4d59-aaf9-34c4be51ab7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Experience Years: estimated years since graduation (age - 22)\n",
    "# This is a key predictor for salary!\n",
    "df_eng = df_eng.withColumn(\"experience_years\", col(\"age_imputed\") - 22)\n",
    "\n",
    "# Calculate Tenure (days since registration)\n",
    "df_eng = df_eng.withColumn(\"tenure_days\", datediff(current_date(), col(\"registration_date\")))\n",
    "\n",
    "# Create the interaction: Age * Tenure (Lifetime Value approximation)\n",
    "# NOTE: We use age * tenure, NOT salary * tenure (to avoid data leakage!)\n",
    "df_eng = df_eng.withColumn(\"ltv_proxy\", col(\"age_imputed\") * col(\"tenure_days\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c5e76c8-9057-43cd-8a38-9610a17f0d6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_eng.select(\"age_imputed\", \"experience_years\", \"tenure_days\", \"ltv_proxy\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "067da7ea-f96a-4ce8-adb6-83d5b285eb20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 2: VectorAssembler (The Final Step)\n",
    "\n",
    "Unlike Scikit-Learn which accepts a matrix of features ($X$), Spark MLlib requires a **single column** of type `Vector` that contains all input features.\n",
    "\n",
    "`VectorAssembler` takes a list of columns (numerical, boolean, or vector) and combines them into this single feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f3eef51-d40e-4c3f-92ea-ae087fab0134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# List of all numerical features we want to use for SALARY PREDICTION\n",
    "# These are VALID features (no data leakage!)\n",
    "input_cols = [\"age_imputed\", \"experience_years\", \"tenure_days\", \"ltv_proxy\", \"reg_rank\"] \n",
    "# Note: We usually include encoded categorical vectors here too, e.g., \"country_vec\"\n",
    "# NOTE: log_salary_demo is NOT included (data leakage - derived from target!)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features_final\")\n",
    "df_final = assembler.transform(df_eng)\n",
    "\n",
    "print(f\"Features used for salary prediction: {input_cols}\")\n",
    "print(\"Note: log_salary_demo NOT used (would be data leakage!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8aceb50-0056-4295-941c-c867d91aded7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_final.select(\"features_final\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2dc6b11-e321-4470-a554-08885c0e4df1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 3: Feature Selection\n",
    "\n",
    "### Example 3.1: Correlation Analysis\n",
    "Which features are correlated with each other? (Multicollinearity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "642b24ff-9859-4888-9fa2-9d18acd76bbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate Correlation Matrix\n",
    "matrix = Correlation.corr(df_final, \"features_final\").head()\n",
    "corr_array = matrix[0].toArray()\n",
    "\n",
    "# Convert to Pandas DataFrame for better visualization\n",
    "corr_df = pd.DataFrame(corr_array, columns=input_cols, index=input_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94c1ec06-b723-490f-b248-09e043c822b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Why Correlation Analysis?\n",
    "\n",
    "Correlation analysis helps us identify pairs of features that move together (are highly correlated). In feature engineering, this is important because:\n",
    "\n",
    "- **Multicollinearity**: Highly correlated features can confuse many ML models (especially linear models), making it hard to interpret coefficients and sometimes degrading performance.\n",
    "- **Feature Selection**: By detecting and removing redundant features, we simplify the model, reduce overfitting, and speed up training.\n",
    "- **Better Insights**: Understanding feature relationships can reveal hidden patterns and guide further feature engineering.\n",
    "\n",
    "> **Goal:** Keep only the most informative, independent features for robust, interpretable models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13573716-231e-4111-af60-93c851d69731",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(corr_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ddb5e49-db5f-46eb-a607-a94a88f920b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display nicely\n",
    "print(f\"Features: {input_cols}\")\n",
    "display(corr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc7c2f54-5e2e-488d-bcb7-ca2641fe2447",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764809185913}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "781244bf-6d74-4851-b501-4e70126dd456",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save for Pipeline\n",
    "df_final.write.mode(\"overwrite\").saveAsTable(\"customer_train_engineered\")\n",
    "print(\" Saved 'customer_train_engineered'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "587dc13f-035e-4f6d-b39a-9717ff9b59bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Best Practices\n",
    "\n",
    "###  Feature Engineering Strategy Guide:\n",
    "\n",
    "| Technique | When to Use | Watch Out For |\n",
    "|-----------|-------------|---------------|\n",
    "| **Log Transform** | Right-skewed data (salary, prices) | Zero values (use log1p) |\n",
    "| **Polynomial** | Non-linear relationships | Overfitting, high dimensions |\n",
    "| **Interaction** | Combined effects matter | Exponential feature growth |\n",
    "| **Date Extraction** | Time patterns | Timezone issues |\n",
    "| **Binning** | Reduce noise, interpretability | Loss of information |\n",
    "\n",
    "### ️ Common Mistakes to Avoid:\n",
    "\n",
    "1. **Creating too many features** → Overfitting and slow training\n",
    "2. **Not checking correlations** → Multicollinearity issues\n",
    "3. **Leaking target info** → Features derived from target\n",
    "4. **Ignoring domain knowledge** → Missing obvious patterns\n",
    "5. **Not validating on holdout** → Overly optimistic results\n",
    "\n",
    "###  Pro Tips:\n",
    "\n",
    "- Always visualize new features vs target\n",
    "- Use domain knowledge to create meaningful interactions\n",
    "- Remove highly correlated features (>0.95 correlation)\n",
    "- Consider using automated feature selection (RFE, Lasso)\n",
    "- Log transform is often the most impactful single technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75510f66-8cdf-4a1f-8948-9b54876afbb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "### What we achieved:\n",
    "\n",
    "- **Log Transformation**: Applied `log1p` to compress skewed salary distribution\n",
    "- **Interaction Features**: Created `LTV_Proxy` from salary × tenure\n",
    "- **VectorAssembler**: Combined all features into single vector column\n",
    "- **Correlation Analysis**: Identified multicollinearity between features\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "| # | Principle |\n",
    "|---|-----------|\n",
    "| 1 | **Log transform skewed data** - most impactful single technique |\n",
    "| 2 | **VectorAssembler is required** - Spark MLlib needs vector column |\n",
    "| 3 | **Check correlations** - avoid multicollinearity |\n",
    "| 4 | **Domain knowledge matters** - create meaningful features |\n",
    "| 5 | **Less can be more** - too many features cause overfitting |\n",
    "\n",
    "### Data Pipeline Status:\n",
    "\n",
    "| Table | Created | Used By |\n",
    "|-------|---------|---------|\n",
    "| `customer_train_transformed` | Module 4 | This module |\n",
    "| `customer_train_engineered` |  This module | Modules 6-7 |\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    " **Next Module:** Module 6 - ML Pipelines (putting it all together)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bf472b4-0422-4015-a397-c27ef6d33b8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "Optionally remove demo tables created during exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a461e7b7-a4ae-4806-bebd-c0d3956eb4fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup - remove demo tables created in this notebook\n",
    "\n",
    "# Uncomment the lines below to remove demo tables:\n",
    "\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {catalog_name}.{schema_name}.customer_train_engineered\")\n",
    "\n",
    "# print(\" All demo tables removed\")\n",
    "\n",
    "print(\"ℹ️ Cleanup disabled (uncomment code to remove demo tables)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6557120918494448,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "05_Feature_Engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
