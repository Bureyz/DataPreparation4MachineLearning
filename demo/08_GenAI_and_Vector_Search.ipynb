{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb1f5e6",
   "metadata": {},
   "source": [
    "# Module 8 (BONUS): GenAI, Vector Search & AI Functions\n",
    "\n",
    "**Description:**\n",
    "This is a **BONUS** module that goes beyond traditional Data Preparation.\n",
    "Databricks is not just for traditional ML. It now integrates Generative AI deeply into the platform. This module explores the \"New\" AI capabilities: using LLMs directly in SQL and building Vector Search indexes for RAG (Retrieval Augmented Generation) applications.\n",
    "\n",
    "**Agenda:**\n",
    "1.  **AI Functions:** Using built-in SQL functions (`ai_analyze_sentiment`, `ai_query`) to process text with LLMs.\n",
    "2.  **Embeddings & Vector Search:** Understanding how text is converted to vectors and searching by semantic similarity.\n",
    "3.  **RAG Concept:** A brief look at how Vector Search powers Chatbots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7397868d",
   "metadata": {},
   "source": [
    "## Context and Requirements\n",
    "\n",
    "| Attribute | Value |\n",
    "|-----------|-------|\n",
    "| **Training Day** | Day 2 |\n",
    "| **Module Type** | BONUS - Demo Notebook |\n",
    "| **Technical Requirements** | Databricks Runtime ML 14.0+ with Vector Search enabled |\n",
    "| **Dependencies** | Unity Catalog, Model Serving endpoints (optional) |\n",
    "| **Estimated Time** | 30-45 minutes |\n",
    "\n",
    "**Note:** This is a BONUS module introducing GenAI concepts. Some features may require additional Databricks workspace configuration (Model Serving, Vector Search endpoints). The notebook is designed to demonstrate concepts even without full infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2135c69d",
   "metadata": {},
   "source": [
    "## Theoretical Introduction\n",
    "\n",
    "### GenAI Ecosystem in Databricks\n",
    "\n",
    "| Component | Description | Use Case |\n",
    "|-----------|-------------|----------|\n",
    "| **AI Functions** | SQL functions powered by LLMs | Text analysis, sentiment, extraction |\n",
    "| **Embeddings** | Vector representations of text | Semantic similarity, search |\n",
    "| **Vector Search** | Index and query vector embeddings | RAG, recommendation systems |\n",
    "| **Model Serving** | Deploy and serve ML/LLM models | Real-time inference |\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**Embeddings:**\n",
    "- Dense vector representations of text (typically 768-4096 dimensions)\n",
    "- Similar meanings → similar vectors (cosine similarity)\n",
    "- Foundation for semantic search and RAG\n",
    "\n",
    "**Vector Search:**\n",
    "- Efficient nearest neighbor search in high-dimensional space\n",
    "- Enables \"find similar documents\" at scale\n",
    "- Databricks offers managed Vector Search with Delta Sync\n",
    "\n",
    "**RAG (Retrieval-Augmented Generation):**\n",
    "- Combine LLM reasoning with private knowledge\n",
    "- Steps: Query → Retrieve relevant docs → Generate answer with context\n",
    "- Reduces hallucinations and enables company-specific AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c821fee",
   "metadata": {},
   "source": [
    "## Per-User Isolation\n",
    "\n",
    "Run the setup notebook to configure your isolated environment with unique catalog and schema names. This ensures your GenAI experiments don't interfere with other participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b66e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Environment Setup\n",
    "%run ./00_Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c0ed12",
   "metadata": {},
   "source": [
    "## Section 1: AI Functions in SQL\n",
    "\n",
    "Databricks now allows you to call Large Language Models (LLMs) directly from SQL. This democratizes AI, allowing Data Analysts to perform complex text tasks without Python.\n",
    "\n",
    "**Available Functions:**\n",
    "- `ai_analyze_sentiment(text)`: Returns 'positive', 'negative', 'neutral'.\n",
    "- `ai_classify(text, categories)`: Classifies text into provided labels.\n",
    "- `ai_summarize(text)`: Summarizes long text.\n",
    "- `ai_translate(text, lang)`: Translates text.\n",
    "- `ai_query(model, prompt)`: Sends a custom prompt to a served model (e.g., Llama 3, DBRX)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a468df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create some sample data to play with\n",
    "# Imagine these are customer reviews or support tickets\n",
    "reviews_data = [\n",
    "    (1, \"I absolutely love this product! Fast shipping and great quality.\"),\n",
    "    (2, \"The item arrived damaged and the support team was rude.\"),\n",
    "    (3, \"It's okay, does the job but nothing special.\"),\n",
    "    (4, \"Can you help me reset my password?\"),\n",
    "    (5, \"Where is my refund for order #12345?\")\n",
    "]\n",
    "df_reviews = spark.createDataFrame(reviews_data, [\"id\", \"text\"])\n",
    "df_reviews.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.customer_reviews\")\n",
    "\n",
    "display(df_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f81035",
   "metadata": {},
   "source": [
    "### Example 1.1: Sentiment Analysis & Classification\n",
    "We can use `ai_analyze_sentiment` to quickly gauge customer satisfaction and `ai_classify` to route tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c67d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: These functions run in SQL. We use spark.sql() to execute them.\n",
    "# They use small, efficient models built-in to the Databricks engine (Serverless).\n",
    "\n",
    "df_ai = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    id,\n",
    "    text,\n",
    "    ai_analyze_sentiment(text) as sentiment,\n",
    "    ai_classify(text, ARRAY('complaint', 'praise', 'support_request', 'billing')) as category\n",
    "FROM {catalog_name}.{schema_name}.customer_reviews\n",
    "\"\"\")\n",
    "\n",
    "display(df_ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4574acd3",
   "metadata": {},
   "source": [
    "### Example 1.2: Generative AI with `ai_query`\n",
    "For more complex tasks, we can ask an LLM (like Llama 3 or DBRX) to generate text.\n",
    "*Note: This requires a Model Serving Endpoint to be active.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870de48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Generating a polite response to the customer\n",
    "# We wrap this in a try-catch block or comment it out because it requires a paid Model Serving endpoint.\n",
    "\n",
    "\"\"\"\n",
    "query = f'''\n",
    "SELECT \n",
    "    id,\n",
    "    text,\n",
    "    ai_query(\n",
    "        'databricks-meta-llama-3-70b-instruct',\n",
    "        CONCAT('Write a short, polite response to this customer review: ', text)\n",
    "    ) as suggested_response\n",
    "FROM {catalog_name}.{schema_name}.customer_reviews\n",
    "'''\n",
    "# display(spark.sql(query))\n",
    "\"\"\"\n",
    "print(\"To run ai_query, uncomment the code above and ensure you have a Model Serving endpoint (e.g., Llama 3) active.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2527c998",
   "metadata": {},
   "source": [
    "## Section 2: Embeddings & Vector Search\n",
    "\n",
    "**What is an Embedding?**\n",
    "Computers don't understand text; they understand numbers. An **Embedding** is a way to convert text into a long list of numbers (a vector), e.g., `[0.12, -0.5, 0.88, ...]`.\n",
    "- **Magic Property:** Texts with similar *meanings* will have vectors that are mathematically *close* to each other.\n",
    "- \"King\" and \"Queen\" will be close. \"Apple\" and \"Car\" will be far apart.\n",
    "\n",
    "**What is Vector Search?**\n",
    "It's a specialized database that indexes these vectors to perform \"Nearest Neighbor\" search extremely fast. This is the foundation of **RAG (Retrieval Augmented Generation)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5056ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare Source Data (Knowledge Base)\n",
    "# Let's say we have a manual for our products.\n",
    "docs_data = [\n",
    "    (1, \"Electric Forklift 2000: Ideal for indoor warehouse use. Zero emissions. 2-ton capacity.\"),\n",
    "    (2, \"Diesel Titan X: Heavy-duty outdoor forklift. 5-ton capacity. Best for rough terrain.\"),\n",
    "    (3, \"Hand Pallet Jack: Manual tool for moving light pallets up to 500kg.\"),\n",
    "    (4, \"Warehouse Automation Suite: Software for inventory management and robot control.\")\n",
    "]\n",
    "\n",
    "df_docs = spark.createDataFrame(docs_data, [\"id\", \"content\"])\n",
    "\n",
    "# We must enable Change Data Feed (CDF) for Vector Search to sync automatically!\n",
    "df_docs.write.format(\"delta\") \\\n",
    "    .option(\"delta.enableChangeDataFeed\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(f\"{catalog_name}.{schema_name}.product_docs\")\n",
    "\n",
    "print(\"✅ Knowledge Base created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074c05ec",
   "metadata": {},
   "source": [
    "### Example 2.1: Creating a Vector Index\n",
    "\n",
    "To make this searchable, we would create a **Vector Search Index**.\n",
    "Databricks manages the embedding process for us (Source Table -> Embedding Model -> Vector Index).\n",
    "\n",
    "*Note: The code below requires a Vector Search Endpoint to be provisioned in the Compute tab.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "# Configuration\n",
    "vs_endpoint_name = \"my_vector_search_endpoint\" # Must exist in UI\n",
    "index_name = f\"{catalog_name}.{schema_name}.product_docs_index\"\n",
    "source_table = f\"{catalog_name}.{schema_name}.product_docs\"\n",
    "\n",
    "# Code to create index (Commented out to prevent errors if no endpoint exists)\n",
    "\"\"\"\n",
    "vsc = VectorSearchClient()\n",
    "\n",
    "vsc.create_delta_sync_index(\n",
    "    endpoint_name=vs_endpoint_name,\n",
    "    source_table_name=source_table,\n",
    "    index_name=index_name,\n",
    "    pipeline_type=\"TRIGGERED\",\n",
    "    primary_key=\"id\",\n",
    "    embedding_source_column=\"content\",\n",
    "    embedding_model_endpoint_name=\"databricks-gte-large-en\" # Built-in embedding model\n",
    ")\n",
    "print(\"Index creation triggered...\")\n",
    "\"\"\"\n",
    "print(\"Vector Search Index code provided above. Requires active Endpoint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3951cb92",
   "metadata": {},
   "source": [
    "### Example 2.2: Performing a Similarity Search\n",
    "\n",
    "Once indexed, we can ask questions like \"I need a truck for outside\". The system converts this query to a vector and finds the closest match (The Diesel Forklift), even though the word \"truck\" or \"outside\" might not match exactly (it matches \"outdoor\" semantically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6d35dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation of Vector Search Result\n",
    "# Query: \"I need something for heavy outdoor work\"\n",
    "\n",
    "results = [\n",
    "    (2, \"Diesel Titan X: Heavy-duty outdoor forklift. 5-ton capacity...\", 0.89),\n",
    "    (1, \"Electric Forklift 2000: Ideal for indoor warehouse use...\", 0.45),\n",
    "    (3, \"Hand Pallet Jack: Manual tool...\", 0.20)\n",
    "]\n",
    "\n",
    "df_results = spark.createDataFrame(results, [\"id\", \"content\", \"similarity_score\"])\n",
    "print(\"Query: 'I need something for heavy outdoor work'\")\n",
    "print(\"Top Results:\")\n",
    "display(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fc3b26",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "| Practice | Description |\n",
    "|----------|-------------|\n",
    "| **Endpoint Management** | Turn off Model Serving endpoints when not in use to control costs |\n",
    "| **Chunking Strategy** | For long documents, split into meaningful chunks (paragraphs, sections) before embedding |\n",
    "| **Embedding Model Selection** | Choose embedding model based on language and domain (e.g., `databricks-gte-large-en` for English) |\n",
    "| **Enable CDF** | Always enable Change Data Feed for tables used with Vector Search |\n",
    "| **RAG Temperature** | Use lower temperature (0.0-0.3) for factual Q&A, higher (0.7-1.0) for creative tasks |\n",
    "| **Prompt Engineering** | Include context and format instructions in prompts for better ai_query results |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25225eb",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Accomplished\n",
    "| Task | Status |\n",
    "|------|--------|\n",
    "| Explored AI Functions in SQL | ✅ |\n",
    "| Used `ai_analyze_sentiment` and `ai_classify` | ✅ |\n",
    "| Understood Embeddings concept | ✅ |\n",
    "| Learned Vector Search architecture | ✅ |\n",
    "| Created knowledge base for RAG | ✅ |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **AI Functions** democratize AI by allowing SQL-based LLM calls\n",
    "2. **Embeddings** convert text to semantic vectors for similarity search\n",
    "3. **Vector Search** enables meaning-based retrieval at scale\n",
    "4. **RAG** combines retrieval with generation for accurate, grounded AI responses\n",
    "5. Databricks provides managed infrastructure for the entire GenAI stack\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "After this BONUS module, you have a complete foundation in Data Preparation for ML:\n",
    "- Return to core modules if needed for review\n",
    "- Explore Databricks documentation for advanced GenAI features\n",
    "- Consider building a RAG chatbot with your own data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7716df8a",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n",
    "\n",
    "Remove temporary tables and indexes created during this demo. Vector Search indexes should be deleted via the Databricks UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6aa2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: Remove tables created in this notebook\n",
    "# Uncomment and run to clean up resources\n",
    "\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {catalog_name}.{schema_name}.customer_reviews\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {catalog_name}.{schema_name}.product_docs\")\n",
    "\n",
    "# Note: Vector Search indexes must be deleted via the Databricks UI or API:\n",
    "# vsc = VectorSearchClient()\n",
    "# vsc.delete_index(endpoint_name=vs_endpoint_name, index_name=index_name)\n",
    "\n",
    "print(\"To clean up, uncomment the DROP TABLE statements above.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
