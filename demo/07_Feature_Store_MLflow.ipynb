{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c625d9c4-32c7-4a07-87ea-acda215d2913",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Module 7: Feature Store and MLOps\n",
    "\n",
    "## Business Context: TechCorp HR Analytics (finale)\n",
    "\n",
    "**Where We Are:**\n",
    "We've built a complete salary prediction pipeline: EDA -> Splitting -> Imputation -> Transformation -> Engineering -> ML Model. Now HR wants to **productionize** this solution.\n",
    "\n",
    "**Why Feature Store Matters for HR:**\n",
    "1. **Consistency:** When predicting salary for a new hire, we need the EXACT same feature transformations\n",
    "2. **Time-travel:** If we retrain the model later, we need features as they were at prediction time\n",
    "3. **Governance:** Auditors need to trace back how each salary prediction was made\n",
    "4. **Reusability:** Other teams (Recruiting, Finance) can reuse our employee features\n",
    "\n",
    "**The Production Flow:**\n",
    "```\n",
    "New Hire Application -> Feature Store (lookup) -> Model (predict) -> Recommended Salary\n",
    "                              |\n",
    "               Same features used in training!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Training Objective:** Master Databricks Feature Store and Model Registry for production-grade ML workflows.\n",
    "\n",
    "**Scope:**\n",
    "- Feature Store Creation: Creating and populating Feature Tables\n",
    "- Feature Lookup: Creating training datasets with point-in-time correctness\n",
    "- Training & Logging: Training models with feature lineage\n",
    "- Model Registry: Promoting models to Production with Unity Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77cc7103-a857-482c-b52d-fc461f6a4043",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Context and Requirements\n",
    "\n",
    "- **Training day:** Day 1 - Data Preparation Fundamentals (Advanced)\n",
    "- **Notebook type:** Demo\n",
    "- **Technical requirements:**\n",
    "  - Databricks Runtime 14.x LTS or newer\n",
    "  - Unity Catalog enabled\n",
    "  - Feature Store enabled (default in UC)\n",
    "  - Permissions: CREATE TABLE, SELECT, MODIFY, CREATE MODEL\n",
    "- **Dependencies:** `05_Feature_Engineering.ipynb` (creates `customer_train_engineered` table)\n",
    "- **Execution time:** ~25 minutes\n",
    "\n",
    "> **Note:** Feature Store is essential for production ML - it ensures consistency between training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c242b29-6bc7-493b-adc9-c10fd128991a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Theoretical Introduction\n",
    "\n",
    "**What is Feature Store?**\n",
    "\n",
    "A centralized repository for features that enables:\n",
    "- **Write Once, Use Everywhere**: Define feature logic once, reuse for training and inference\n",
    "- **Training-Serving Consistency**: Same features in development and production\n",
    "- **Point-in-Time Correctness**: Prevent data leakage with temporal joins\n",
    "- **Feature Discovery**: Teams can find and reuse existing features\n",
    "\n",
    "**Feature Store Workflow:**\n",
    "\n",
    "```\n",
    "Raw Data → Feature Engineering → Feature Table → Training Set → Model\n",
    "                                       ↓\n",
    "                              Inference (Batch/Online)\n",
    "```\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Feature Table** | Delta table with primary key and optional timestamp |\n",
    "| **Feature Lookup** | Join features to training labels by key |\n",
    "| **Point-in-Time Join** | Only use features available at observation time |\n",
    "| **fs.log_model** | Model remembers which features it needs |\n",
    "| **score_batch** | Auto-fetches features by ID for inference |\n",
    "\n",
    "**Model Registry with Unity Catalog:**\n",
    "- Models registered in `catalog.schema.model_name`\n",
    "- Use aliases like \"Champion\" and \"Challenger\" for lifecycle management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "816d2858-bcac-4b9a-b805-5df16e9bd8fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Per-User Isolation\n",
    "\n",
    "Run the initialization script for per-user catalog and schema isolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13c3f311-c243-4440-8a8a-ad9b63e9f4dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36a9c66b-b3e3-4a34-b70b-2d56ee7d2d62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Initialize Feature Store Client:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "989b7b6f-91ea-4099-af0e-36da370b9080",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_store import FeatureStoreClient\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "fs = FeatureStoreClient()\n",
    "df = spark.table(\"customer_train_engineered\")\n",
    "\n",
    "# Add timestamp for Feature Store\n",
    "df_fs = df.withColumn(\"event_timestamp\", current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cca24ef-c20b-4dea-b448-8f01c61d4cff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 1: Creating a Feature Table\n",
    "\n",
    "**The Problem:**\n",
    "In traditional ML, data scientists often rewrite feature engineering code for training, and engineers rewrite it again for production. This leads to **Training-Serving Skew** (inconsistencies).\n",
    "\n",
    "**The Solution:**\n",
    "The **Feature Store** acts as a centralized repository.\n",
    "1.  **Write Once:** Define feature logic once.\n",
    "2.  **Use Everywhere:** Fetch features for training (offline) or inference (online) ensuring consistency.\n",
    "3.  **Discovery:** Other teams can find and reuse your features (e.g., \"Customer LTV\").\n",
    "\n",
    "We register our engineered features so others can use them without re-running the engineering code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dadb075c-b2a1-4249-b706-a2b727903637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Info for user: You probably get an error because VectorUTC type is not supported in FeatureStore.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97b4f666-8139-41a8-a5b2-e280cf181fbc",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764808220157}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_fs.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e6c940d-9257-4bd3-8e0e-2ac337222a32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_fs.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5251367d-35ed-4fb8-841f-ce61848632fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# Zakładamy, że Twoja ramka danych to 'df'\n",
    "# Konwersja wszystkich kolumn VectorUDT na tablice\n",
    "cols_to_convert = [\n",
    "    \"country_vec\", \"features_num\", \"features_std\", \n",
    "    \"features_minmax\", \"features_robust\", \"features_final\"\n",
    "]\n",
    "\n",
    "for col_name in cols_to_convert:\n",
    "    # \"float32\" oszczędza miejsce, \"float64\" to domyślna precyzja\n",
    "    df_fs = df_fs.withColumn(col_name, vector_to_array(col_name, dtype=\"float64\"))\n",
    "\n",
    "# Teraz df ma typ ArrayType zamiast VectorUDT i Feature Store to przyjm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67b84854-cd4c-4c2d-9483-4647aba7acae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_fs.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2974971a-7076-47e6-9914-081a8e076dac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run that query again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d23218b-e328-4cb4-a786-27f69a0da8ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c54b0096-6de2-46dc-820f-e022f9109fca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%skip\n",
    "fs.drop_table(name=f\"{catalog_name}.{schema_name}.customer_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9efdf7a-92be-4cda-a73f-702ededda9db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = f\"{catalog_name}.{schema_name}.customer_features\"\n",
    "\n",
    "# Create Feature Table\n",
    "# We use mode=\"overwrite\" to allow re-running this cell\n",
    "fs.create_table(\n",
    "    name=table_name,\n",
    "    primary_keys=[\"id\"], \n",
    "    # timestamp_keys=[\"event_timestamp\"],\n",
    "    df=df_fs,\n",
    "    description=\"Customer features for salary prediction: Age, Experience Years, Tenure Days, LTV Proxy\"\n",
    ")\n",
    "\n",
    "print(f\"Feature Table {table_name} created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad9d2fe9-8331-4e12-b8d4-1d60518187ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 2: Reading & Training Set (Lookup)\n",
    "\n",
    "One of the main benefits of Feature Store is **Point-in-Time Lookup**.\n",
    "When creating a training set, we need to join features to our labels. Feature Store ensures that for each label (observation), we only use feature values that were available *at that time*, preventing data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3168d28-21dd-4f04-8916-d03ec55be04c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Simple Read\n",
    "df_features = fs.read_table(name=table_name)\n",
    "display(df_features.limit(5))\n",
    "\n",
    "# 2. Create Training Set with Feature Lookup\n",
    "df_spine = spark.table(\"customer_train\").select(\"id\", \"salary\")\n",
    "\n",
    "from databricks.feature_store import FeatureLookup\n",
    "\n",
    "# Features for SALARY PREDICTION (no log_salary - that would be data leakage!)\n",
    "feature_lookups = [\n",
    "    FeatureLookup(\n",
    "        table_name=table_name,\n",
    "        feature_names=[\"salary_imputed\",\"age_imputed\", \"experience_years\", \"tenure_days\", \"ltv_proxy\"],\n",
    "        lookup_key=\"id\"\n",
    "    )\n",
    "]\n",
    "\n",
    "training_set = fs.create_training_set(\n",
    "    df=df_spine,\n",
    "    feature_lookups=feature_lookups,\n",
    "    label=\"salary\",\n",
    "    exclude_columns=[\"id\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "df_training = training_set.load_df()\n",
    "display(df_training.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a910884e-444a-41e3-a875-6101a16b4143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 3: Training & Logging with Feature Store\n",
    "\n",
    "We will now train a model and log it using `fs.log_model`.\n",
    "**Why?**\n",
    "When we log with Feature Store, the model remembers exactly which features it needs. At inference time, we just provide the `id`, and the model automatically fetches the features from the store!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c9737de-551d-4eb8-bd8c-3ee6ed94eaaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Prepare data for Scikit-Learn\n",
    "pdf = df_training.toPandas()\n",
    "X = pdf.drop(\"salary_imputed\", axis=1)\n",
    "y = pdf[\"salary_imputed\"]\n",
    "\n",
    "# Train a simple model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Log Model\n",
    "# We use fs.log_model instead of mlflow.sklearn.log_model\n",
    "model_name = f\"{catalog_name}.{schema_name}.customer_salary_model\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"FS_Model_Demo\") as run:\n",
    "    fs.log_model(\n",
    "        model,\n",
    "        artifact_path=\"model\",\n",
    "        flavor=mlflow.sklearn,\n",
    "        training_set=training_set,\n",
    "        registered_model_name=model_name # This registers the model automatically!\n",
    "    )\n",
    "    print(f\"Model logged and registered as: {model_name}\")\n",
    "    print(f\"Run ID: {run.info.run_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "692a029f-00f3-47d2-ab0a-b6518bbd4d13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 4: Model Registry (Transition to Production)\n",
    "\n",
    "The model is now registered in Unity Catalog. We can manage its lifecycle using aliases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "573b3462-3fae-4fb2-976d-b21787c52dc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# Get the latest version of the model we just registered\n",
    "latest_version = client.get_registered_model(model_name).latest_versions[0].version\n",
    "\n",
    "print(f\"Latest Version: {latest_version}\")\n",
    "\n",
    "# Transition to Production (Alias in Unity Catalog)\n",
    "# In UC, we use Aliases like 'Champion' or 'Challenger' instead of Stages\n",
    "client.set_registered_model_alias(model_name, \"Champion\", latest_version)\n",
    "\n",
    "print(f\"Model version {latest_version} set as 'Champion'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3f86e6a-82f9-4492-a583-4ddb7aa2e41c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 5: Batch Inference (Scoring)\n",
    "\n",
    "The magic of Feature Store is that we don't need to assemble features manually for inference.\n",
    "We just provide the **IDs**, and `fs.score_batch` automatically looks up the correct features from the store and applies the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1da2f834-d8b6-4117-a1e0-fb4fd17908c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simulate new data (Just IDs!)\n",
    "df_new = spark.createDataFrame([(1,), (2,), (5,)], [\"id\"])\n",
    "\n",
    "print(\"Scoring new data (IDs only)...\")\n",
    "\n",
    "# Score Batch\n",
    "# Note: We use the model URI from Unity Catalog\n",
    "predictions = fs.score_batch(\n",
    "    model_uri=f\"models:/{model_name}/Champion\",\n",
    "    df=df_new\n",
    ")\n",
    "\n",
    "display(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00d899be-0b51-4ff4-9a9e-0078ee29ee21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Best Practices\n",
    "\n",
    "###  Feature Store Strategy Guide:\n",
    "\n",
    "| Aspect | Best Practice | Why |\n",
    "|--------|--------------|-----|\n",
    "| **Primary Keys** | Use stable IDs (customer_id, not row number) | Consistent lookups |\n",
    "| **Timestamps** | Include `event_timestamp` | Point-in-time correctness |\n",
    "| **Feature Names** | Descriptive (e.g., `customer_ltv_30d`) | Discoverability |\n",
    "| **Granularity** | One table per entity type | Cleaner schema |\n",
    "| **Updates** | Use `mode=\"merge\"` for incremental | Efficiency |\n",
    "\n",
    "### ️ Common Mistakes to Avoid:\n",
    "\n",
    "1. **No timestamp column** → Cannot do point-in-time joins\n",
    "2. **Duplicating features** → Use existing tables, don't recreate\n",
    "3. **Logging without training_set** → Model doesn't know its features\n",
    "4. **Using wrong model URI** → Include alias (Champion) for production\n",
    "5. **Not testing inference** → Always verify score_batch works\n",
    "\n",
    "###  Pro Tips:\n",
    "\n",
    "- Use Unity Catalog for governance and lineage\n",
    "- Set up online store for real-time inference (<10ms)\n",
    "- Use aliases (Champion/Challenger) for A/B testing\n",
    "- Monitor feature freshness in production\n",
    "- Document features in the description field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c472131b-457f-4549-bcc8-606ead930773",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "### What we achieved:\n",
    "\n",
    "- **Feature Table**: Created centralized feature repository\n",
    "- **Feature Lookup**: Built training set with automatic joins\n",
    "- **fs.log_model**: Logged model with feature lineage\n",
    "- **Model Registry**: Registered and promoted model to Champion\n",
    "- **score_batch**: Demonstrated automatic feature fetching\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "| # | Principle |\n",
    "|---|-----------|\n",
    "| 1 | **Feature Store ensures consistency** - same features everywhere |\n",
    "| 2 | **Point-in-time joins prevent leakage** - use timestamps |\n",
    "| 3 | **fs.log_model links model to features** - automatic inference |\n",
    "| 4 | **Unity Catalog provides governance** - lineage and access control |\n",
    "| 5 | **score_batch simplifies inference** - just provide IDs |\n",
    "\n",
    "### Artifacts Created:\n",
    "\n",
    "| Artifact | Location | Purpose |\n",
    "|----------|----------|---------|\n",
    "| Feature Table | `{catalog}.{schema}.customer_features` | Centralized features |\n",
    "| Registered Model | `{catalog}.{schema}.customer_salary_model` | Production model |\n",
    "| Champion Alias | Applied to latest version | Production deployment |\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    " **Next Module:** Module 8 (BONUS) - GenAI, Vector Search & AI Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3ceeba3-5f80-4cfa-aebb-76ea1f9a7fdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "Optionally remove demo artifacts created during exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a57f94d8-28b5-4741-a896-f48d9d92ef80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup - remove demo artifacts created in this notebook\n",
    "\n",
    "# Uncomment the lines below to remove demo artifacts:\n",
    "\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")  # Feature table\n",
    "# client.delete_registered_model(model_name)  # Registered model\n",
    "\n",
    "# print(\" All demo artifacts removed\")\n",
    "\n",
    "print(\"ℹ️ Cleanup disabled (uncomment code to remove demo artifacts)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6557120918494279,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "07_Feature_Store_MLflow",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
