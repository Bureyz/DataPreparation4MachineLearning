{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8c00e12-55eb-4ba7-820a-16d9a6203deb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Module 3: Handling Missing Data (Imputation)\n",
    "\n",
    "## Business Context: TechCorp HR Analytics (continued)\n",
    "\n",
    "**Where We Are:**\n",
    "We have multiple data quality issues to fix:\n",
    "\n",
    "| Column | Issue | % Missing | Strategy |\n",
    "|--------|-------|-----------|----------|\n",
    "| `age` | Missing values | ~5% | Imputer (median) |\n",
    "| `salary` | Missing values | ~3% | Imputer (median) |\n",
    "| `source` | Missing values | ~4% | fillna(UNKNOWN) |\n",
    "\n",
    "---\n",
    "\n",
    "**Scope:**\n",
    "- dropna: When to drop missing data\n",
    "- fillna: Categorical defaults (source)\n",
    "- Imputer: Numerical imputation (age, salary)\n",
    "- Missing flags: Capture missingness as feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e36e2d49-c77a-4d0b-bbc2-1122290d5bab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Context and Requirements\n",
    "\n",
    "- **Training day:** Day 1 - Data Preparation Fundamentals\n",
    "- **Notebook type:** Demo\n",
    "- **Technical requirements:**\n",
    "  - Databricks Runtime 14.x LTS or newer\n",
    "  - Unity Catalog enabled\n",
    "  - Permissions: CREATE TABLE, SELECT, MODIFY\n",
    "- **Dependencies:** `02_Data_Splitting.ipynb` (creates `customer_train` table)\n",
    "- **Execution time:** ~20 minutes\n",
    "\n",
    "> **Critical:** Always fit imputers on TRAINING data only to prevent data leakage!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6f45cf9-f1fc-40cc-bea6-d9fd36d30b69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Theoretical Introduction\n",
    "\n",
    "**Why handle missing data?**\n",
    "\n",
    "Most ML algorithms cannot handle `NULL` values directly. We must decide how to handle them before training.\n",
    "\n",
    "**Imputation Strategies Comparison:**\n",
    "\n",
    "| Strategy | When to Use | Pros | Cons |\n",
    "|----------|-------------|------|------|\n",
    "| **Drop (dropna)** | <5% missing, random | Simple | Loses data, may introduce bias |\n",
    "| **Constant (fillna)** | Categorical data | Simple, interpretable | May not reflect reality |\n",
    "| **Mean** | Normally distributed | Uses all data | Sensitive to outliers |\n",
    "| **Median** | Skewed data, outliers | Robust | Ignores distribution shape |\n",
    "| **Mode** | Categorical | Preserves distribution | May overfit majority class |\n",
    "\n",
    "**Data Leakage Warning:**\n",
    "> \u26a0\ufe0f **Critical Rule:** Always calculate imputation statistics (mean, median) on TRAINING data only! Applying test data statistics would leak future information into the model.\n",
    "\n",
    "**Informative Missingness:**\n",
    "Sometimes, the fact that data is missing is a signal itself:\n",
    "- A sensor returning `NULL` might indicate equipment failure\n",
    "- A customer not providing phone number might indicate privacy concerns\n",
    "- Creating a \"missing flag\" allows the model to learn these patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "120573ab-d644-4fa4-a2f5-b4110b212b2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Per-User Isolation\n",
    "\n",
    "Run the initialization script for per-user catalog and schema isolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b205f094-d88b-4f0e-ba14-ba25f66087e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e63ebba8-6669-42cd-b935-4270ea346bc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Load Training Data:**\n",
    "\n",
    "Remember: We fit imputers on TRAIN data only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b06d853-61de-4f63-a5bc-01d703cb3dfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Training Data\n",
    "df = spark.table(\"customer_train\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37609f12-1a86-487e-a671-53e82e06e33d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 1: Dropping Missing Values (`dropna`)\n",
    "\n",
    "**When to drop?**\n",
    "Dropping data is the easiest but most dangerous method.\n",
    "- **Pros:** Simple, removes noise.\n",
    "- **Cons:** You lose data (reduced sample size). If the missingness is not random (e.g., rich people refusing to share salary), dropping them introduces **Bias**.\n",
    "\n",
    "Use `dropna` only when:\n",
    "1.  The missing data is very small (< 5%).\n",
    "2.  The column is mostly empty (> 90% missing) and useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "112e5eb4-4e39-41a2-8494-9cfcc43ffb81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop rows where ANY column is null\n",
    "df_drop_any = df.dropna(how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cab3554-7f96-491f-ac86-19998280fddf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_drop_any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "861ec32c-52b0-445a-bec1-d4df3016a233",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop rows where ALL columns are null\n",
    "df_drop_all = df.dropna(how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36af7503-839a-4447-8b4f-145197e896fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_drop_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9f2e96a-bb39-4cc2-9364-7771a608ac1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Drop rows where specific columns are null (e.g., 'age')\n",
    "df_drop_subset = df.dropna(subset=[\"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef730309-eb2d-41bc-b4a5-c95716769737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_drop_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb1cd636-919e-4de6-8adf-8cc55ce442d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Original: {df.count()}\")\n",
    "print(f\"Drop Any: {df_drop_any.count()}\")\n",
    "print(f\"Drop Age: {df_drop_subset.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfc37bca-047a-4c0a-bc22-d88e9ce2bb67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 2: Filling with Constants (`fillna`)\n",
    "\n",
    "**When to use fillna?**\n",
    "Best for **categorical data** where a constant like \"Unknown\" or \"Other\" makes business sense.\n",
    "\n",
    "In our TechCorp data:\n",
    "- `source` has ~4% missing values (unknown recruitment channel)\n",
    "- We'll fill with \"UNKNOWN\" - this becomes a valid category for the model to learn from!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcd61df6-bef9-41d1-a3bd-fbc92d222b4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check missing source values before filling\n",
    "print(f\"Missing source values: {df.filter(df.source.isNull()).count()}\")\n",
    "\n",
    "# Fill missing source with 'UNKNOWN' (categorical imputation)\n",
    "df_source_filled = df.fillna({\"source\": \"UNKNOWN\"})\n",
    "\n",
    "# Verify the fill worked\n",
    "print(f\"After fillna - Missing source: {df_source_filled.filter(df_source_filled.source.isNull()).count()}\")\n",
    "print(f\"UNKNOWN source count: {df_source_filled.filter(df_source_filled.source == 'UNKNOWN').count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92aa9847-176c-47de-9248-04a2109c7370",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show the distribution of source values (including UNKNOWN)\n",
    "display(df_source_filled.groupBy(\"source\").count().orderBy(\"count\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02669c4b-f6fd-4023-9cc8-56bc24eefeab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 3: MLlib Imputer (Mean vs Median)\n",
    "\n",
    "For numerical data, we can fill gaps with a statistical summary.\n",
    "\n",
    "**Mean vs. Median:**\n",
    "- **Mean:** Good for normally distributed data. **Bad** if there are outliers (one billionaire pulls the mean up).\n",
    "- **Median:** Robust to outliers. Usually the safer default choice for things like Salary or House Prices.\n",
    "\n",
    "*Note: We always calculate the Mean/Median on the TRAINING set and apply that value to the Test set to avoid data leakage.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4c82e5d-5a82-4ee3-b488-7a38fb0faad0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PySpark MLlib `Imputer` Options\n",
    "\n",
    "The `pyspark.ml.feature.Imputer` is used to fill missing values in numerical columns. Key options:\n",
    "\n",
    "- **inputCols**: List of input column names with missing values.\n",
    "- **outputCols**: List of output column names for imputed values.\n",
    "- **strategy**: Imputation method. Options:\n",
    "  - `\"mean\"`: Replace missing values with the mean (default).\n",
    "  - `\"median\"`: Replace missing values with the median.\n",
    "  - `\"mode\"`: Replace missing values with the most frequent value.\n",
    "- **missingValue**: Value to consider as missing (default: `float('nan')`).\n",
    "- **relativeError**: Precision for the approximate quantile algorithm (for median/mode).\n",
    "- **addIndicatorCols**: If `True`, adds boolean columns indicating missingness.\n",
    "\n",
    "**Example:**\n",
    "python\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=[\"age\", \"salary\"],\n",
    "    outputCols=[\"age_imputed\", \"salary_imputed\"],\n",
    "    strategy=\"median\"\n",
    ")\n",
    "model = imputer.fit(df)\n",
    "df_imputed = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07b80b8c-846b-4651-a612-41676d2c9c24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "# Check missing values before imputation\n",
    "print(\"Missing values BEFORE imputation:\")\n",
    "print(f\"  - age: {df_source_filled.filter(df_source_filled.age.isNull()).count()}\")\n",
    "print(f\"  - salary: {df_source_filled.filter(df_source_filled.salary.isNull()).count()}\")\n",
    "\n",
    "# Define Imputer for NUMERICAL columns\n",
    "# inputCols: columns to fix\n",
    "# outputCols: new columns with fixed values\n",
    "imputer = Imputer(\n",
    "    inputCols=[\"age\", \"salary\"],\n",
    "    outputCols=[\"age_imputed\", \"salary_imputed\"]\n",
    ")\n",
    "\n",
    "# Strategy: 'mean' or 'median' - we use median because salary has outliers (C-level execs)\n",
    "imputer.setStrategy(\"median\")\n",
    "\n",
    "# Fit on Data (Calculate the median from TRAINING data only!)\n",
    "imputer_model = imputer.fit(df_source_filled)\n",
    "\n",
    "# Print the learned medians\n",
    "print(f\"\\nLearned imputation values (medians):\")\n",
    "print(f\"  - age median: {imputer_model.surrogateDF.collect()[0]['age']}\")\n",
    "print(f\"  - salary median: {imputer_model.surrogateDF.collect()[0]['salary']}\")\n",
    "\n",
    "# Transform Data (Apply the median)\n",
    "df_imputed = imputer_model.transform(df_source_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e9f5090-0a13-4535-83d0-23dceed78f39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show imputed values for age\n",
    "print(\"Age imputation examples (where original age was NULL):\")\n",
    "display(df_imputed.select(\"age\", \"age_imputed\", \"salary\", \"salary_imputed\").filter(\"age IS NULL OR salary IS NULL\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef649f40-918a-4e68-a172-89fb9c717d50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 4: Creating Missing Flags (Informative Missingness)\n",
    "\n",
    "Sometimes, the fact that data is missing is a signal in itself.\n",
    "- *Example:* A user who doesn't fill in \"Phone Number\" might be less likely to convert than one who does.\n",
    "- *Example:* A sensor returning `NULL` might mean it's broken, which predicts failure.\n",
    "\n",
    "By creating a binary flag (`is_missing`), we allow the model to learn this pattern instead of just hiding it with imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21457be1-1d33-4a8b-9dc5-eecfe259accc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Create missing flags for BOTH age and salary\n",
    "# This allows the model to learn if \"missingness\" itself is predictive\n",
    "\n",
    "df_flagged = df_imputed \\\n",
    "    .withColumn(\"age_missing_flag\", when(col(\"age\").isNull(), 1).otherwise(0)) \\\n",
    "    .withColumn(\"salary_missing_flag\", when(col(\"salary\").isNull(), 1).otherwise(0)) \\\n",
    "    .withColumn(\"source_was_unknown\", when(col(\"source\") == \"UNKNOWN\", 1).otherwise(0))\n",
    "\n",
    "# Summary of missing flags\n",
    "print(\"Missing flags summary:\")\n",
    "print(f\"  - Records with missing age: {df_flagged.filter(col('age_missing_flag') == 1).count()}\")\n",
    "print(f\"  - Records with missing salary: {df_flagged.filter(col('salary_missing_flag') == 1).count()}\")\n",
    "print(f\"  - Records with unknown source: {df_flagged.filter(col('source_was_unknown') == 1).count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab070447-1ef4-4628-806c-68cdcb523a8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show examples of flagged records\n",
    "display(df_flagged.select(\n",
    "    \"age\", \"age_imputed\", \"age_missing_flag\",\n",
    "    \"salary\", \"salary_imputed\", \"salary_missing_flag\",\n",
    "    \"source\", \"source_was_unknown\"\n",
    ").filter(\"age_missing_flag == 1 OR salary_missing_flag == 1\").limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "971d30dd-de97-4d00-8692-c2b590f0ee04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the imputed data for the next module (Feature Transformation)\n",
    "# We save the version with:\n",
    "# - source filled with \"UNKNOWN\"\n",
    "# - age_imputed and salary_imputed (median values)\n",
    "# - missing flags (age_missing_flag, salary_missing_flag, source_was_unknown)\n",
    "\n",
    "df_flagged.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.customer_train_imputed\")\n",
    "\n",
    "print(\"\u2705 Saved 'customer_train_imputed' table with:\")\n",
    "print(\"   - Categorical imputation: source \u2192 'UNKNOWN'\")\n",
    "print(\"   - Numerical imputation: age_imputed, salary_imputed (median)\")\n",
    "print(\"   - Missing flags: age_missing_flag, salary_missing_flag, source_was_unknown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc72afb9-ab90-4b59-b8db-4c6622a358a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Best Practices\n",
    "\n",
    "### \ud83c\udfaf Imputation Strategy Guide:\n",
    "\n",
    "| Data Type | Missing % | Recommended Approach |\n",
    "|-----------|-----------|---------------------|\n",
    "| Numerical, normal distribution | <30% | Mean imputation |\n",
    "| Numerical, skewed/outliers | <30% | Median imputation |\n",
    "| Categorical | <30% | Mode or \"Unknown\" constant |\n",
    "| Any | >50% | Consider dropping column |\n",
    "| Any | <5% | Can drop rows (if random) |\n",
    "\n",
    "### \u26a0\ufe0f Common Mistakes to Avoid:\n",
    "\n",
    "1. **Imputing before splitting** \u2192 Data leakage (test statistics leak into train)\n",
    "2. **Using global mean/median** \u2192 Should be train-only statistics\n",
    "3. **Ignoring informative missingness** \u2192 Miss valuable signal\n",
    "4. **Always using mean** \u2192 Sensitive to outliers, use median\n",
    "5. **Not documenting imputation** \u2192 Reproducibility issues\n",
    "\n",
    "### \ud83d\udca1 Pro Tips:\n",
    "\n",
    "- Always create missing flags for important features\n",
    "- Use `Imputer` from MLlib for Spark-native imputation\n",
    "- Save the imputer model for applying to test/production data\n",
    "- Consider domain knowledge (e.g., age=0 might mean \"unknown\", not impute)\n",
    "- For time-series: use forward-fill or backward-fill instead of mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e840948-36a9-4e67-b223-96311991f64a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "### What we achieved:\n",
    "\n",
    "- **dropna**: Learned when to safely drop missing data\n",
    "- **fillna**: Used constants for categorical imputation\n",
    "- **Imputer**: Applied median imputation for numerical data\n",
    "- **Missing Flags**: Created binary flags to capture missingness signal\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "| # | Principle |\n",
    "|---|-----------|\n",
    "| 1 | **Fit on train, transform on all** - prevent data leakage |\n",
    "| 2 | **Median is safer than mean** - robust to outliers |\n",
    "| 3 | **Missingness can be informative** - create flags |\n",
    "| 4 | **Document your strategy** - reproducibility matters |\n",
    "| 5 | **Domain knowledge helps** - understand why data is missing |\n",
    "\n",
    "### Data Pipeline Status:\n",
    "\n",
    "| Table | Created | Used By |\n",
    "|-------|---------|---------|\n",
    "| `customer_train` | Module 2 | This module |\n",
    "| `customer_train_imputed` | \u2705 This module | Modules 4-7 |\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "\ud83d\udcda **Next Module:** Module 4 - Feature Transformation (encoding, scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c630327-95eb-4f5a-8f94-b460bfc6d4cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "Optionally remove demo tables created during exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a698370-043d-4e7a-bb95-a107cbbd129d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup - remove demo tables created in this notebook\n",
    "\n",
    "# \u26a0\ufe0f WARNING: Do NOT delete customer_train_imputed - it is needed for subsequent modules!\n",
    "\n",
    "# Uncomment the lines below to remove demo tables:\n",
    "\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {catalog_name}.{schema_name}.customer_train_imputed\")\n",
    "\n",
    "# print(\"\u2705 All demo tables removed\")\n",
    "\n",
    "print(\"\u2139\ufe0f Cleanup disabled (uncomment code to remove demo tables)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Data_Imputing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}