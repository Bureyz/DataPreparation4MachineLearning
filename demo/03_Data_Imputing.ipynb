{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4816b34a",
      "metadata": {},
      "source": [
        "# Module 3: Handling Missing Data (Imputation)\n",
        "\n",
        "**Training Objective:** Master techniques for handling missing values in ML pipelines, from simple deletion to advanced statistical imputation.\n",
        "\n",
        "**Scope:**\n",
        "- Dropping values: When to use `dropna`\n",
        "- Constant filling: Using `fillna` for categorical defaults\n",
        "- Statistical imputation: Using `Imputer` (Mean/Median) for numerical data\n",
        "- Missing flags: Capturing \"missingness\" as a feature"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e01fa050",
      "metadata": {},
      "source": [
        "## Context and Requirements\n",
        "\n",
        "- **Training day:** Day 1 - Data Preparation Fundamentals\n",
        "- **Notebook type:** Demo\n",
        "- **Technical requirements:**\n",
        "  - Databricks Runtime 14.x LTS or newer\n",
        "  - Unity Catalog enabled\n",
        "  - Permissions: CREATE TABLE, SELECT, MODIFY\n",
        "- **Dependencies:** `02_Data_Splitting.ipynb` (creates `customer_train` table)\n",
        "- **Execution time:** ~20 minutes\n",
        "\n",
        "> **Critical:** Always fit imputers on TRAINING data only to prevent data leakage!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57f33664",
      "metadata": {},
      "source": [
        "## Theoretical Introduction\n",
        "\n",
        "**Why handle missing data?**\n",
        "\n",
        "Most ML algorithms cannot handle `NULL` values directly. We must decide how to handle them before training.\n",
        "\n",
        "**Imputation Strategies Comparison:**\n",
        "\n",
        "| Strategy | When to Use | Pros | Cons |\n",
        "|----------|-------------|------|------|\n",
        "| **Drop (dropna)** | <5% missing, random | Simple | Loses data, may introduce bias |\n",
        "| **Constant (fillna)** | Categorical data | Simple, interpretable | May not reflect reality |\n",
        "| **Mean** | Normally distributed | Uses all data | Sensitive to outliers |\n",
        "| **Median** | Skewed data, outliers | Robust | Ignores distribution shape |\n",
        "| **Mode** | Categorical | Preserves distribution | May overfit majority class |\n",
        "\n",
        "**Data Leakage Warning:**\n",
        "> ‚ö†Ô∏è **Critical Rule:** Always calculate imputation statistics (mean, median) on TRAINING data only! Applying test data statistics would leak future information into the model.\n",
        "\n",
        "**Informative Missingness:**\n",
        "Sometimes, the fact that data is missing is a signal itself:\n",
        "- A sensor returning `NULL` might indicate equipment failure\n",
        "- A customer not providing phone number might indicate privacy concerns\n",
        "- Creating a \"missing flag\" allows the model to learn these patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eee92472",
      "metadata": {},
      "source": [
        "## Per-User Isolation\n",
        "\n",
        "Run the initialization script for per-user catalog and schema isolation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebb8f4c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "%run ./00_Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e84ad45e",
      "metadata": {},
      "source": [
        "**Load Training Data:**\n",
        "\n",
        "Remember: We fit imputers on TRAIN data only!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaad4b08",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Training Data\n",
        "df = spark.table(\"customer_train\")\n",
        "display(df.limit(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c6c05d2",
      "metadata": {},
      "source": [
        "## Section 1: Dropping Missing Values (`dropna`)\n",
        "\n",
        "**When to drop?**\n",
        "Dropping data is the easiest but most dangerous method.\n",
        "- **Pros:** Simple, removes noise.\n",
        "- **Cons:** You lose data (reduced sample size). If the missingness is not random (e.g., rich people refusing to share salary), dropping them introduces **Bias**.\n",
        "\n",
        "Use `dropna` only when:\n",
        "1.  The missing data is very small (< 5%).\n",
        "2.  The column is mostly empty (> 90% missing) and useless."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81f2321f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop rows where ANY column is null\n",
        "df_drop_any = df.dropna(how=\"any\")\n",
        "\n",
        "# Drop rows where ALL columns are null\n",
        "df_drop_all = df.dropna(how=\"all\")\n",
        "\n",
        "# Drop rows where specific columns are null (e.g., 'age')\n",
        "df_drop_subset = df.dropna(subset=[\"age\"])\n",
        "\n",
        "print(f\"Original: {df.count()}\")\n",
        "print(f\"Drop Any: {df_drop_any.count()}\")\n",
        "print(f\"Drop Age: {df_drop_subset.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1a9704e",
      "metadata": {},
      "source": [
        "## Section 2: Filling with Constants (`fillna`)\n",
        "\n",
        "Useful for categorical data (e.g., filling missing Country with \"Unknown\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32d597c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fill missing Country with 'Unknown' and missing Age with -1\n",
        "df_filled = df.fillna({\n",
        "    \"country\": \"Unknown\",\n",
        "    \"age\": -1\n",
        "})\n",
        "\n",
        "display(df_filled.filter(df_filled.age == -1).limit(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44332498",
      "metadata": {},
      "source": [
        "## Section 3: MLlib Imputer (Mean vs Median)\n",
        "\n",
        "For numerical data, we can fill gaps with a statistical summary.\n",
        "\n",
        "**Mean vs. Median:**\n",
        "- **Mean:** Good for normally distributed data. **Bad** if there are outliers (one billionaire pulls the mean up).\n",
        "- **Median:** Robust to outliers. Usually the safer default choice for things like Salary or House Prices.\n",
        "\n",
        "*Note: We always calculate the Mean/Median on the TRAINING set and apply that value to the Test set to avoid data leakage.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "762621c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "# Define Imputer\n",
        "# inputCols: columns to fix\n",
        "# outputCols: new columns with fixed values\n",
        "imputer = Imputer(\n",
        "    inputCols=[\"age\", \"salary\"],\n",
        "    outputCols=[\"age_imputed\", \"salary_imputed\"]\n",
        ")\n",
        "\n",
        "# Strategy: 'mean' or 'median'\n",
        "imputer.setStrategy(\"median\")\n",
        "\n",
        "# Fit on Data (Calculate the median)\n",
        "imputer_model = imputer.fit(df)\n",
        "\n",
        "# Transform Data (Apply the median)\n",
        "df_imputed = imputer_model.transform(df)\n",
        "\n",
        "display(df_imputed.select(\"age\", \"age_imputed\", \"salary\", \"salary_imputed\").filter(\"age IS NULL\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d560d68",
      "metadata": {},
      "source": [
        "## Section 4: Creating Missing Flags (Informative Missingness)\n",
        "\n",
        "Sometimes, the fact that data is missing is a signal in itself.\n",
        "- *Example:* A user who doesn't fill in \"Phone Number\" might be less likely to convert than one who does.\n",
        "- *Example:* A sensor returning `NULL` might mean it's broken, which predicts failure.\n",
        "\n",
        "By creating a binary flag (`is_missing`), we allow the model to learn this pattern instead of just hiding it with imputation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "948ce286",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import when, col\n",
        "\n",
        "# Create a flag: 1 if Age was missing, 0 otherwise\n",
        "df_flagged = df_imputed.withColumn(\"age_missing_flag\", when(col(\"age\").isNull(), 1).otherwise(0))\n",
        "\n",
        "display(df_flagged.select(\"age\", \"age_imputed\", \"age_missing_flag\").filter(\"age_missing_flag == 1\").limit(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ff64acb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the imputed data for the next module (Feature Transformation)\n",
        "# We save the version with imputed values and the missing flags\n",
        "df_flagged.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.customer_train_imputed\")\n",
        "print(\"‚úÖ Saved 'customer_train_imputed' table.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "740a72f3",
      "metadata": {},
      "source": [
        "## Best Practices\n",
        "\n",
        "### üéØ Imputation Strategy Guide:\n",
        "\n",
        "| Data Type | Missing % | Recommended Approach |\n",
        "|-----------|-----------|---------------------|\n",
        "| Numerical, normal distribution | <30% | Mean imputation |\n",
        "| Numerical, skewed/outliers | <30% | Median imputation |\n",
        "| Categorical | <30% | Mode or \"Unknown\" constant |\n",
        "| Any | >50% | Consider dropping column |\n",
        "| Any | <5% | Can drop rows (if random) |\n",
        "\n",
        "### ‚ö†Ô∏è Common Mistakes to Avoid:\n",
        "\n",
        "1. **Imputing before splitting** ‚Üí Data leakage (test statistics leak into train)\n",
        "2. **Using global mean/median** ‚Üí Should be train-only statistics\n",
        "3. **Ignoring informative missingness** ‚Üí Miss valuable signal\n",
        "4. **Always using mean** ‚Üí Sensitive to outliers, use median\n",
        "5. **Not documenting imputation** ‚Üí Reproducibility issues\n",
        "\n",
        "### üí° Pro Tips:\n",
        "\n",
        "- Always create missing flags for important features\n",
        "- Use `Imputer` from MLlib for Spark-native imputation\n",
        "- Save the imputer model for applying to test/production data\n",
        "- Consider domain knowledge (e.g., age=0 might mean \"unknown\", not impute)\n",
        "- For time-series: use forward-fill or backward-fill instead of mean"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc8989be",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### What we achieved:\n",
        "\n",
        "- **dropna**: Learned when to safely drop missing data\n",
        "- **fillna**: Used constants for categorical imputation\n",
        "- **Imputer**: Applied median imputation for numerical data\n",
        "- **Missing Flags**: Created binary flags to capture missingness signal\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "| # | Principle |\n",
        "|---|-----------|\n",
        "| 1 | **Fit on train, transform on all** - prevent data leakage |\n",
        "| 2 | **Median is safer than mean** - robust to outliers |\n",
        "| 3 | **Missingness can be informative** - create flags |\n",
        "| 4 | **Document your strategy** - reproducibility matters |\n",
        "| 5 | **Domain knowledge helps** - understand why data is missing |\n",
        "\n",
        "### Data Pipeline Status:\n",
        "\n",
        "| Table | Created | Used By |\n",
        "|-------|---------|---------|\n",
        "| `customer_train` | Module 2 | This module |\n",
        "| `customer_train_imputed` | ‚úÖ This module | Modules 4-7 |\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "üìö **Next Module:** Module 4 - Feature Transformation (encoding, scaling)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdaf93e8",
      "metadata": {},
      "source": [
        "## Cleanup\n",
        "\n",
        "Optionally remove demo tables created during exercises:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcfd8f69",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup - remove demo tables created in this notebook\n",
        "\n",
        "# ‚ö†Ô∏è WARNING: Do NOT delete customer_train_imputed - it is needed for subsequent modules!\n",
        "\n",
        "# Uncomment the lines below to remove demo tables:\n",
        "\n",
        "# spark.sql(f\"DROP TABLE IF EXISTS {catalog_name}.{schema_name}.customer_train_imputed\")\n",
        "\n",
        "# print(\"‚úÖ All demo tables removed\")\n",
        "\n",
        "print(\"‚ÑπÔ∏è Cleanup disabled (uncomment code to remove demo tables)\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
