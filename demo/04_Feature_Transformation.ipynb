{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0effba56",
      "metadata": {},
      "source": [
        "# Module 4: Feature Transformation\n",
        "\n",
        "**Training Objective:** Master techniques for converting raw data into ML-ready features through encoding and scaling.\n",
        "\n",
        "**Scope:**\n",
        "- Categorical Encoding: `StringIndexer` and `OneHotEncoder`\n",
        "- Target Encoding: Advanced encoding technique (introduction)\n",
        "- Feature Scaling: `StandardScaler`, `MinMaxScaler`, `RobustScaler`\n",
        "- Window Functions: Creating features from sequential data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21a61ce8",
      "metadata": {},
      "source": [
        "## Context and Requirements\n",
        "\n",
        "- **Training day:** Day 1 - Data Preparation Fundamentals\n",
        "- **Notebook type:** Demo\n",
        "- **Technical requirements:**\n",
        "  - Databricks Runtime 14.x LTS or newer\n",
        "  - Unity Catalog enabled\n",
        "  - Permissions: CREATE TABLE, SELECT, MODIFY\n",
        "- **Dependencies:** `03_Data_Imputing.ipynb` (creates `customer_train_imputed` table)\n",
        "- **Execution time:** ~25 minutes\n",
        "\n",
        "> **Note:** This module covers essential transformations that prepare data for ML algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf0e36ea",
      "metadata": {},
      "source": [
        "## Theoretical Introduction\n",
        "\n",
        "**Why transform features?**\n",
        "\n",
        "Machine Learning models require numerical input. Raw data often contains text categories and features with vastly different scales.\n",
        "\n",
        "**Encoding Techniques:**\n",
        "\n",
        "| Technique | When to Use | Pros | Cons |\n",
        "|-----------|-------------|------|------|\n",
        "| **StringIndexer** | Ordinal categories (Small/Medium/Large) | Simple, preserves order | Introduces false ordinality for nominal |\n",
        "| **OneHotEncoder** | Nominal categories (Country, Color) | No false ordinality | High dimensionality |\n",
        "| **Target Encoding** | High-cardinality categories | Reduces dimensions | High overfitting risk |\n",
        "\n",
        "**Scaling Techniques:**\n",
        "\n",
        "| Scaler | Formula | When to Use |\n",
        "|--------|---------|-------------|\n",
        "| **StandardScaler** | $(x - \\mu) / \\sigma$ | Normally distributed data |\n",
        "| **MinMaxScaler** | $(x - min) / (max - min)$ | Neural Networks, bounded range needed |\n",
        "| **RobustScaler** | $(x - median) / IQR$ | Data with outliers |\n",
        "\n",
        "**Why scaling matters:**\n",
        "> Many algorithms (Linear Regression, K-Means, KNN, SVM) calculate distances. If one feature has range [0, 1] and another [0, 1,000,000], the second dominates!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6c57a92",
      "metadata": {},
      "source": [
        "## Per-User Isolation\n",
        "\n",
        "Run the initialization script for per-user catalog and schema isolation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ccd4f49",
      "metadata": {},
      "outputs": [],
      "source": [
        "%run ./00_Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b882ea4",
      "metadata": {},
      "source": [
        "**Load Imputed Data:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "463a4c53",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Imputed Data\n",
        "df = spark.table(\"customer_train_imputed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df85ba7d",
      "metadata": {},
      "source": [
        "## Section 1: Categorical Encoding\n",
        "\n",
        "### Example 1.1: StringIndexer\n",
        "Machine Learning algorithms generally require numerical input. `StringIndexer` maps each unique string category to a numerical index (0.0, 1.0, 2.0, ...).\n",
        "\n",
        "- **How it works:** It assigns indices based on frequency (most frequent = 0.0).\n",
        "- **Limitation:** It introduces an artificial order (e.g., 0 < 1 < 2). If the category is \"Country\", this implies \"USA < UK\", which is mathematically incorrect for nominal data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ae1be32",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Index 'country'\n",
        "indexer = StringIndexer(inputCol=\"country\", outputCol=\"country_idx\", handleInvalid=\"keep\")\n",
        "indexer_model = indexer.fit(df)\n",
        "df_idx = indexer_model.transform(df)\n",
        "\n",
        "display(df_idx.select(\"country\", \"country_idx\").distinct())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab0409fe",
      "metadata": {},
      "source": [
        "### Example 1.2: OneHotEncoder\n",
        "To fix the ordinality issue of StringIndexer, we use One-Hot Encoding (or Dummy Variables).\n",
        "\n",
        "- **How it works:** It creates a binary vector for each category.\n",
        "- **Why use it:** It allows the model to treat each category independently without assuming any order (e.g., USA is not \"smaller\" than UK).\n",
        "- **Trade-off:** It increases the dimensionality of the dataset (Curse of Dimensionality)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d515e5e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(inputCols=[\"country_idx\"], outputCols=[\"country_vec\"])\n",
        "encoder_model = encoder.fit(df_idx)\n",
        "df_encoded = encoder_model.transform(df_idx)\n",
        "\n",
        "display(df_encoded.select(\"country\", \"country_idx\", \"country_vec\").limit(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "999c02a0",
      "metadata": {},
      "source": [
        "### Example 1.3: Target Encoding (Advanced)\n",
        "Instead of indexing, we replace the category with the **mean of the target variable** for that category.\n",
        "*Example:* If average salary in \"USA\" is 80k, replace \"USA\" with 80000.\n",
        "\n",
        "> ‚ö†Ô∏è **Warning:** High risk of overfitting! Use with regularization or cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b818f7cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual Target Encoding Example\n",
        "# Calculate mean salary per country\n",
        "country_means = df.groupBy(\"country\").agg({\"salary_imputed\": \"avg\"}).withColumnRenamed(\"avg(salary_imputed)\", \"country_target_enc\")\n",
        "\n",
        "# Join back to main table\n",
        "df_target_enc = df.join(country_means, on=\"country\", how=\"left\")\n",
        "\n",
        "display(df_target_enc.select(\"country\", \"country_target_enc\").distinct())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9212c04",
      "metadata": {},
      "source": [
        "## Section 2: Feature Scaling\n",
        "\n",
        "We need to assemble features into a vector first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1392da31",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(inputCols=[\"age_imputed\", \"salary_imputed\"], outputCol=\"features_num\")\n",
        "df_vec = assembler.transform(df_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f95a61f5",
      "metadata": {},
      "source": [
        "### Example 2.1: StandardScaler vs MinMaxScaler vs RobustScaler\n",
        "\n",
        "Many algorithms (like Linear Regression, K-Means, KNN) calculate distances between data points. If one feature has a range of [0, 1] and another [0, 1,000,000], the second feature will dominate the distance calculation. Scaling brings them to a comparable range.\n",
        "\n",
        "| Scaler | Best For | Characteristics |\n",
        "|--------|----------|-----------------|\n",
        "| **StandardScaler** | Normally distributed data | Mean=0, Std=1 |\n",
        "| **MinMaxScaler** | Neural Networks, bounded range | Range [0, 1] |\n",
        "| **RobustScaler** | Data with outliers | Uses Median/IQR |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d87ced0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StandardScaler, MinMaxScaler, RobustScaler\n",
        "\n",
        "# 1. Standard Scaler\n",
        "scaler_std = StandardScaler(inputCol=\"features_num\", outputCol=\"features_std\")\n",
        "df_scaled = scaler_std.fit(df_vec).transform(df_vec)\n",
        "\n",
        "# 2. MinMax Scaler\n",
        "scaler_minmax = MinMaxScaler(inputCol=\"features_num\", outputCol=\"features_minmax\")\n",
        "df_scaled = scaler_minmax.fit(df_scaled).transform(df_scaled)\n",
        "\n",
        "# 3. Robust Scaler (Great for our Salary outliers!)\n",
        "scaler_robust = RobustScaler(inputCol=\"features_num\", outputCol=\"features_robust\")\n",
        "df_scaled = scaler_robust.fit(df_scaled).transform(df_scaled)\n",
        "\n",
        "display(df_scaled.select(\"features_num\", \"features_std\", \"features_minmax\", \"features_robust\").limit(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a727332",
      "metadata": {},
      "source": [
        "## Section 3: Window Functions (Sequential Features)\n",
        "\n",
        "For time-series or ordered data, we often need values from \"previous rows\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0514f125",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import lag, lead, row_number, avg, col\n",
        "\n",
        "# Define Window: Partition by Country, Order by Date\n",
        "w = Window.partitionBy(\"country\").orderBy(\"registration_date\")\n",
        "\n",
        "# 1. Lag: Previous salary in the same country\n",
        "df_window = df_scaled.withColumn(\"prev_salary\", lag(\"salary_imputed\", 1).over(w))\n",
        "\n",
        "# 2. Row Number: Order of registration\n",
        "df_window = df_window.withColumn(\"reg_rank\", row_number().over(w))\n",
        "\n",
        "# 3. Rolling Average: Avg salary of last 3 people\n",
        "w_rolling = w.rowsBetween(-2, 0)\n",
        "df_window = df_window.withColumn(\"rolling_avg_salary\", avg(\"salary_imputed\").over(w_rolling))\n",
        "\n",
        "display(df_window.select(\"country\", \"registration_date\", \"salary_imputed\", \"prev_salary\", \"rolling_avg_salary\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfb08523",
      "metadata": {},
      "source": [
        "## Best Practices\n",
        "\n",
        "### üéØ Transformation Strategy Guide:\n",
        "\n",
        "| Feature Type | Recommended Transformation |\n",
        "|--------------|---------------------------|\n",
        "| Categorical (low cardinality <10) | OneHotEncoder |\n",
        "| Categorical (high cardinality >100) | Target Encoding or Embeddings |\n",
        "| Categorical (ordinal) | StringIndexer only |\n",
        "| Numerical (normal distribution) | StandardScaler |\n",
        "| Numerical (with outliers) | RobustScaler |\n",
        "| Numerical (bounded range needed) | MinMaxScaler |\n",
        "\n",
        "### ‚ö†Ô∏è Common Mistakes to Avoid:\n",
        "\n",
        "1. **Using StringIndexer for nominal data** ‚Üí Introduces false ordinality\n",
        "2. **OneHotEncoder on high-cardinality** ‚Üí Curse of dimensionality\n",
        "3. **Target Encoding without regularization** ‚Üí Overfitting\n",
        "4. **Scaling before splitting** ‚Üí Data leakage\n",
        "5. **Not scaling for distance-based models** ‚Üí Feature dominance\n",
        "\n",
        "### üí° Pro Tips:\n",
        "\n",
        "- Always fit scalers on TRAINING data only\n",
        "- Use `handleInvalid=\"keep\"` for StringIndexer to handle new categories\n",
        "- Consider RobustScaler as default (more robust than StandardScaler)\n",
        "- Window functions are powerful for time-series feature engineering\n",
        "- Save transformer models for applying to test/production data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e0d43d5",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### What we achieved:\n",
        "\n",
        "- **StringIndexer**: Converted categories to numerical indices\n",
        "- **OneHotEncoder**: Created binary vectors for nominal categories\n",
        "- **Target Encoding**: Introduced advanced encoding (with caveats)\n",
        "- **Scalers**: Compared StandardScaler, MinMaxScaler, RobustScaler\n",
        "- **Window Functions**: Created lag and rolling average features\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "| # | Principle |\n",
        "|---|-----------|\n",
        "| 1 | **Choose encoder by category type** - nominal vs ordinal |\n",
        "| 2 | **RobustScaler for outliers** - uses median/IQR |\n",
        "| 3 | **Fit on train only** - prevent data leakage |\n",
        "| 4 | **Window functions for time-series** - powerful feature engineering |\n",
        "| 5 | **Target Encoding is risky** - use with cross-validation |\n",
        "\n",
        "### Data Pipeline Status:\n",
        "\n",
        "| Table | Created | Used By |\n",
        "|-------|---------|---------|\n",
        "| `customer_train_imputed` | Module 3 | This module |\n",
        "| `customer_train_transformed` | ‚úÖ This module | Modules 5-7 |\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "üìö **Next Module:** Module 5 - Feature Engineering (VectorAssembler, log transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "292cb950",
      "metadata": {},
      "source": [
        "## Cleanup\n",
        "\n",
        "Optionally remove demo tables created during exercises:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac080fc3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup - remove demo tables created in this notebook\n",
        "\n",
        "# Uncomment the lines below to remove demo tables:\n",
        "\n",
        "# spark.sql(f\"DROP TABLE IF EXISTS {catalog_name}.{schema_name}.customer_train_transformed\")\n",
        "\n",
        "# print(\"‚úÖ All demo tables removed\")\n",
        "\n",
        "print(\"‚ÑπÔ∏è Cleanup disabled (uncomment code to remove demo tables)\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
