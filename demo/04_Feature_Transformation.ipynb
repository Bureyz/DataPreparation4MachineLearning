{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5f3bdc0-5be2-4f95-86fa-11ac8e3ff39a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Module 4: Feature Transformation\n",
    "\n",
    "## Business Context: TechCorp HR Analytics (continued)\n",
    "\n",
    "**Where We Are:**\n",
    "Our TechCorp employee data now has complete values (imputed). But ML algorithms require numerical input.\n",
    "\n",
    "**Columns to Transform:**\n",
    "| Column | Type | Transformation |\n",
    "|--------|------|----------------|\n",
    "| `country` | Categorical (5 values) | StringIndexer \u2192 OneHotEncoder |\n",
    "| `source` | Categorical (5 values incl. UNKNOWN) | StringIndexer \u2192 OneHotEncoder |\n",
    "| `age_imputed` | Numerical | VectorAssembler \u2192 Scaler |\n",
    "| `salary_imputed` | Numerical | VectorAssembler \u2192 Scaler |\n",
    "\n",
    "---\n",
    "\n",
    "**Training Objective:** Master feature transformation techniques.\n",
    "\n",
    "**Scope:**\n",
    "- Categorical Encoding: `StringIndexer` and `OneHotEncoder`\n",
    "- Feature Scaling: `StandardScaler`, `MinMaxScaler`, `RobustScaler`\n",
    "- Window Functions: Creating features from sequential data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0b701d7-e599-4a7d-8c92-8b8024c5038e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Context and Requirements\n",
    "\n",
    "- **Training day:** Day 1 - Data Preparation Fundamentals\n",
    "- **Notebook type:** Demo\n",
    "- **Technical requirements:**\n",
    "  - Databricks Runtime 14.x LTS or newer\n",
    "  - Unity Catalog enabled\n",
    "  - Permissions: CREATE TABLE, SELECT, MODIFY\n",
    "- **Dependencies:** `03_Data_Imputing.ipynb` (creates `customer_train_imputed` table)\n",
    "- **Execution time:** ~25 minutes\n",
    "\n",
    "> **Note:** This module covers essential transformations that prepare data for ML algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3bc1213-57d1-45b8-9c0d-d812f71982c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Theoretical Introduction\n",
    "\n",
    "**Why transform features?**\n",
    "\n",
    "Machine Learning models require numerical input. Raw data often contains text categories and features with vastly different scales.\n",
    "\n",
    "**Encoding Techniques:**\n",
    "\n",
    "| Technique | When to Use | Pros | Cons |\n",
    "|-----------|-------------|------|------|\n",
    "| **StringIndexer** | Ordinal categories (Small/Medium/Large) | Simple, preserves order | Introduces false ordinality for nominal |\n",
    "| **OneHotEncoder** | Nominal categories (Country, Color) | No false ordinality | High dimensionality |\n",
    "| **Target Encoding** | High-cardinality categories | Reduces dimensions | High overfitting risk |\n",
    "\n",
    "**Scaling Techniques:**\n",
    "\n",
    "| Scaler | Formula | When to Use |\n",
    "|--------|---------|-------------|\n",
    "| **StandardScaler** | $(x - \\mu) / \\sigma$ | Normally distributed data |\n",
    "| **MinMaxScaler** | $(x - min) / (max - min)$ | Neural Networks, bounded range needed |\n",
    "| **RobustScaler** | $(x - median) / IQR$ | Data with outliers |\n",
    "\n",
    "**Why scaling matters:**\n",
    "> Many algorithms (Linear Regression, K-Means, KNN, SVM) calculate distances. If one feature has range [0, 1] and another [0, 1,000,000], the second dominates!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0b38370-6d08-424d-a154-45ca65591695",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Per-User Isolation\n",
    "\n",
    "Run the initialization script for per-user catalog and schema isolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e76f042-1583-4a51-9f9c-3eaf35bbf54b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "633392f1-339e-4fd5-a8bb-7925ae9ab6ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Load Imputed Data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "926ec8f3-0a1b-485d-8fa5-cf6e3b80246f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Imputed Data\n",
    "df = spark.table(\"customer_train_imputed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94d59994-4954-48e3-b98d-6603a53c49bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 1: Categorical Encoding\n",
    "\n",
    "### Example 1.1: StringIndexer\n",
    "Machine Learning algorithms generally require numerical input. `StringIndexer` maps each unique string category to a numerical index (0.0, 1.0, 2.0, ...).\n",
    "\n",
    "- **How it works:** It assigns indices based on frequency (most frequent = 0.0).\n",
    "- **Limitation:** It introduces an artificial order (e.g., 0 < 1 < 2). If the category is \"Country\", this implies \"USA < UK\", which is mathematically incorrect for nominal data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5199528-53da-4063-a7c9-7bd86d603bd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### StringIndexer Options\n",
    "\n",
    "| Option         | Description                                                                                                 | Values / Default                |\n",
    "|----------------|------------------------------------------------------------------------------------------------------------|---------------------------------|\n",
    "| **inputCol**   | Name of the input column containing string categories.                                                      | String                          |\n",
    "| **outputCol**  | Name of the output column with indexed values.                                                              | String                          |\n",
    "| **handleInvalid** | How to handle unseen or NULL values in the input column.                                                 | 'error' (default), 'skip', 'keep' |\n",
    "| **stringOrderType** | How to order labels before assigning indices.                                                          | 'frequencyDesc' (default), 'frequencyAsc', 'alphabetDesc', 'alphabetAsc' |\n",
    "| **inputCols**  | List of input columns (for multi-column indexing).                                                         | List[String]                    |\n",
    "| **outputCols** | List of output columns (for multi-column indexing).                                                        | List[String]                    |\n",
    "\n",
    "**Details:**\n",
    "- `handleInvalid='error'`: Throws error for unseen/null values (default).\n",
    "- `handleInvalid='skip'`: Drops rows with unseen/null values.\n",
    "- `handleInvalid='keep'`: Assigns unseen/null values to a special index.\n",
    "- `stringOrderType`: Controls how categories are ordered before indexing (by frequency or alphabetically, ascending or descending)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fc32320-2a6e-422a-bd83-ad73bc032da5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Index 'country'\n",
    "indexer = StringIndexer(inputCol=\"country\", outputCol=\"country_idx\", handleInvalid=\"keep\")\n",
    "indexer_model = indexer.fit(df)\n",
    "df_idx = indexer_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df8a329d-4b21-4c95-a2eb-9bcf35e94999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_idx.select(\"country\", \"country_idx\").distinct())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3514f01c-8241-4d25-9eb6-d31eb108ffc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 1.2: OneHotEncoder\n",
    "To fix the ordinality issue of StringIndexer, we use One-Hot Encoding (or Dummy Variables).\n",
    "\n",
    "- **How it works:** It creates a binary vector for each category.\n",
    "- **Why use it:** It allows the model to treat each category independently without assuming any order (e.g., USA is not \"smaller\" than UK).\n",
    "- **Trade-off:** It increases the dimensionality of the dataset (Curse of Dimensionality)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c12bafd5-6fbd-4888-9e71-5e6a3e618e1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### OneHotEncoder Options\n",
    "\n",
    "| Option           | Description                                                                                                 | Values / Default                |\n",
    "|------------------|------------------------------------------------------------------------------------------------------------|---------------------------------|\n",
    "| **inputCol**     | Name of the input column to encode (single column).                                                        | String                          |\n",
    "| **inputCols**    | List of input columns to encode (multi-column support).                                                    | List[String]                    |\n",
    "| **outputCol**    | Name of the output column for the encoded vector (single column).                                          | String                          |\n",
    "| **outputCols**   | List of output columns for the encoded vectors (multi-column support).                                     | List[String]                    |\n",
    "| **dropLast**     | Whether to drop the last category to avoid collinearity (one less output column per input).                | True (default), False           |\n",
    "| **handleInvalid**| How to handle invalid (unseen or null) values during transform: 'error' (default) or 'keep' (extra index). | 'error' (default), 'keep'       |\n",
    "\n",
    "**Details:**\n",
    "- `inputCol`/`outputCol`: For encoding a single column.\n",
    "- `inputCols`/`outputCols`: For encoding multiple columns at once.\n",
    "- `dropLast=True`: Drops the last category to avoid the dummy variable trap.\n",
    "- `handleInvalid='error'`: Throws error for unseen/null values (default).\n",
    "- `handleInvalid='keep'`: Assigns unseen/null values to an extra category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d13c5cf0-656b-450c-9985-364db2c1ecdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"country_idx\"], outputCols=[\"country_vec\"])\n",
    "encoder_model = encoder.fit(df_idx)\n",
    "df_encoded = encoder_model.transform(df_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e65dd2d9-193d-4201-912f-e5629577be4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"country_idx\"], outputCols=[\"country_vec\"], dropLast=True)\n",
    "encoder_model = encoder.fit(df_idx)\n",
    "df_encoded = encoder_model.transform(df_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78be8f52-8fd6-4388-8397-6fdb6a6b53a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_encoded.select(\"country\", \"country_idx\", \"country_vec\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f432c76-2a8d-46b5-a6d0-ac7d2985a439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 1.3: Target Encoding (Advanced)\n",
    "Instead of indexing, we replace the category with the **mean of the target variable** for that category.\n",
    "*Example:* If average salary in \"USA\" is 80k, replace \"USA\" with 80000.\n",
    "\n",
    "> \u26a0\ufe0f **Warning:** High risk of overfitting! Use with regularization or cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62da7567-463f-4e7e-87b1-e8a0d5c8143a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Manual Target Encoding Example\n",
    "# Calculate mean salary per country\n",
    "country_means = df.groupBy(\"country\").agg({\"salary_imputed\": \"avg\"}).withColumnRenamed(\"avg(salary_imputed)\", \"country_target_enc\")\n",
    "\n",
    "# Join back to main table\n",
    "df_target_enc = df.join(country_means, on=\"country\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fefb28e4-4a4b-443a-8f99-28c217357dd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_target_enc.select(\"country\", \"country_target_enc\").distinct())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d90d0c64-cbd8-46c8-bb8f-7562fe827688",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 2: Feature Scaling\n",
    "\n",
    "We need to assemble features into a vector first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1242022f-e2a3-4ea5-aa49-aea20743b65c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### VectorAssembler Options\n",
    "\n",
    "| Option           | Description                                                                                                 | Values / Default                |\n",
    "|------------------|------------------------------------------------------------------------------------------------------------|---------------------------------|\n",
    "| **inputCols**    | List of input column names to assemble into a vector.                                                      | List[String] (required)         |\n",
    "| **outputCol**    | Name of the output column for the assembled vector.                                                        | String (required)               |\n",
    "| **handleInvalid**| How to handle invalid (NULL or NaN) values: 'error' (throw), 'skip' (drop row), 'keep' (NaN in vector).    | 'error' (default), 'skip', 'keep' |\n",
    "| **params**       | Returns all params ordered by name.                                                                        | -                               |\n",
    "| **uid**          | Unique identifier for the instance.                                                                        | -                               |\n",
    "\n",
    "**Details:**\n",
    "- `inputCols`: Columns can be numeric or vector type.\n",
    "- `outputCol`: Output column will be of vector type.\n",
    "- `handleInvalid`: Use 'keep' to retain rows with invalid values as NaN in the output vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a6c80e8-3a06-4896-97c8-4e013b389ac2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"age_imputed\", \"salary_imputed\"], outputCol=\"features_num\")\n",
    "df_vec = assembler.transform(df_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a26fb74a-268c-4153-a36d-c90a4695f23f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8160a953-3336-4e1f-a369-e3c3ebed89e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 2.1: StandardScaler vs MinMaxScaler vs RobustScaler\n",
    "\n",
    "Many algorithms (like Linear Regression, K-Means, KNN) calculate distances between data points. If one feature has a range of [0, 1] and another [0, 1,000,000], the second feature will dominate the distance calculation. Scaling brings them to a comparable range.\n",
    "\n",
    "| Scaler | Best For | Characteristics |\n",
    "|--------|----------|-----------------|\n",
    "| **StandardScaler** | Normally distributed data | Mean=0, Std=1 |\n",
    "| **MinMaxScaler** | Neural Networks, bounded range | Range [0, 1] |\n",
    "| **RobustScaler** | Data with outliers | Uses Median/IQR |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaf24fe9-fc86-44cf-9f63-a388eaa97b08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Options for: StandardScaler vs MinMaxScaler vs RobustScaler\n",
    "\n",
    "| Scaler            | Option         | Description                                                                 | Values / Default                |\n",
    "|-------------------|---------------|-----------------------------------------------------------------------------|---------------------------------|\n",
    "| **StandardScaler**| `inputCol`    | Name of input column (vector).                                              | String (required)               |\n",
    "|                   | `outputCol`   | Name of output column.                                                      | String (required)               |\n",
    "|                   | `withMean`    | Center data with mean.                                                      | False (default), True           |\n",
    "|                   | `withStd`     | Scale to unit standard deviation.                                           | True (default), False           |\n",
    "| **MinMaxScaler**  | `inputCol`    | Name of input column (vector).                                              | String (required)               |\n",
    "|                   | `outputCol`   | Name of output column.                                                      | String (required)               |\n",
    "|                   | `min`         | Lower bound after transformation.                                           | 0.0 (default)                   |\n",
    "|                   | `max`         | Upper bound after transformation.                                           | 1.0 (default)                   |\n",
    "| **RobustScaler**  | `inputCol`    | Name of input column (vector).                                              | String (required)               |\n",
    "|                   | `outputCol`   | Name of output column.                                                      | String (required)               |\n",
    "|                   | `withCentering`| Center data with median.                                                    | False (default), True           |\n",
    "|                   | `withScaling` | Scale data according to IQR (interquartile range).                          | True (default), False           |\n",
    "|                   | `lower`       | Lower quantile to calculate IQR.                                            | 0.25 (default)                  |\n",
    "|                   | `upper`       | Upper quantile to calculate IQR.                                            | 0.75 (default)                  |\n",
    "\n",
    "**Notes:**\n",
    "- All scalers require input as a vector column (use `VectorAssembler` first).\n",
    "- `StandardScaler` is sensitive to outliers; `RobustScaler` is robust to outliers.\n",
    "- `MinMaxScaler` is useful for neural networks or when a bounded range is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8ab103c-1470-4cf5-9d58-6c913463fb39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler, MinMaxScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "441b58a2-81e2-4099-9385-51c2db58619b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Standard Scaler\n",
    "scaler_std = StandardScaler(inputCol=\"features_num\", outputCol=\"features_std\")\n",
    "df_scaled = scaler_std.fit(df_vec).transform(df_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "570ffa5a-a640-4bab-90dc-d77425a12208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_scaled.select(\"features_std\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e1fbbb6-8c14-4e95-bd35-99c095679528",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. MinMax Scaler\n",
    "scaler_minmax = MinMaxScaler(inputCol=\"features_num\", outputCol=\"features_minmax\")\n",
    "df_scaled = scaler_minmax.fit(df_scaled).transform(df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dac876c9-7742-4d93-bd32-4feaf1ded09b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_scaled.select(\"features_minmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c75557e6-3fc5-4923-86b1-2d3be44c78ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Robust Scaler (Great for our Salary outliers!)\n",
    "scaler_robust = RobustScaler(inputCol=\"features_num\", outputCol=\"features_robust\")\n",
    "df_scaled = scaler_robust.fit(df_scaled).transform(df_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "302d878b-ffc0-446c-8a9b-b4d8245272a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_scaled.select(\"features_robust\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14b4612e-2595-4948-8591-37f17417da70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_scaled.select(\"features_num\", \"features_std\", \"features_minmax\", \"features_robust\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deef4df4-3a0c-4932-8d7e-6349291de3ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 3: Window Functions (Sequential Features)\n",
    "\n",
    "For time-series or ordered data, we often need values from \"previous rows\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d82103a-a277-494c-a880-0b7cee1201cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, lead, row_number, avg, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee0a5da8-6700-41f3-b827-e859318667b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Window: Partition by Country, Order by Date\n",
    "w = Window.partitionBy(\"country\").orderBy(\"registration_date\")\n",
    "\n",
    "# 1. Lag: Previous salary in the same country\n",
    "df_window = df_scaled.withColumn(\"prev_salary\", lag(\"salary_imputed\", 1).over(w))\n",
    "\n",
    "# 2. Row Number: Order of registration\n",
    "df_window = df_window.withColumn(\"reg_rank\", row_number().over(w))\n",
    "\n",
    "# 3. Rolling Average: Avg salary of last 3 people\n",
    "w_rolling = w.rowsBetween(-2, 0)\n",
    "df_window = df_window.withColumn(\"rolling_avg_salary\", avg(\"salary_imputed\").over(w_rolling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfbe31fd-4126-4ec8-a04d-3db27bae12fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Lag: Previous salary in the same country\n",
    "df_window = df_scaled.withColumn(\"prev_salary\", lag(\"salary_imputed\", 1).over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eb8032a-686d-4d13-a2f8-55f237d13bbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Row Number: Order of registration\n",
    "df_window = df_window.withColumn(\"reg_rank\", row_number().over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "499f6dc7-9d82-41d2-84ac-2dd09b6902c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Rolling Average: Avg salary of last 3 people\n",
    "w_rolling = w.rowsBetween(-2, 0)\n",
    "df_window = df_window.withColumn(\"rolling_avg_salary\", avg(\"salary_imputed\").over(w_rolling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee44d584-bdcc-4804-b7ed-af6d8762d3e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_window.select(\"country\", \"registration_date\", \"salary_imputed\", \"prev_salary\", \"rolling_avg_salary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "002dc94f-9743-4192-8db9-0fb8c96baf7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_window.write.mode(\"overwrite\").saveAsTable(\"customer_train_transformed\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65b76d75-80f4-449a-8512-9fc7a14fb343",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(\"customer_train_transformed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0093e85-90a0-41e5-90fe-2549236d24cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register the DataFrame as a temp view for SQL\n",
    "df_scaled.createOrReplaceTempView(\"df_scaled_sql\")\n",
    "\n",
    "# SQL version of the window operations\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "  country,\n",
    "  registration_date,\n",
    "  salary_imputed,\n",
    "  LAG(salary_imputed, 1) OVER (PARTITION BY country ORDER BY registration_date) AS prev_salary,\n",
    "  ROW_NUMBER() OVER (PARTITION BY country ORDER BY registration_date) AS reg_rank,\n",
    "  AVG(salary_imputed) OVER (\n",
    "    PARTITION BY country\n",
    "    ORDER BY registration_date\n",
    "    ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n",
    "  ) AS rolling_avg_salary\n",
    "FROM df_scaled_sql\n",
    "\"\"\"\n",
    "\n",
    "df_window_sql = spark.sql(query)\n",
    "display(df_window_sql.select(\"country\", \"registration_date\", \"salary_imputed\", \"prev_salary\", \"rolling_avg_salary\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe4eda3e-c7a6-402d-aa82-8889eba4e8ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Best Practices\n",
    "\n",
    "### \ud83c\udfaf Transformation Strategy Guide:\n",
    "\n",
    "| Feature Type | Recommended Transformation |\n",
    "|--------------|---------------------------|\n",
    "| Categorical (low cardinality <10) | OneHotEncoder |\n",
    "| Categorical (high cardinality >100) | Target Encoding or Embeddings |\n",
    "| Categorical (ordinal) | StringIndexer only |\n",
    "| Numerical (normal distribution) | StandardScaler |\n",
    "| Numerical (with outliers) | RobustScaler |\n",
    "| Numerical (bounded range needed) | MinMaxScaler |\n",
    "\n",
    "### \u26a0\ufe0f Common Mistakes to Avoid:\n",
    "\n",
    "1. **Using StringIndexer for nominal data** \u2192 Introduces false ordinality\n",
    "2. **OneHotEncoder on high-cardinality** \u2192 Curse of dimensionality\n",
    "3. **Target Encoding without regularization** \u2192 Overfitting\n",
    "4. **Scaling before splitting** \u2192 Data leakage\n",
    "5. **Not scaling for distance-based models** \u2192 Feature dominance\n",
    "\n",
    "### \ud83d\udca1 Pro Tips:\n",
    "\n",
    "- Always fit scalers on TRAINING data only\n",
    "- Use `handleInvalid=\"keep\"` for StringIndexer to handle new categories\n",
    "- Consider RobustScaler as default (more robust than StandardScaler)\n",
    "- Window functions are powerful for time-series feature engineering\n",
    "- Save transformer models for applying to test/production data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9ae1bfb-a445-4d57-9e74-755f0270f271",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "### What we achieved:\n",
    "\n",
    "- **StringIndexer**: Converted categories to numerical indices\n",
    "- **OneHotEncoder**: Created binary vectors for nominal categories\n",
    "- **Target Encoding**: Introduced advanced encoding (with caveats)\n",
    "- **Scalers**: Compared StandardScaler, MinMaxScaler, RobustScaler\n",
    "- **Window Functions**: Created lag and rolling average features\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "| # | Principle |\n",
    "|---|-----------|\n",
    "| 1 | **Choose encoder by category type** - nominal vs ordinal |\n",
    "| 2 | **RobustScaler for outliers** - uses median/IQR |\n",
    "| 3 | **Fit on train only** - prevent data leakage |\n",
    "| 4 | **Window functions for time-series** - powerful feature engineering |\n",
    "| 5 | **Target Encoding is risky** - use with cross-validation |\n",
    "\n",
    "### Data Pipeline Status:\n",
    "\n",
    "| Table | Created | Used By |\n",
    "|-------|---------|---------|\n",
    "| `customer_train_imputed` | Module 3 | This module |\n",
    "| `customer_train_transformed` | \u2705 This module | Modules 5-7 |\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "\ud83d\udcda **Next Module:** Module 5 - Feature Engineering (VectorAssembler, log transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edc2751f-229b-4a35-9bd1-c307ed5a34d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "Optionally remove demo tables created during exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8192117-e9dc-4f9a-b788-34d259daf0aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup - remove demo tables created in this notebook\n",
    "\n",
    "# Uncomment the lines below to remove demo tables:\n",
    "\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {catalog_name}.{schema_name}.customer_train_transformed\")\n",
    "\n",
    "# print(\"\u2705 All demo tables removed\")\n",
    "\n",
    "print(\"\u2139\ufe0f Cleanup disabled (uncomment code to remove demo tables)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5098221191298781,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Feature_Transformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}