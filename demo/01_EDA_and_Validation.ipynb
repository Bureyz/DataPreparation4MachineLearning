{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84e02285-d4ec-42f5-9697-5fe3504c8b35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Module 1: Exploratory Data Analysis (EDA) and Validation\n",
    "\n",
    "**Training Objective:** Master exploratory data analysis (EDA) techniques and data quality validation before starting the ML process.\n",
    "\n",
    "**Scope:**\n",
    "- Generating synthetic data with defects (outliers, missing values)\n",
    "- Data profiling: schema, descriptive statistics\n",
    "- Visualizations: histograms, box plots\n",
    "- Outlier detection: IQR (Interquartile Range) method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f8414b9-140e-42aa-a8dc-ab9dec319281",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Context and Requirements\n",
    "\n",
    "- **Training day:** Day 1 - Data Preparation Fundamentals\n",
    "- **Notebook type:** Demo\n",
    "- **Technical requirements:**\n",
    "  - Databricks Runtime 14.x LTS or newer\n",
    "  - Unity Catalog enabled\n",
    "  - Permissions: CREATE TABLE, SELECT, MODIFY\n",
    "- **Dependencies:** None (first notebook)\n",
    "- **Execution time:** ~30 minutes\n",
    "\n",
    "> **Note:** This notebook generates synthetic data - each run creates fresh data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e1d8c2b-6e87-484b-907d-455f99bb2f47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Theoretical Introduction\n",
    "\n",
    "**Why is EDA crucial in ML?**\n",
    "\n",
    "Exploratory Data Analysis (EDA) is the first and most important step in any ML project. The **\"Garbage In, Garbage Out\"** principle means that even the best model won't help if the data quality is poor.\n",
    "\n",
    "**What does EDA give us?**\n",
    "\n",
    "| Aspect | Question | Consequence for ML |\n",
    "|--------|----------|-------------------|\n",
    "| **Data Quality** | Missing values? Duplicates? Impossible values? | Requires imputation or removal |\n",
    "| **Distribution** | Normal or skewed data? | Affects model choice and scaling |\n",
    "| **Outliers** | Extreme values? | Can \"break\" linear models |\n",
    "| **Correlations** | Which features are correlated? | Helps with feature selection |\n",
    "\n",
    "**IQR Method for Outliers:**\n",
    "\n",
    "We use the **Interquartile Range (IQR)** because it is robust against outliers themselves (unlike standard deviation).\n",
    "\n",
    "Rule: A point is an outlier if it is above $Q3 + 1.5 \\times IQR$ or below $Q1 - 1.5 \\times IQR$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "def67470-0f78-43d1-acbf-358893e2e847",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Per-User Isolation\n",
    "\n",
    "Run the initialization script for per-user catalog and schema isolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20af69ae-47fc-44e3-b40a-2fa33d2fd165",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d025277c-1b15-4a85-9f72-5032b135a912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 1: Data Generation (Bronze Layer)\n",
    "\n",
    "We will generate a dataset representing raw customer data. We will intentionally introduce:\n",
    "- **Outliers**: Extremely high salaries.\n",
    "- **Missing Values**: Nulls in Age and Email.\n",
    "- **Noise**: Inconsistent categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1222f32-cba0-4f76-bb57-2b0cac796b2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install Faker for synthetic data generation\n",
    "%pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "376a2d2b-1ee0-481a-9a34-22896f3f51b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "fake = Faker()\n",
    "random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "n_rows = 2000\n",
    "data = []\n",
    "\n",
    "for i in range(n_rows):\n",
    "    # Simulate Outliers in Salary (1% of data)\n",
    "    if random.random() < 0.01:\n",
    "        salary = random.randint(500000, 1000000) # Outlier\n",
    "    else:\n",
    "        salary = random.randint(30000, 120000)   # Normal\n",
    "        \n",
    "    # Simulate Missing Age (5% of data)\n",
    "    age = None if random.random() < 0.05 else random.randint(18, 85)\n",
    "    \n",
    "    row = (\n",
    "        i, \n",
    "        fake.name(),\n",
    "        age,\n",
    "        salary,\n",
    "        fake.date_between(start_date=\"-2y\", end_date=\"today\").strftime(\"%Y-%m-%d\"),\n",
    "        random.choice([\"CRM\", \"WEB\", \"MOBILE\", \"PARTNER\"]),\n",
    "        random.choice([\"USA\", \"UK\", \"DE\", \"FR\", \"PL\"])\n",
    "    )\n",
    "    data.append(row)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"registration_date\", StringType(), True),\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_raw = spark.createDataFrame(data, schema)\n",
    "df_raw.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.customer_bronze\")\n",
    "\n",
    "print(\"âœ… Generated 'customer_bronze' table.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93ce0be9-29ee-413e-83a1-5334b6c2d3a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 2: Data Profiling & Summary\n",
    "\n",
    "**Why do we need EDA?**\n",
    "Before training any model, we must understand our data. \"Garbage In, Garbage Out\" is the golden rule of ML.\n",
    "EDA helps us answer:\n",
    "1.  **Data Quality:** Are there missing values? Duplicates? Impossible values (e.g., Age = -5)?\n",
    "2.  **Data Distribution:** Is the data normal or skewed? (Affects model choice).\n",
    "3.  **Correlations:** Which features might be useful for prediction?\n",
    "\n",
    "The first step is to look at the raw numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac9cf47d-1933-4a63-9981-5474b7d181f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = spark.table(\"customer_bronze\")\n",
    "\n",
    "# 1. Basic Display - Use the 'Data Profile' tab in the output below!\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6234d3f-8d71-4969-a81e-b67408ddc8a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.data.summarize(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6789466-b5c2-4aac-9624-24e2e2272eb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data Profiling: Display Customer Bronze Table \n",
    "This cell loads the 'customer_bronze' table and displays its contents for initial data profiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d47b1b9d-220b-42ce-bced-b982f15cb463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be6efae2-4935-4c7b-aefe-790b6827bd53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89952bee-faeb-483b-a3bf-17b9d69b4b6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 2.1: Statistical Summary\n",
    "The `summary()` command provides count, mean, stddev, min, max, and quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71cf2828-6901-4b26-b2c2-77f71e8836ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check statistics for numerical columns\n",
    "display(df.select(\"age\", \"salary\").summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "855c723d-eb6e-485d-be87-9fb080a83972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check Skewness\n",
    "# Skewness > 1 indicates highly skewed data (long tail). \n",
    "# This suggests we might need Log Transformation later.\n",
    "from pyspark.sql.functions import skewness\n",
    "\n",
    "display(df.select(skewness(\"age\").alias(\"age_skewness\"), skewness(\"salary\").alias(\"salary_skewness\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f363447-9930-4394-966c-73ffcd1818e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 3: Visualizations\n",
    "\n",
    "We can use the built-in plotting tool in `display()` to visualize distributions.\n",
    "\n",
    "**Task:**\n",
    "1.  Click the **+** icon in the result of the cell below.\n",
    "2.  Select **Visualization**.\n",
    "3.  Choose **Histogram** to see the distribution of `age`.\n",
    "4.  Choose **Box Plot** to see the distribution of `salary` (and spot outliers!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84069bff-6d6f-4e0c-b966-161acd6aa113",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22ab601e-0176-46c5-a7d5-8f770b7f77d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 4: Identifying Outliers\n",
    "\n",
    "**Why are outliers a problem?**\n",
    "Outliers are extreme values that deviate significantly from other observations.\n",
    "- **Impact on Mean:** A single billionaire in a neighborhood can skew the \"average income\" massively, making it unrepresentative.\n",
    "- **Impact on Models:** Linear models (Regression) minimize \"squared error\". An outlier has a huge error, so the model will try too hard to fit it, ruining the fit for the rest of the data.\n",
    "\n",
    "**The IQR Method:**\n",
    "We use the **Interquartile Range (IQR)** because it is robust. Unlike Standard Deviation, it is not influenced by the outliers themselves.\n",
    "Rule: A point is an outlier if it is above $Q3 + 1.5 \\times IQR$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e7b5767-6ac5-47a4-8dfa-32df873d9fd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate Q1 and Q3 for Salary\n",
    "quantiles = df.approxQuantile(\"salary\", [0.25, 0.75], 0.01)\n",
    "q1, q3 = quantiles[0], quantiles[1]\n",
    "iqr = q3 - q1\n",
    "\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "print(f\"Q1: {q1}, Q3: {q3}, IQR: {iqr}\")\n",
    "print(f\"Upper Bound for Outliers: {upper_bound}\")\n",
    "\n",
    "# Filter outliers\n",
    "outliers = df.filter(df.salary > upper_bound)\n",
    "print(f\"Number of outliers found: {outliers.count()}\")\n",
    "display(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7906f080-6517-4bb4-aef3-4f4150738bcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Best Practices\n",
    "\n",
    "### ðŸŽ¯ EDA Strategy (in order of priority):\n",
    "\n",
    "**1. Start with the big picture:**\n",
    "- Check shape: `df.count()`, `len(df.columns)`\n",
    "- Use Data Profile tab in Databricks\n",
    "- Look at `df.printSchema()` for data types\n",
    "\n",
    "**2. Identify quality issues:**\n",
    "- Missing values: `df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])`\n",
    "- Duplicates: `df.count() - df.dropDuplicates().count()`\n",
    "- Invalid values: domain-specific checks (e.g., negative ages)\n",
    "\n",
    "**3. Understand distributions:**\n",
    "- Use `summary()` for numerical columns\n",
    "- Check `skewness()` - values > 1 indicate strong skew\n",
    "- Visualize with histograms and box plots\n",
    "\n",
    "**4. Document findings:**\n",
    "- Keep notes on data quality issues\n",
    "- Document assumptions made\n",
    "- Track decisions for handling outliers/missing values\n",
    "\n",
    "| Metric | Good Value | Action if Exceeded |\n",
    "|--------|------------|-------------------|\n",
    "| Missing rate | < 5% | Consider imputation |\n",
    "| Outlier rate | < 1% | Investigate and decide |\n",
    "| Skewness | -1 to 1 | Consider log transform |\n",
    "| Duplicate rate | 0% | Remove or investigate |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47c91a7e-d002-4b91-9dc2-c04c175b1a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "### What we achieved:\n",
    "\n",
    "- **Data Generation**: Created synthetic `customer_bronze` table with realistic defects\n",
    "- **Data Profiling**: Used `summary()`, `skewness()`, and Data Profile tab\n",
    "- **Visualizations**: Histograms and Box Plots for distribution analysis\n",
    "- **Outlier Detection**: Applied IQR method to identify extreme values\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "| # | Principle |\n",
    "|---|-----------|\n",
    "| 1 | **Always start with EDA** - understand your data before modeling |\n",
    "| 2 | **\"Garbage In, Garbage Out\"** - data quality determines model quality |\n",
    "| 3 | **IQR is robust** - use it instead of standard deviation for outliers |\n",
    "| 4 | **Skewness matters** - affects model choice and feature transformation |\n",
    "| 5 | **Document everything** - track data quality issues and decisions |\n",
    "\n",
    "### Metrics to Monitor:\n",
    "\n",
    "| Metric | Our Data | Status |\n",
    "|--------|----------|--------|\n",
    "| Missing values (age) | ~5% | âš ï¸ Needs imputation |\n",
    "| Outliers (salary) | ~1% | âš ï¸ Needs handling |\n",
    "| Skewness (salary) | High | âš ï¸ Consider log transform |\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "ðŸ“š **Next Module:** Module 2 - Data Splitting (train/test split strategies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e3c4b2f-7798-4b06-9b00-c1d62fc74a13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "Optionally remove demo tables created during exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb3225bf-e530-4a27-adac-49302235f23c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup - remove demo tables created in this notebook\n",
    "\n",
    "# Uncomment the lines below to remove demo tables:\n",
    "\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {catalog_name}.{schema_name}.customer_bronze\")\n",
    "\n",
    "# print(\"âœ… All demo tables removed\")\n",
    "\n",
    "print(\"â„¹ï¸ Cleanup disabled (uncomment code to remove demo tables)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_EDA_and_Validation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
