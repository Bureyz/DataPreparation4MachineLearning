{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "828b6eee",
      "metadata": {},
      "source": [
        "# Module 1: Exploratory Data Analysis (EDA) and Validation\n",
        "\n",
        "**Training Objective:** Master exploratory data analysis (EDA) techniques and data quality validation before starting the ML process.\n",
        "\n",
        "**Scope:**\n",
        "- Generating synthetic data with defects (outliers, missing values)\n",
        "- Data profiling: schema, descriptive statistics\n",
        "- Visualizations: histograms, box plots\n",
        "- Outlier detection: IQR (Interquartile Range) method"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c0d5009",
      "metadata": {},
      "source": [
        "## Context and Requirements\n",
        "\n",
        "- **Training day:** Day 1 - Data Preparation Fundamentals\n",
        "- **Notebook type:** Demo\n",
        "- **Technical requirements:**\n",
        "  - Databricks Runtime 14.x LTS or newer\n",
        "  - Unity Catalog enabled\n",
        "  - Permissions: CREATE TABLE, SELECT, MODIFY\n",
        "- **Dependencies:** None (first notebook)\n",
        "- **Execution time:** ~30 minutes\n",
        "\n",
        "> **Note:** This notebook generates synthetic data - each run creates fresh data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "267d14f0",
      "metadata": {},
      "source": [
        "## Theoretical Introduction\n",
        "\n",
        "**Why is EDA crucial in ML?**\n",
        "\n",
        "Exploratory Data Analysis (EDA) is the first and most important step in any ML project. The **\"Garbage In, Garbage Out\"** principle means that even the best model won't help if the data quality is poor.\n",
        "\n",
        "**What does EDA give us?**\n",
        "\n",
        "| Aspect | Question | Consequence for ML |\n",
        "|--------|----------|-------------------|\n",
        "| **Data Quality** | Missing values? Duplicates? Impossible values? | Requires imputation or removal |\n",
        "| **Distribution** | Normal or skewed data? | Affects model choice and scaling |\n",
        "| **Outliers** | Extreme values? | Can \"break\" linear models |\n",
        "| **Correlations** | Which features are correlated? | Helps with feature selection |\n",
        "\n",
        "**IQR Method for Outliers:**\n",
        "\n",
        "We use the **Interquartile Range (IQR)** because it is robust against outliers themselves (unlike standard deviation).\n",
        "\n",
        "Rule: A point is an outlier if it is above $Q3 + 1.5 \\times IQR$ or below $Q1 - 1.5 \\times IQR$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3419d42c",
      "metadata": {},
      "source": [
        "## Per-User Isolation\n",
        "\n",
        "Run the initialization script for per-user catalog and schema isolation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74fd7ec7",
      "metadata": {},
      "outputs": [],
      "source": [
        "%run ./00_Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64142f0b",
      "metadata": {},
      "source": [
        "## Section 1: Data Generation (Bronze Layer)\n",
        "\n",
        "We will generate a dataset representing raw customer data. We will intentionally introduce:\n",
        "- **Outliers**: Extremely high salaries.\n",
        "- **Missing Values**: Nulls in Age and Email.\n",
        "- **Noise**: Inconsistent categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efb6aa74",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Faker for synthetic data generation\n",
        "%pip install faker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1356ece2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from faker import Faker\n",
        "import random\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "fake = Faker()\n",
        "random.seed(42)\n",
        "\n",
        "# Configuration\n",
        "n_rows = 2000\n",
        "data = []\n",
        "\n",
        "for i in range(n_rows):\n",
        "    # Simulate Outliers in Salary (1% of data)\n",
        "    if random.random() < 0.01:\n",
        "        salary = random.randint(500000, 1000000) # Outlier\n",
        "    else:\n",
        "        salary = random.randint(30000, 120000)   # Normal\n",
        "        \n",
        "    # Simulate Missing Age (5% of data)\n",
        "    age = None if random.random() < 0.05 else random.randint(18, 85)\n",
        "    \n",
        "    row = (\n",
        "        i, \n",
        "        fake.name(),\n",
        "        age,\n",
        "        salary,\n",
        "        fake.date_between(start_date=\"-2y\", end_date=\"today\").strftime(\"%Y-%m-%d\"),\n",
        "        random.choice([\"CRM\", \"WEB\", \"MOBILE\", \"PARTNER\"]),\n",
        "        random.choice([\"USA\", \"UK\", \"DE\", \"FR\", \"PL\"])\n",
        "    )\n",
        "    data.append(row)\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"salary\", IntegerType(), True),\n",
        "    StructField(\"registration_date\", StringType(), True),\n",
        "    StructField(\"source\", StringType(), True),\n",
        "    StructField(\"country\", StringType(), True)\n",
        "])\n",
        "\n",
        "df_raw = spark.createDataFrame(data, schema)\n",
        "df_raw.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.customer_bronze\")\n",
        "\n",
        "print(\"âœ… Generated 'customer_bronze' table.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed8143b8",
      "metadata": {},
      "source": [
        "## Section 2: Data Profiling & Summary\n",
        "\n",
        "**Why do we need EDA?**\n",
        "Before training any model, we must understand our data. \"Garbage In, Garbage Out\" is the golden rule of ML.\n",
        "EDA helps us answer:\n",
        "1.  **Data Quality:** Are there missing values? Duplicates? Impossible values (e.g., Age = -5)?\n",
        "2.  **Data Distribution:** Is the data normal or skewed? (Affects model choice).\n",
        "3.  **Correlations:** Which features might be useful for prediction?\n",
        "\n",
        "The first step is to look at the raw numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34389d80",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "df = spark.table(\"customer_bronze\")\n",
        "\n",
        "# 1. Basic Display - Use the 'Data Profile' tab in the output below!\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8566795",
      "metadata": {},
      "source": [
        "### Example 2.1: Statistical Summary\n",
        "The `summary()` command provides count, mean, stddev, min, max, and quartiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69f031b5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check statistics for numerical columns\n",
        "display(df.select(\"age\", \"salary\").summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2305826c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check Skewness\n",
        "# Skewness > 1 indicates highly skewed data (long tail). \n",
        "# This suggests we might need Log Transformation later.\n",
        "from pyspark.sql.functions import skewness\n",
        "\n",
        "display(df.select(skewness(\"age\").alias(\"age_skewness\"), skewness(\"salary\").alias(\"salary_skewness\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb309daa",
      "metadata": {},
      "source": [
        "## Section 3: Visualizations\n",
        "\n",
        "We can use the built-in plotting tool in `display()` to visualize distributions.\n",
        "\n",
        "**Task:**\n",
        "1.  Click the **+** icon in the result of the cell below.\n",
        "2.  Select **Visualization**.\n",
        "3.  Choose **Histogram** to see the distribution of `age`.\n",
        "4.  Choose **Box Plot** to see the distribution of `salary` (and spot outliers!)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27f72018",
      "metadata": {},
      "outputs": [],
      "source": [
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e8fd86f",
      "metadata": {},
      "source": [
        "## Section 4: Identifying Outliers\n",
        "\n",
        "**Why are outliers a problem?**\n",
        "Outliers are extreme values that deviate significantly from other observations.\n",
        "- **Impact on Mean:** A single billionaire in a neighborhood can skew the \"average income\" massively, making it unrepresentative.\n",
        "- **Impact on Models:** Linear models (Regression) minimize \"squared error\". An outlier has a huge error, so the model will try too hard to fit it, ruining the fit for the rest of the data.\n",
        "\n",
        "**The IQR Method:**\n",
        "We use the **Interquartile Range (IQR)** because it is robust. Unlike Standard Deviation, it is not influenced by the outliers themselves.\n",
        "Rule: A point is an outlier if it is above $Q3 + 1.5 \\times IQR$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4598632",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate Q1 and Q3 for Salary\n",
        "quantiles = df.approxQuantile(\"salary\", [0.25, 0.75], 0.01)\n",
        "q1, q3 = quantiles[0], quantiles[1]\n",
        "iqr = q3 - q1\n",
        "\n",
        "upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "print(f\"Q1: {q1}, Q3: {q3}, IQR: {iqr}\")\n",
        "print(f\"Upper Bound for Outliers: {upper_bound}\")\n",
        "\n",
        "# Filter outliers\n",
        "outliers = df.filter(df.salary > upper_bound)\n",
        "print(f\"Number of outliers found: {outliers.count()}\")\n",
        "display(outliers)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3217d0f",
      "metadata": {},
      "source": [
        "## Best Practices\n",
        "\n",
        "### ðŸŽ¯ EDA Strategy (in order of priority):\n",
        "\n",
        "**1. Start with the big picture:**\n",
        "- Check shape: `df.count()`, `len(df.columns)`\n",
        "- Use Data Profile tab in Databricks\n",
        "- Look at `df.printSchema()` for data types\n",
        "\n",
        "**2. Identify quality issues:**\n",
        "- Missing values: `df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])`\n",
        "- Duplicates: `df.count() - df.dropDuplicates().count()`\n",
        "- Invalid values: domain-specific checks (e.g., negative ages)\n",
        "\n",
        "**3. Understand distributions:**\n",
        "- Use `summary()` for numerical columns\n",
        "- Check `skewness()` - values > 1 indicate strong skew\n",
        "- Visualize with histograms and box plots\n",
        "\n",
        "**4. Document findings:**\n",
        "- Keep notes on data quality issues\n",
        "- Document assumptions made\n",
        "- Track decisions for handling outliers/missing values\n",
        "\n",
        "| Metric | Good Value | Action if Exceeded |\n",
        "|--------|------------|-------------------|\n",
        "| Missing rate | < 5% | Consider imputation |\n",
        "| Outlier rate | < 1% | Investigate and decide |\n",
        "| Skewness | -1 to 1 | Consider log transform |\n",
        "| Duplicate rate | 0% | Remove or investigate |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4040ee14",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### What we achieved:\n",
        "\n",
        "- **Data Generation**: Created synthetic `customer_bronze` table with realistic defects\n",
        "- **Data Profiling**: Used `summary()`, `skewness()`, and Data Profile tab\n",
        "- **Visualizations**: Histograms and Box Plots for distribution analysis\n",
        "- **Outlier Detection**: Applied IQR method to identify extreme values\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "| # | Principle |\n",
        "|---|-----------|\n",
        "| 1 | **Always start with EDA** - understand your data before modeling |\n",
        "| 2 | **\"Garbage In, Garbage Out\"** - data quality determines model quality |\n",
        "| 3 | **IQR is robust** - use it instead of standard deviation for outliers |\n",
        "| 4 | **Skewness matters** - affects model choice and feature transformation |\n",
        "| 5 | **Document everything** - track data quality issues and decisions |\n",
        "\n",
        "### Metrics to Monitor:\n",
        "\n",
        "| Metric | Our Data | Status |\n",
        "|--------|----------|--------|\n",
        "| Missing values (age) | ~5% | âš ï¸ Needs imputation |\n",
        "| Outliers (salary) | ~1% | âš ï¸ Needs handling |\n",
        "| Skewness (salary) | High | âš ï¸ Consider log transform |\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "ðŸ“š **Next Module:** Module 2 - Data Splitting (train/test split strategies)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c507cd3",
      "metadata": {},
      "source": [
        "## Cleanup\n",
        "\n",
        "Optionally remove demo tables created during exercises:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3eeb90c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup - remove demo tables created in this notebook\n",
        "\n",
        "# Uncomment the lines below to remove demo tables:\n",
        "\n",
        "# spark.sql(f\"DROP TABLE IF EXISTS {catalog_name}.{schema_name}.customer_bronze\")\n",
        "\n",
        "# print(\"âœ… All demo tables removed\")\n",
        "\n",
        "print(\"â„¹ï¸ Cleanup disabled (uncomment code to remove demo tables)\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
