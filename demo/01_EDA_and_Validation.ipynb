{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84e02285-d4ec-42f5-9697-5fe3504c8b35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Module 1: Exploratory Data Analysis (EDA) and Validation\n",
    "\n",
    "## Business Context: TechCorp HR Analytics\n",
    "\n",
    "**The Challenge:**\n",
    "TechCorp, a global technology company, wants to build a **Salary Prediction Model** to help HR:\n",
    "1. **Fair compensation:** Ensure new hires receive competitive, unbiased salaries\n",
    "2. **Budget planning:** Predict salary costs for new positions\n",
    "3. **Market benchmarking:** Compare internal salaries with market rates\n",
    "\n",
    "**The Data:**\n",
    "HR has collected employee records with demographics and employment details. Before building any model, we must understand and validate this data.\n",
    "\n",
    "**Our Goal in This Module:**\n",
    "Perform EDA to discover data quality issues (missing values, outliers) that could corrupt the ML model.\n",
    "\n",
    "---\n",
    "\n",
    "**Training Objective:** Master exploratory data analysis (EDA) techniques and data quality validation before starting the ML process.\n",
    "\n",
    "**Scope:**\n",
    "- Generating synthetic employee data with realistic patterns and defects\n",
    "- Data profiling: schema, descriptive statistics\n",
    "- Visualizations: histograms, box plots\n",
    "- Outlier detection: IQR (Interquartile Range) method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f8414b9-140e-42aa-a8dc-ab9dec319281",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Context and Requirements\n",
    "\n",
    "- **Training day:** Day 1 - Data Preparation Fundamentals\n",
    "- **Notebook type:** Demo\n",
    "- **Technical requirements:**\n",
    "  - Databricks Runtime 14.x LTS or newer\n",
    "  - Unity Catalog enabled\n",
    "  - Permissions: CREATE TABLE, SELECT, MODIFY\n",
    "- **Dependencies:** None (first notebook)\n",
    "- **Execution time:** ~30 minutes\n",
    "\n",
    "> **Note:** This notebook generates synthetic data - each run creates fresh data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e1d8c2b-6e87-484b-907d-455f99bb2f47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Theoretical Introduction\n",
    "\n",
    "**Why is EDA crucial in ML?**\n",
    "\n",
    "Exploratory Data Analysis (EDA) is the first and most important step in any ML project. The **\"Garbage In, Garbage Out\"** principle means that even the best model won't help if the data quality is poor.\n",
    "\n",
    "**What does EDA give us?**\n",
    "\n",
    "| Aspect | Question | Consequence for ML |\n",
    "|--------|----------|-------------------|\n",
    "| **Data Quality** | Missing values? Duplicates? Impossible values? | Requires imputation or removal |\n",
    "| **Distribution** | Normal or skewed data? | Affects model choice and scaling |\n",
    "| **Outliers** | Extreme values? | Can \"break\" linear models |\n",
    "| **Correlations** | Which features are correlated? | Helps with feature selection |\n",
    "\n",
    "**IQR Method for Outliers:**\n",
    "\n",
    "We use the **Interquartile Range (IQR)** because it is robust against outliers themselves (unlike standard deviation).\n",
    "\n",
    "Rule: A point is an outlier if it is above $Q3 + 1.5 \\times IQR$ or below $Q1 - 1.5 \\times IQR$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "def67470-0f78-43d1-acbf-358893e2e847",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Per-User Isolation\n",
    "\n",
    "Run the initialization script for per-user catalog and schema isolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20af69ae-47fc-44e3-b40a-2fa33d2fd165",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d025277c-1b15-4a85-9f72-5032b135a912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 1: Data Generation (Bronze Layer)\n",
    "\n",
    "We will generate a dataset representing **TechCorp employee records**. The data has realistic patterns:\n",
    "- **Salary depends on:** Age (experience), Country (market rates), Source (recruitment channel)\n",
    "- **Intentional defects:** Outliers (executive salaries), Missing values (incomplete records)\n",
    "\n",
    "**Realistic Salary Logic:**\n",
    "```\n",
    "base_salary = 35,000\n",
    "+ age_bonus = (age - 22) √ó 800     # Experience premium\n",
    "√ó country_multiplier              # USA: 1.5, UK: 1.3, DE: 1.2, FR: 1.1, PL: 0.7\n",
    "√ó source_multiplier               # PARTNER: 1.15, CRM: 1.0, WEB: 0.95, MOBILE: 0.9\n",
    "+ random_noise                    # Individual variation\n",
    "```\n",
    "\n",
    "This ensures our ML model will find real patterns to learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1222f32-cba0-4f76-bb57-2b0cac796b2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install Faker for synthetic data generation\n",
    "%pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "376a2d2b-1ee0-481a-9a34-22896f3f51b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "fake = Faker()\n",
    "random.seed(42)\n",
    "\n",
    "# Salary multipliers (realistic market differences)\n",
    "COUNTRY_MULTIPLIER = {\"USA\": 1.5, \"UK\": 1.3, \"DE\": 1.2, \"FR\": 1.1, \"PL\": 0.7}\n",
    "SOURCE_MULTIPLIER = {\"PARTNER\": 1.15, \"CRM\": 1.0, \"WEB\": 0.95, \"MOBILE\": 0.9}\n",
    "\n",
    "# Configuration\n",
    "n_rows = 2000\n",
    "data = []\n",
    "\n",
    "for i in range(n_rows):\n",
    "    # Generate demographics first\n",
    "    country = random.choice([\"USA\", \"UK\", \"DE\", \"FR\", \"PL\"])\n",
    "    source = random.choice([\"CRM\", \"WEB\", \"MOBILE\", \"PARTNER\"])\n",
    "    \n",
    "    # Simulate Missing Age (5% of data)\n",
    "    age = None if random.random() < 0.05 else random.randint(22, 65)\n",
    "    \n",
    "    # Calculate REALISTIC salary based on age, country, source\n",
    "    if random.random() < 0.01:\n",
    "        # 1% Outliers: C-level executives\n",
    "        salary = random.randint(400000, 800000)\n",
    "    elif age is None:\n",
    "        # If age is missing, use median age for salary calculation\n",
    "        base_age = 40\n",
    "        base_salary = 35000 + (base_age - 22) * 800\n",
    "        salary = int(base_salary * COUNTRY_MULTIPLIER[country] * SOURCE_MULTIPLIER[source])\n",
    "        salary += random.randint(-8000, 8000)  # noise\n",
    "    else:\n",
    "        # Normal case: salary depends on age, country, source\n",
    "        base_salary = 35000 + (age - 22) * 800  # Experience bonus\n",
    "        salary = int(base_salary * COUNTRY_MULTIPLIER[country] * SOURCE_MULTIPLIER[source])\n",
    "        salary += random.randint(-8000, 8000)  # Individual variation\n",
    "    \n",
    "    row = (\n",
    "        i, \n",
    "        fake.name(),\n",
    "        age,\n",
    "        max(salary, 25000),  # Minimum salary floor\n",
    "        fake.date_between(start_date=\"-2y\", end_date=\"today\").strftime(\"%Y-%m-%d\"),\n",
    "        source,\n",
    "        country\n",
    "    )\n",
    "    data.append(row)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"registration_date\", StringType(), True),  # hire_date\n",
    "    StructField(\"source\", StringType(), True),             # recruitment channel\n",
    "    StructField(\"country\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_raw = spark.createDataFrame(data, schema)\n",
    "df_raw.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.customer_bronze\")\n",
    "\n",
    "print(\"‚úÖ Generated 'customer_bronze' table with realistic salary patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93ce0be9-29ee-413e-83a1-5334b6c2d3a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 2: Data Profiling & Summary\n",
    "\n",
    "**Why do we need EDA?**\n",
    "Before training any model, we must understand our data. \"Garbage In, Garbage Out\" is the golden rule of ML.\n",
    "EDA helps us answer:\n",
    "1.  **Data Quality:** Are there missing values? Duplicates? Impossible values (e.g., Age = -5)?\n",
    "2.  **Data Distribution:** Is the data normal or skewed? (Affects model choice).\n",
    "3.  **Correlations:** Which features might be useful for prediction?\n",
    "\n",
    "The first step is to look at the raw numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac9cf47d-1933-4a63-9981-5474b7d181f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = spark.table(\"customer_bronze\")\n",
    "\n",
    "# 1. Basic Display - Use the 'Data Profile' tab in the output below!\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7240eb5d-aea6-4d12-b0bc-d260b5a64a7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  dbutils.data.summarize(spark.sql(r\"\"\"SELECT * FROM data_ml_preparation.ml_dp_trainer.customer_bronze \"\"\"))\nelse:\n  print(\"This DBR version does not support data profiles.\")\n",
       "commandTitle": "Data Profile 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 1764681900626,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": [
        [
         "mimeBundle",
         null
        ]
       ],
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "471cac62-576b-419e-b0a2-cdddb074550c",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 11.5,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 1764681897925,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 1764681897895,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql \n",
    "SELECT * FROM customer_bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6234d3f-8d71-4969-a81e-b67408ddc8a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.data.summarize(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6789466-b5c2-4aac-9624-24e2e2272eb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Data Profiling: TechCorp Employee Data**\n",
    "\n",
    "This cell loads the 'customer_bronze' table (our HR employee records) and displays its contents for initial data profiling.\n",
    "Notice how salary varies by country and age - these are the patterns our ML model will learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d47b1b9d-220b-42ce-bced-b982f15cb463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be6efae2-4935-4c7b-aefe-790b6827bd53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89952bee-faeb-483b-a3bf-17b9d69b4b6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 2.1: Statistical Summary\n",
    "The `summary()` command provides count, mean, stddev, min, max, and quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71cf2828-6901-4b26-b2c2-77f71e8836ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check statistics for numerical columns\n",
    "display(df.select(\"age\", \"salary\").summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "855c723d-eb6e-485d-be87-9fb080a83972",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"age_skewness\":176},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764682218438}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check Skewness\n",
    "# Skewness > 1 indicates highly skewed data (long tail). \n",
    "# This suggests we might need Log Transformation later.\n",
    "from pyspark.sql.functions import skewness\n",
    "\n",
    "display(df.select(skewness(\"age\").alias(\"age_skewness\"), skewness(\"salary\").alias(\"salary_skewness\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af0fbc21-325a-4b2c-80c7-da367d406e97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 2.2: Delta Lake Fundamental\n",
    "**Delta Lake** is an open-source storage layer that brings ACID transactions to Apache Spark and big data workloads. It's the foundation of the Databricks Lakehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65370ab8-d1b1-4848-83b7-94c037ba5861",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Delta table from Bronze data\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"customers_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd281afa-b8c8-4fb7-b450-cb5507e3323a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM customers_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e7a4772-17bf-44f3-ac63-3ca0b4237b77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read from Delta table\n",
    "df_delta = spark.table(\"customers_delta\")\n",
    "display(df_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e22a2819-f181-4576-80d6-cedd613f31af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "INSERT INTO customers_delta\n",
    "SELECT * FROM customers_delta LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b88fd95-9756-436e-9f29-4cc712dc0c64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "UPDATE customers_delta\n",
    "SET age = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "517ec297-8733-4f41-b67c-437ab3043bdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DELETE FROM customers_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "621b89f9-4ff3-4b33-8d50-dde5e0610cd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Delta table history - essential for ML reproducibility\n",
    "DESCRIBE HISTORY customers_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb4119ae-3924-475d-abd0-3463517f517a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM customers_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2859d310-e039-41d0-b8aa-d2b1f8793970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Time travel example: query the customers_delta table as of version 1\n",
    "SELECT * FROM customers_delta VERSION AS OF 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4082bb84-38fa-4417-ab38-78d65d86fb12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Rollback the Delta table 'customers_delta' to version 1\n",
    "RESTORE TABLE customers_delta TO VERSION AS OF 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f363447-9930-4394-966c-73ffcd1818e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 3: Visualizations\n",
    "\n",
    "We can use the built-in plotting tool in `display()` to visualize distributions.\n",
    "\n",
    "**Task:**\n",
    "1.  Click the **+** icon in the result of the cell below.\n",
    "2.  Select **Visualization**.\n",
    "3.  Choose **Histogram** to see the distribution of `age`.\n",
    "4.  Choose **Box Plot** to see the distribution of `salary` (and spot outliers!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84069bff-6d6f-4e0c-b966-161acd6aa113",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShkZik=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView006e490\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView006e490\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView006e490\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView006e490) ,min_max AS (SELECT `age`,(SELECT MAX(`age`) FROM q) `target_column_max`,(SELECT MIN(`age`) FROM q) `target_column_min` FROM q) ,histogram_meta AS (SELECT `age`,`target_column_min` `min_value`,IF(`target_column_max` = `target_column_min`,`target_column_max` + 1,`target_column_max`) `max_value`,(`target_column_max` - `target_column_min`) / 10 `step` FROM min_max) SELECT IF(ISNULL(`age`),NULL,LEAST(WIDTH_BUCKET(`age`,`min_value`,`max_value`,10),10)) `age_BIN`,FIRST(`min_value` + ((IF(ISNULL(`age`),NULL,LEAST(WIDTH_BUCKET(`age`,`min_value`,`max_value`,10),10)) - 1) * `step`)) `age_BIN_LOWER_BOUND`,FIRST(`step`) `age_BIN_STEP`,COUNT(`age`) `COUNT` FROM histogram_meta GROUP BY `age_BIN`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView006e490\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "age",
             "id": "column_23e9bd72232"
            }
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "histogram",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numBins": 10,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {},
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "ce5b383e-ab8d-4f0a-b7f5-23a296d97ffc",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 20,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "age_BIN",
           "type": "column"
          }
         ],
         "selects": [
          {
           "alias": "age_BIN",
           "args": [
            {
             "column": "age",
             "type": "column"
            },
            {
             "number": 10,
             "type": "number"
            }
           ],
           "function": "BIN",
           "type": "function"
          },
          {
           "alias": "age_BIN_LOWER_BOUND",
           "args": [
            {
             "column": "age",
             "type": "column"
            },
            {
             "number": 10,
             "type": "number"
            }
           ],
           "function": "BIN_LOWER_BOUND",
           "type": "function"
          },
          {
           "alias": "age_BIN_STEP",
           "args": [
            {
             "column": "age",
             "type": "column"
            },
            {
             "number": 10,
             "type": "number"
            }
           ],
           "function": "BIN_STEP",
           "type": "function"
          },
          {
           "alias": "COUNT",
           "args": [
            {
             "column": "age",
             "type": "column"
            }
           ],
           "function": "COUNT",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1349f319-37ee-4cb6-9155-4e8a47ecce45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Advanced Visualizations with Python Libraries\n",
    "\n",
    "For more sophisticated ML visualizations, we can integrate matplotlib and seaborn with Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "313783d8-6dc5-44cc-9d1f-f22ab08d7b7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f899e70c-e447-4f1a-8de9-224da89a661a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df = df.select(\"age\", \"salary\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "034f9e43-01d0-4434-8c68-2c4cf57c418d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37e65f0f-0445-40f0-8d19-c6837b7d2dc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(pandas_df.corr(), annot=True, cmap='coolwarm', center=0, fmt='.3f')\n",
    "plt.title('Feature Correlation Matrix for ML')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3274fc5-ff94-4997-a0a9-5375bf1572c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Distribution analysis for ML feature engineering\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Age distribution\n",
    "axes[0].hist(pandas_df['age'], bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0].set_title('Age Distribution')\n",
    "axes[0].set_xlabel('Age')\n",
    "axes[0].set_ylabel('Frequency')  \n",
    "\n",
    "# Salary distribution\n",
    "axes[1].hist(pandas_df['salary'], bins=1000, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[1].set_title('Salary Distribution')\n",
    "axes[1].set_xlabel('Salary')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "685fb5d4-3f83-4453-9e5f-a5479d222a8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This code generates a correlation heatmap for the features in a pandas DataFrame. It uses matplotlib to set the figure size and seaborn to plot the correlation matrix, displaying correlation coefficients with color gradients and annotations. The heatmap helps visualize relationships between features, which is useful for machine learning analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22ab601e-0176-46c5-a7d5-8f770b7f77d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 4: Identifying Outliers\n",
    "\n",
    "**Why are outliers a problem?**\n",
    "Outliers are extreme values that deviate significantly from other observations.\n",
    "- **Impact on Mean:** A single billionaire in a neighborhood can skew the \"average income\" massively, making it unrepresentative.\n",
    "- **Impact on Models:** Linear models (Regression) minimize \"squared error\". An outlier has a huge error, so the model will try too hard to fit it, ruining the fit for the rest of the data.\n",
    "\n",
    "**The IQR Method:**\n",
    "We use the **Interquartile Range (IQR)** because it is robust. Unlike Standard Deviation, it is not influenced by the outliers themselves.\n",
    "Rule: A point is an outlier if it is above $Q3 + 1.5 \\times IQR$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e7b5767-6ac5-47a4-8dfa-32df873d9fd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate Q1 and Q3 for Salary\n",
    "quantiles = df.approxQuantile(\"salary\", [0.25, 0.75], 0.01)\n",
    "q1, q3 = quantiles[0], quantiles[1]\n",
    "iqr = q3 - q1\n",
    "\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "print(f\"Q1: {q1}, Q3: {q3}, IQR: {iqr}\")\n",
    "print(f\"Upper Bound for Outliers: {upper_bound}\")\n",
    "\n",
    "# Filter outliers\n",
    "outliers = df.filter(df.salary > upper_bound)\n",
    "print(f\"Number of outliers found: {outliers.count()}\")\n",
    "display(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6cde54c-5087-48dc-9cb0-b6fc3a6e38ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 5: Data Validation\n",
    "\n",
    "Data validation ensures that the data meets defined quality standards before it is used for analysis or modeling. This process checks for issues such as missing values, invalid data types, out-of-range values, and inconsistencies. By validating data early, we can prevent downstream errors, improve model reliability, and maintain trust in analytics results.\n",
    "\n",
    "Typical data validation steps include:\n",
    "- Verifying data types and formats\n",
    "- Checking for missing or null values\n",
    "- Ensuring values fall within expected ranges (e.g., age > 0)\n",
    "- Detecting duplicates\n",
    "- Validating categorical values against allowed lists\n",
    "\n",
    "Implementing robust data validation is a best practice for maintaining high data quality in any data pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4e797da-ee05-43cd-a9b1-46b612b11164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Schema validation is the process of ensuring that your data conforms to a predefined structure before it is used for analysis or modeling. This includes checking that each column has the correct data type, required columns are present, and constraints (such as NOT NULL or value ranges) are enforced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ebc44a3-cf03-4875-a13a-ca7fb284f3b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Basic schema validation\n",
    "expected_columns = [\"id\", \"name\", \"age\", \"email\", \"salary\"]\n",
    "actual_columns = df_raw.columns\n",
    "\n",
    "print(actual_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60c14d49-fb80-4c9f-b137-37f0217e2c94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "missing_columns = set(expected_columns) - set(actual_columns)\n",
    "extra_columns = set(actual_columns) - set(expected_columns)\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"‚ùå Missing columns: {missing_columns}\")\n",
    "if extra_columns:\n",
    "    print(f\"‚ö†Ô∏è Extra columns: {extra_columns}\")\n",
    "if not missing_columns and not extra_columns:\n",
    "    print(\"‚úÖ All expected columns present\")\n",
    "\n",
    "# Check data types\n",
    "print(\"üìä Current data types:\")\n",
    "for col_name, col_type in df_raw.dtypes:\n",
    "    print(f\"   {col_name}: {col_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a9b0031-1171-4672-bbd0-05835d210796",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a282d1d5-7e67-4b6f-a362-04d5365379ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37b3819d-e3e5-49a8-9f57-cd51ea92a78b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data type validation for each column\n",
    "expected_types = {\n",
    "    \"id\": \"int\",\n",
    "    \"name\": \"string\",\n",
    "    \"age\": \"int\",\n",
    "    \"salary\": \"int\",\n",
    "    \"registration_date\": \"string\"\n",
    "}\n",
    "\n",
    "actual_types = dict(df.dtypes)\n",
    "\n",
    "type_mismatches = []\n",
    "for col_name, expected_type in expected_types.items():\n",
    "    actual_type = actual_types.get(col_name)\n",
    "    if actual_type != expected_type:\n",
    "        type_mismatches.append((col_name, expected_type, actual_type))\n",
    "\n",
    "if type_mismatches:\n",
    "    print(\"‚ùå Data type mismatches found:\")\n",
    "    for col_name, expected, actual in type_mismatches:\n",
    "        print(f\"   {col_name}: expected {expected}, found {actual}\")\n",
    "else:\n",
    "    print(\"‚úÖ All column data types are correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6947ae59-7abe-47f5-b907-b87fd53a63c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Business Rule Validation Example: Filter/Where Validation\n",
    "\n",
    "Business rule validation using filter/where applies logical conditions to your data to ensure it meets specific requirements. For example, you can filter out records where `age < 0` or `salary > 1_000_000` to enforce domain rules and maintain data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11a30d32-8de9-4edc-9a2b-8729ac7906eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Business rule validation example: filter/where validation\n",
    "# Example: Age must be between 18 and 100 (inclusive), Salary must be >0 and <=500000\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Age validation\n",
    "invalid_age_df = df_raw.filter(\n",
    "    col(\"age\").isNotNull() & ((col(\"age\") < 18) | (col(\"age\") > 100))\n",
    ")\n",
    "display(invalid_age_df)\n",
    "\n",
    "# Salary validation\n",
    "invalid_salary_df = df_raw.filter(\n",
    "    col(\"salary\").isNotNull() & ((col(\"salary\") <= 0) | (col(\"salary\") > 500000))\n",
    ")\n",
    "display(invalid_salary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7906f080-6517-4bb4-aef3-4f4150738bcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Best Practices\n",
    "\n",
    "### üéØ EDA Strategy (in order of priority):\n",
    "\n",
    "**1. Start with the big picture:**\n",
    "- Check shape: `df.count()`, `len(df.columns)`\n",
    "- Use Data Profile tab in Databricks\n",
    "- Look at `df.printSchema()` for data types\n",
    "\n",
    "**2. Identify quality issues:**\n",
    "- Missing values: `df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])`\n",
    "- Duplicates: `df.count() - df.dropDuplicates().count()`\n",
    "- Invalid values: domain-specific checks (e.g., negative ages)\n",
    "\n",
    "**3. Understand distributions:**\n",
    "- Use `summary()` for numerical columns\n",
    "- Check `skewness()` - values > 1 indicate strong skew\n",
    "- Visualize with histograms and box plots\n",
    "\n",
    "**4. Document findings:**\n",
    "- Keep notes on data quality issues\n",
    "- Document assumptions made\n",
    "- Track decisions for handling outliers/missing values\n",
    "\n",
    "| Metric | Good Value | Action if Exceeded |\n",
    "|--------|------------|-------------------|\n",
    "| Missing rate | < 5% | Consider imputation |\n",
    "| Outlier rate | < 1% | Investigate and decide |\n",
    "| Skewness | -1 to 1 | Consider log transform |\n",
    "| Duplicate rate | 0% | Remove or investigate |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47c91a7e-d002-4b91-9dc2-c04c175b1a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "### What we achieved:\n",
    "\n",
    "- **Data Generation**: Created synthetic `customer_bronze` table with realistic defects\n",
    "- **Data Profiling**: Used `summary()`, `skewness()`, and Data Profile tab\n",
    "- **Visualizations**: Histograms and Box Plots for distribution analysis\n",
    "- **Outlier Detection**: Applied IQR method to identify extreme values\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "| # | Principle |\n",
    "|---|-----------|\n",
    "| 1 | **Always start with EDA** - understand your data before modeling |\n",
    "| 2 | **\"Garbage In, Garbage Out\"** - data quality determines model quality |\n",
    "| 3 | **IQR is robust** - use it instead of standard deviation for outliers |\n",
    "| 4 | **Skewness matters** - affects model choice and feature transformation |\n",
    "| 5 | **Document everything** - track data quality issues and decisions |\n",
    "\n",
    "### Metrics to Monitor:\n",
    "\n",
    "| Metric | Our Data | Status |\n",
    "|--------|----------|--------|\n",
    "| Missing values (age) | ~5% | ‚ö†Ô∏è Needs imputation |\n",
    "| Outliers (salary) | ~1% | ‚ö†Ô∏è Needs handling |\n",
    "| Skewness (salary) | High | ‚ö†Ô∏è Consider log transform |\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "üìö **Next Module:** Module 2 - Data Splitting (train/test split strategies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e3c4b2f-7798-4b06-9b00-c1d62fc74a13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "Optionally remove demo tables created during exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb3225bf-e530-4a27-adac-49302235f23c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup - remove demo tables created in this notebook\n",
    "\n",
    "# Uncomment the lines below to remove demo tables:\n",
    "\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {catalog_name}.{schema_name}.customer_bronze\")\n",
    "\n",
    "# print(\"‚úÖ All demo tables removed\")\n",
    "\n",
    "print(\"‚ÑπÔ∏è Cleanup disabled (uncomment code to remove demo tables)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5098221191298569,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_EDA_and_Validation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
